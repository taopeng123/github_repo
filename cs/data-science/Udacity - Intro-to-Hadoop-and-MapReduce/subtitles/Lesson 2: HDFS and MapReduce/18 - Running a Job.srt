1
00:00:00,500 --> 00:00:02,560
It's often the case that MapReduce code is written in

2
00:00:02,560 --> 00:00:05,730
Java. However, to make things a little easier for us,

3
00:00:05,730 --> 00:00:09,260
we've actually written our mapper and reducer in Python instead.

4
00:00:09,260 --> 00:00:11,660
And we can do that thanks to a feature called Hadoop

5
00:00:11,660 --> 00:00:14,360
streaming, which allows you to write your code in pretty

6
00:00:14,360 --> 00:00:17,585
much any language you'd like. So first of all, let's

7
00:00:17,585 --> 00:00:21,460
double-check that we have our input data in HDFS. So,

8
00:00:21,460 --> 00:00:26,250
if I Hadoop fs minus ls, then there's my input directory.

9
00:00:26,250 --> 00:00:28,160
And if I look at that directory, then

10
00:00:28,160 --> 00:00:33,092
yes, there's purchases.txt in there. And in my local

11
00:00:33,092 --> 00:00:36,524
directory, I have mapper.py and reducer.py, that's the

12
00:00:36,524 --> 00:00:40,190
code for the mapper and reducer, written in Python.

13
00:00:40,190 --> 00:00:41,600
We'll look at the actual code in the

14
00:00:41,600 --> 00:00:45,270
next lesson. Okay, to submit the job we have

15
00:00:45,270 --> 00:00:48,090
to give this rather cumbersome command. We say hadoop

16
00:00:48,090 --> 00:00:51,410
jar, a path to a jar, then I specify

17
00:00:51,410 --> 00:00:57,390
the mapper, I specify the reducer, I need to say -file, for both the mapper and

18
00:00:57,390 --> 00:01:04,170
the reducer code. I specify the input directory in HDFS and I specify the output

19
00:01:04,170 --> 00:01:06,790
directory to which the reducers will write their

20
00:01:06,790 --> 00:01:10,330
output data. And we're calling that joboutput. I

21
00:01:10,330 --> 00:01:16,570
hit Enter and off we go. Hadoop's pretty verbose, as you can see. As the job

22
00:01:16,570 --> 00:01:19,840
runs, you'll see a bunch of output which shows us how

23
00:01:19,840 --> 00:01:22,880
far along the job is. It turns out that for this

24
00:01:22,880 --> 00:01:26,000
job Hadoop will be running four mappers. And our virtual machine

25
00:01:26,000 --> 00:01:28,430
here can only run two at a time. So the job is

26
00:01:28,430 --> 00:01:31,510
going to take longer than it would on a larger cluster.

27
00:01:31,510 --> 00:01:34,780
Actually, that's worth mentioning here. With the size of the data we

28
00:01:34,780 --> 00:01:38,000
have for this example which is only 200 megs, realistically, we

29
00:01:38,000 --> 00:01:41,800
could probably have solved this problem faster by just importing the data

30
00:01:41,800 --> 00:01:44,550
into a relational database and querying it from there.

31
00:01:44,550 --> 00:01:47,030
And that's often the case when we're developing and testing

32
00:01:47,030 --> 00:01:50,280
code. Because the test data sets are pretty small, Hadoop

33
00:01:50,280 --> 00:01:53,140
isn't necessarily the optimal tool for the job. But when

34
00:01:53,140 --> 00:01:55,000
we're done testing and we need to process our full

35
00:01:55,000 --> 00:01:58,040
production data, that's when Hadoop really comes into its own.

36
00:01:59,150 --> 00:02:01,230
So, as you can see the job is now nearly

37
00:02:01,230 --> 00:02:06,125
complete, and when the job has finished we'll get a

38
00:02:06,125 --> 00:02:06,240
[UNKNOWN]

39
00:02:06,240 --> 00:02:08,820
output. And we'll see that the last line tells

40
00:02:08,820 --> 00:02:13,230
me that the output directory is called joboutput. Let's

41
00:02:13,230 --> 00:02:16,225
take a look at what we've got in there.

42
00:02:16,225 --> 00:02:18,265
Hadoop fs minus ls, shows me that yes I do

43
00:02:18,265 --> 00:02:21,340
have a job output directory. And if we look

44
00:02:21,340 --> 00:02:23,390
at the job output directory, you'll see that it

45
00:02:23,390 --> 00:02:27,890
contains three things. It contains a file called _SUCCESS,

46
00:02:27,890 --> 00:02:31,240
which just tells me that the job has successfully completed.

47
00:02:31,240 --> 00:02:34,210
It contains a directory called _logs, which

48
00:02:34,210 --> 00:02:37,050
contains some log information about what happened during

49
00:02:37,050 --> 00:02:39,730
the job's run. And then, it contains a

50
00:02:39,730 --> 00:02:45,480
file called part-00000. That file is the output

51
00:02:45,480 --> 00:02:50,210
from the one reducer that we had for this job. Let's take a look at that by

52
00:02:50,210 --> 00:02:56,380
saying hadoop fs minus cat part 00000, and we'll pipe that to less on

53
00:02:56,380 --> 00:02:59,540
our local machine. That's the contents of that file,

54
00:02:59,540 --> 00:03:01,640
which is the output from our reducer. It's the

55
00:03:01,640 --> 00:03:05,490
sum total sales broken down by store exactly as

56
00:03:05,490 --> 00:03:09,320
we want it. Incidentally, if you want to retrieve data

57
00:03:09,320 --> 00:03:12,210
from HFDS and put it onto your local disk,

58
00:03:12,210 --> 00:03:14,590
you can do that with Hadoop fs minus get.

59
00:03:14,590 --> 00:03:18,080
Hadoop fs minus get is the opposite of Hadoop

60
00:03:18,080 --> 00:03:21,830
fs minus put. It just pulls data from HDFS and

61
00:03:21,830 --> 00:03:24,740
puts it on the local disk. So as you can see, now I have

62
00:03:24,740 --> 00:03:30,000
my local file.txt on my local disk And I can manipulate that however I'd like.

