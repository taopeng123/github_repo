1
00:00:00,220 --> 00:00:03,060
And that's how Map Reduce works. Lets summarize. The

2
00:00:03,060 --> 00:00:06,540
Mappers are just little programs that each deal with

3
00:00:06,540 --> 00:00:08,840
a relatively small amount of data, and work in

4
00:00:08,840 --> 00:00:13,220
parallel. We call that output the Intermediate Records. That's what

5
00:00:13,220 --> 00:00:16,160
we were writing on our index cards. Hadoop deals

6
00:00:16,160 --> 00:00:18,270
with all data in the form of key and

7
00:00:18,270 --> 00:00:22,020
value. So these records are actually keys and values.

8
00:00:22,020 --> 00:00:25,450
In our example, the key was the store name and

9
00:00:25,450 --> 00:00:28,080
the value was the sale total for each

10
00:00:28,080 --> 00:00:31,740
particular piece of input. Once the Mappers have finished,

11
00:00:31,740 --> 00:00:33,740
a phase of Map Reduce called the Shuffle

12
00:00:33,740 --> 00:00:37,290
and Sort takes place. The Shuffle is the movement

13
00:00:37,290 --> 00:00:40,870
of the intermediate records from the Mappers to

14
00:00:40,870 --> 00:00:43,650
the Reducers. The Sort is the fact that the

15
00:00:43,650 --> 00:00:46,440
Reducers will organize these sets of records, in our

16
00:00:46,440 --> 00:00:50,300
case these piles of index cards, into sorted order.

17
00:00:50,300 --> 00:00:55,130
Finally, each Reducer works on one set of records at a time, or one pile of

18
00:00:55,130 --> 00:01:01,850
cards. It gets the key and then a list of all the values. In our case, a store

19
00:01:01,850 --> 00:01:04,260
like New York City. And in all the

20
00:01:04,260 --> 00:01:07,730
sales for that store, it processes those values in

21
00:01:07,730 --> 00:01:10,530
some way. In our case, we were adding up

22
00:01:10,530 --> 00:01:14,450
the sales. Then it writes out the final results.

