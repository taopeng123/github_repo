1
00:00:00,220 --> 00:00:03,520
Files are stored in something called the Hadoop Distributed

2
00:00:03,520 --> 00:00:07,960
File System. Which everyone just refers to as HDFS.

3
00:00:07,960 --> 00:00:10,550
As a developer, this looks very much like a

4
00:00:10,550 --> 00:00:12,900
regular file system. The kind you're used to working

5
00:00:12,900 --> 00:00:15,290
with on a standard machine. But it's helpful to

6
00:00:15,290 --> 00:00:17,930
understand what's going on behind the scenes. So that's

7
00:00:17,930 --> 00:00:20,120
what we're going to talk about here. Imagine we're

8
00:00:20,120 --> 00:00:24,130
going to store a file called mydata.txt. In HDFS.

9
00:00:25,360 --> 00:00:28,210
This file is 150 megabytes. When a file

10
00:00:28,210 --> 00:00:31,140
is loaded into HDFS, it's split into chunks which

11
00:00:31,140 --> 00:00:33,940
we call blocks. Each block is pretty big. The

12
00:00:33,940 --> 00:00:36,830
default is 64 megabytes. Each block is given a

13
00:00:36,830 --> 00:00:40,550
unique name, which is BLK, an underscore, and

14
00:00:40,550 --> 00:00:43,910
a large number. We'll call ours block one, two,

15
00:00:43,910 --> 00:00:47,460
and three. And in our case the first block

16
00:00:47,460 --> 00:00:50,720
is 64 megabytes. The second block is 64 megabytes.

17
00:00:50,720 --> 00:00:53,810
The third block is the remaining 22 megabytes, to make

18
00:00:53,810 --> 00:00:56,960
up our 150 megabyte file. As the file is uploaded

19
00:00:56,960 --> 00:00:59,950
to HDFS, each block will get stored on one node

20
00:00:59,950 --> 00:01:02,990
in the cluster. There's a Damon, or piece of software,

21
00:01:02,990 --> 00:01:06,070
running on each of the machines in the cluster, called

22
00:01:06,070 --> 00:01:09,150
the DataNode. Now clearly we need to know which blocks

23
00:01:09,150 --> 00:01:11,760
make up the original file. And that's handled by a

24
00:01:11,760 --> 00:01:16,060
separate machine, running the Damon called the NameNode. The information

25
00:01:16,060 --> 00:01:19,770
stored on the NameNode is known as the Metadata. That's fine

26
00:01:19,770 --> 00:01:21,890
as far as it goes, but there are some problems with

27
00:01:21,890 --> 00:01:24,960
this. Take a look at the diagram and see if you

28
00:01:24,960 --> 00:01:27,590
can spot where we might run into trouble. For example, let's

29
00:01:27,590 --> 00:01:30,870
say there was network failure between the nodes, or a disk

30
00:01:30,870 --> 00:01:34,670
failure on a DataNode. Do you think it's an issue that

31
00:01:34,670 --> 00:01:38,250
not all of the DataNodes are used to store data? Or

32
00:01:38,250 --> 00:01:41,850
could it be a problem that that the block sizes differ?

33
00:01:41,850 --> 00:01:46,690
Two of them are 64 megabytes, but one was only 22 megabytes. Finally, do

34
00:01:46,690 --> 00:01:49,490
you think it'd be a problem if we had a disk failure on the NameNode?

