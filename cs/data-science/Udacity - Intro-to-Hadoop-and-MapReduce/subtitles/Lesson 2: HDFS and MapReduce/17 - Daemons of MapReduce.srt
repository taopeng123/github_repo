1
00:00:00,490 --> 00:00:03,250
So now we've seen conceptually how MapReduce works. In the

2
00:00:03,250 --> 00:00:05,990
next lesson, we'll talk about how to write the code

3
00:00:05,990 --> 00:00:09,510
to perform these MapReduce jobs on your cluster. But before

4
00:00:09,510 --> 00:00:12,010
that, let's go back to the cluster. Just as we

5
00:00:12,010 --> 00:00:15,720
was with HDFS, there are a set of daemons, which

6
00:00:15,720 --> 00:00:18,710
is just a piece of code, running all the time,

7
00:00:18,710 --> 00:00:21,570
running on each of these machines. There were the data

8
00:00:21,570 --> 00:00:25,750
nodes and the name node. When you run a MapReduce

9
00:00:25,750 --> 00:00:27,950
job, you submit the job to what's called

10
00:00:27,950 --> 00:00:31,394
the Job Tracker. That splits the work into mappers

11
00:00:31,394 --> 00:00:34,740
and reducers. Those mappers and reducers will run

12
00:00:34,740 --> 00:00:38,240
on the other cluster nodes. Running the actual map

13
00:00:38,240 --> 00:00:41,140
and reduce tasks is handled by a daemon

14
00:00:41,140 --> 00:00:44,880
called the TaskTracker. The TaskTracker software will run on

15
00:00:44,880 --> 00:00:47,445
each of these nodes. Note that since the

16
00:00:47,445 --> 00:00:50,860
TaskTracker runs on the same machine as the data

17
00:00:50,860 --> 00:00:53,560
nodes, the Hadoop framework will be able to have

18
00:00:53,560 --> 00:00:56,980
the map tasks work directly on the pieces of data

19
00:00:56,980 --> 00:00:59,240
that are stored on that machine. This will save

20
00:00:59,240 --> 00:01:02,590
a lot of network traffic. As we saw, each Mapper

21
00:01:02,590 --> 00:01:06,010
processes a portion of the input data. That's known

22
00:01:06,010 --> 00:01:09,920
as the input split. And by default, Hadoop use an

23
00:01:09,920 --> 00:01:13,860
HDFS block as the input split for each Mapper.

24
00:01:13,860 --> 00:01:16,220
It will try to make sure that a Mapper works

25
00:01:16,220 --> 00:01:19,600
on data on the same machine. If this green

26
00:01:19,600 --> 00:01:22,990
black, for example, needs processing, then the TaskTracker on this

27
00:01:22,990 --> 00:01:25,540
machine will likely be the one chosen to process

28
00:01:25,540 --> 00:01:29,880
that block. That won't always be possible, because the TaskTrackers

29
00:01:29,880 --> 00:01:32,350
on these three machines that have the green block

30
00:01:32,350 --> 00:01:36,010
could already be busy. In which case, a different node

31
00:01:36,010 --> 00:01:38,400
will be chosen to process the green block, and

32
00:01:38,400 --> 00:01:41,390
it will be streamed over the network. This actually happens

33
00:01:41,390 --> 00:01:43,750
rather rarely. So the Mappers will

34
00:01:43,750 --> 00:01:46,860
read their input data. They'll produce intermediate

35
00:01:46,860 --> 00:01:50,900
data, which the Hadoop framework will pass to the reducers, remember that's the

36
00:01:50,900 --> 00:01:55,390
shuffle and sort. Then the reducers process that data and write their final

37
00:01:55,390 --> 00:02:00,370
output back to HDFS. So let's have Ian run a job on our cluster.

