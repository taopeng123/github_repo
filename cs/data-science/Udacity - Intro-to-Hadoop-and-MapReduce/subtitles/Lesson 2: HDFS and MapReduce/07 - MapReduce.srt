1
00:00:00,290 --> 00:00:02,460
Thanks, Ian. Okay, now that we've seen how data

2
00:00:02,460 --> 00:00:05,640
is stored in HDFS, let's discuss how that data

3
00:00:05,640 --> 00:00:08,430
is processed with MapReduce. Say I had a large

4
00:00:08,430 --> 00:00:11,073
file. Processing that serially from the top to the

5
00:00:11,073 --> 00:00:14,030
bottom could take a long time. Instead, MapReduce is

6
00:00:14,030 --> 00:00:17,200
designed to process data in parallel, so your file

7
00:00:17,200 --> 00:00:21,140
is broken into chunks, and then processed in parallel.

8
00:00:21,140 --> 00:00:23,310
To explain, let's look at a real world scenario.

