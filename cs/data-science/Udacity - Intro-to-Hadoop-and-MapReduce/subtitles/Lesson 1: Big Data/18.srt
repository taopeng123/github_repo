1
00:00:00,400 --> 00:00:05,537
So, let me tell you how Hadoop came to be. About

2
00:00:05,537 --> 00:00:10,200
ten years ago in around 2003, I was working on an Open

3
00:00:10,200 --> 00:00:15,040
Source web search engine called Nutch, and

4
00:00:15,040 --> 00:00:19,550
we knew it needed to be something very scalable, because the Web was you know,

5
00:00:19,550 --> 00:00:25,630
billions of pages. terabytes, petabytes, of data, that we needed

6
00:00:25,630 --> 00:00:30,340
to be able to process, and we set about, you know, doing the best job we

7
00:00:30,340 --> 00:00:36,470
could and it was tough. We got things up and running on four or five machines

8
00:00:36,470 --> 00:00:40,410
not very well, and around that time Google

9
00:00:40,410 --> 00:00:42,990
published some papers. About how they were doing

10
00:00:42,990 --> 00:00:45,870
things internally. Published a paper about their distributed

11
00:00:45,870 --> 00:00:50,980
file system, TFS. and about their processing, framework, MapReduce.

12
00:00:52,740 --> 00:00:57,585
So my partner and I, at the time, in this project, Mike

13
00:00:57,585 --> 00:01:02,700
Cafarella. Said about trying to reimplement these in Open Source. So that

14
00:01:02,700 --> 00:01:08,110
more people could use them than just folks at Google. Took us a couple of years,

15
00:01:08,110 --> 00:01:13,020
and we had Nutch up and running on, instead of four or five

16
00:01:13,020 --> 00:01:18,120
machines, on, 20 to 40 machines. It wasn't perfect,

17
00:01:18,120 --> 00:01:21,740
it was it wasn't, wasn't totally reliable, but it

18
00:01:21,740 --> 00:01:24,550
worked. And we realize that to get it to the

19
00:01:24,550 --> 00:01:27,110
point where it was scaled to thousands of machines,

20
00:01:27,110 --> 00:01:29,850
and be as bullet proof as it needed to be,

21
00:01:29,850 --> 00:01:31,640
would take more than just the two of us,

22
00:01:31,640 --> 00:01:36,210
working part time. Around that time, Yahoo approached me and

23
00:01:36,210 --> 00:01:38,540
said they were interested in investing in this. So

24
00:01:38,540 --> 00:01:41,727
I went to work for Yahoo in January of 2006.

25
00:01:43,180 --> 00:01:47,240
First thing I did there, was, we took the parts of Nutch that were a distributed

26
00:01:47,240 --> 00:01:49,860
computing platform, and put em into a separate

27
00:01:49,860 --> 00:01:54,880
project. A new project christened Hadoop. Over the

28
00:01:54,880 --> 00:01:58,370
next couple years, with, Yahoo's help, and the

29
00:01:58,370 --> 00:02:02,670
help of others. we took Hadoop, and really

30
00:02:02,670 --> 00:02:04,060
got it to the point where it did

31
00:02:04,060 --> 00:02:08,880
scale to petabytes, and running on thousands of processors.

32
00:02:08,880 --> 00:02:15,510
And doing so quite reliably. it spread to lots of companies. and mostly in the

33
00:02:15,510 --> 00:02:21,240
Internet sector, and became quite a success. after that, we, we started to see

34
00:02:21,240 --> 00:02:26,580
a bunch of other projects grow up around it. And Hadoop's grown to be the kernel

35
00:02:26,580 --> 00:02:33,875
of a, which, pretty much an operating system for big data. we've got,

36
00:02:33,875 --> 00:02:41,373
we've got tools that, allow you to, more easily do, MapReduce programming.

37
00:02:41,373 --> 00:02:47,658
so, you can develop using SQL or a data flow language called Pig. and

38
00:02:47,658 --> 00:02:50,026
we've also got the beginnings of

39
00:02:50,026 --> 00:02:54,200
higher-level tools. We've got interactive SQL with

40
00:02:54,200 --> 00:02:59,930
Impala. We've got Search. and so we're really seeing this develop to being a

41
00:02:59,930 --> 00:03:06,125
general purpose platform for data processing. that scale's much better and

42
00:03:06,125 --> 00:03:11,360
that it is much more flexible than anything that's, that's, else is out there.

