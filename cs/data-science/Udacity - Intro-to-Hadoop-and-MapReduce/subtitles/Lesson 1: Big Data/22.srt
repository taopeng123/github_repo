1
00:00:00,110 --> 00:00:03,840
We've been talking about core Hadoop. Which consists of HDFS

2
00:00:03,840 --> 00:00:06,970
and map reduce but since the project was first started, an

3
00:00:06,970 --> 00:00:09,530
awful lot of other software has grown up around it. And

4
00:00:09,530 --> 00:00:12,760
that's what we call the Hadoop Ecosystem. Some of the software

5
00:00:12,760 --> 00:00:14,910
is intended to make it easier to load data into

6
00:00:14,910 --> 00:00:17,790
the Hadoop cluster. Well lots of it designed to make Hadoop

7
00:00:17,790 --> 00:00:20,920
easier to use. For example as you'll see in the next

8
00:00:20,920 --> 00:00:25,140
lesson. Writing Map reduced code isn't completely simple. You need to

9
00:00:25,140 --> 00:00:28,930
know a programming language such as Java, Python, Ruby or

10
00:00:28,930 --> 00:00:31,195
Perl. But there are lots of folks out there who

11
00:00:31,195 --> 00:00:34,410
aren't programmers, but can write SQL queries to access data

12
00:00:34,410 --> 00:00:37,540
in a traditional relational database system, like Sequel Server. And,

13
00:00:37,540 --> 00:00:40,400
of course, lots of business intelligence tools one way to

14
00:00:40,400 --> 00:00:44,670
hook into Hadoop. For that reason, Other open source projects

15
00:00:44,670 --> 00:00:46,580
have been created to make it easier for people to

16
00:00:46,580 --> 00:00:50,550
query their data without knowing how to code. Two key ones

17
00:00:50,550 --> 00:00:53,860
are Hive and Pig. Instead of having to write

18
00:00:53,860 --> 00:00:57,180
macros and reducers, in Hive you just write statements

19
00:00:57,180 --> 00:01:00,670
like this. Which looks very much like standard SQL.

20
00:01:00,670 --> 00:01:04,890
The Hive interpreter turns the SQL into map produced code,

21
00:01:04,890 --> 00:01:07,080
which then runs on the cluster. And an alternative

22
00:01:07,080 --> 00:01:09,735
is Pig, which allows you to write code to

23
00:01:09,735 --> 00:01:12,690
analyse your data in a fairly simple scripting language,

24
00:01:12,690 --> 00:01:15,660
rather than map reduce. Again the code is just turned

25
00:01:15,660 --> 00:01:20,774
into map reduce and run on a cluster. Hive and Pig are great, but they're still

26
00:01:20,774 --> 00:01:23,140
running map reduce jobs. Which as you'll see

27
00:01:23,140 --> 00:01:26,090
can take a reasonable around of time to run.

28
00:01:26,090 --> 00:01:29,420
Especially over large amounts of data. So another

29
00:01:29,420 --> 00:01:33,290
open source project, is called Impala. Impala was developed

30
00:01:33,290 --> 00:01:35,520
as a way to query your data with

31
00:01:35,520 --> 00:01:40,350
SQL, but which directly accesses the data in HDFS.

32
00:01:40,350 --> 00:01:43,960
Rather than needing map reduce. Impala is

33
00:01:43,960 --> 00:01:46,650
optimized for low latency queries. In other

34
00:01:46,650 --> 00:01:49,410
words Impala queries run very quickly, typically

35
00:01:49,410 --> 00:01:52,350
many times faster than Hive, while Hive

36
00:01:52,350 --> 00:01:55,520
is optimized for running long batch processing

37
00:01:55,520 --> 00:01:58,580
jobs. Another project used by many people

38
00:01:58,580 --> 00:02:01,290
is Sqoop. Sqoop takes data from a

39
00:02:01,290 --> 00:02:05,570
traditional relational database, such as Microsoft SQL Server.

40
00:02:05,570 --> 00:02:08,020
And, puts it in HDFS, as the limited

41
00:02:08,020 --> 00:02:10,330
files. So, it can be processed along with other

42
00:02:10,330 --> 00:02:13,930
data on the cluster. Then, there's Flume. Which

43
00:02:13,930 --> 00:02:17,600
injests data as it's generated by external systems. And,

44
00:02:17,600 --> 00:02:20,470
again, puts it into the cluster. HBase is

45
00:02:20,470 --> 00:02:23,780
a real time database, built on top of HDFS.

46
00:02:23,780 --> 00:02:26,750
And there's more. Hue is a graphical front end

47
00:02:26,750 --> 00:02:30,990
to the questor. Oozie is a workflow management tool.

48
00:02:30,990 --> 00:02:34,020
Mahout is a machine learning library. In fact

49
00:02:34,020 --> 00:02:36,630
there are so many ecosystem projects that making

50
00:02:36,630 --> 00:02:38,470
them all talk to one another, and work

51
00:02:38,470 --> 00:02:42,490
well, can be tricky. To make installing and maintaining

52
00:02:42,490 --> 00:02:46,530
a cluster like this easier, Cloudera, the company

53
00:02:46,530 --> 00:02:49,200
we work for, has put together a distribution

54
00:02:49,200 --> 00:02:52,670
of HADOOP called CDH. CDH or the Cloudera

55
00:02:52,670 --> 00:02:56,310
distribution including a patchy HADOOP, takes all the key

56
00:02:56,310 --> 00:03:00,340
ecosystem projects, along with HADOOP itself, and packages them

57
00:03:00,340 --> 00:03:03,820
together so that installation is a really easy process. And

58
00:03:03,820 --> 00:03:05,500
the components are all tested together, so you can

59
00:03:05,500 --> 00:03:08,800
be sure there's no incompatibilities between them. Of course, it's

60
00:03:08,800 --> 00:03:11,740
free and open source, just like Hadoop itself. While

61
00:03:11,740 --> 00:03:15,630
you could install everything from scratch, it's far easier to

62
00:03:15,630 --> 00:03:19,040
use CDH, and that's certainly what we'd recommend. In

63
00:03:19,040 --> 00:03:21,520
the next lesson, in fact, you'll be downloading and running

64
00:03:21,520 --> 00:03:26,350
a virtual machine, which has CDH installed. For more information on the

65
00:03:26,350 --> 00:03:28,250
Hadoop ecosystem and how each of

66
00:03:28,250 --> 00:03:30,840
these components works, see the instructor notes.

