We've been talking about core Hadoop. Which consists of HDFS and map reduce but since the project was first started, an awful lot of other software has grown up around it. And that's what we call the Hadoop Ecosystem. Some of the software is intended to make it easier to load data into the Hadoop cluster. Well lots of it designed to make Hadoop easier to use. For example as you'll see in the next lesson. Writing Map reduced code isn't completely simple. You need to know a programming language such as Java, Python, Ruby or Perl. But there are lots of folks out there who aren't programmers, but can write SQL queries to access data in a traditional relational database system, like Sequel Server. And, of course, lots of business intelligence tools one way to hook into Hadoop. For that reason, Other open source projects have been created to make it easier for people to query their data without knowing how to code. Two key ones are Hive and Pig. Instead of having to write macros and reducers, in Hive you just write statements like this. Which looks very much like standard SQL.  The Hive interpreter turns the SQL into map produced code, which then runs on the cluster. And an alternative is Pig, which allows you to write code to analyse your data in a fairly simple scripting language, rather than map reduce. Again the code is just turned into map reduce and run on a cluster. Hive and Pig are great, but they're still running map reduce jobs. Which as you'll see can take a reasonable around of time to run.  Especially over large amounts of data. So another open source project, is called Impala. Impala was developed as a way to query your data with SQL, but which directly accesses the data in HDFS.  Rather than needing map reduce. Impala is optimized for low latency queries. In other words Impala queries run very quickly, typically many times faster than Hive, while Hive is optimized for running long batch processing jobs. Another project used by many people is Sqoop. Sqoop takes data from a traditional relational database, such as Microsoft SQL Server.  And, puts it in HDFS, as the limited files. So, it can be processed along with other data on the cluster. Then, there's Flume. Which injests data as it's generated by external systems. And, again, puts it into the cluster. HBase is a real time database, built on top of HDFS.  And there's more. Hue is a graphical front end to the questor. Oozie is a workflow management tool.  Mahout is a machine learning library. In fact there are so many ecosystem projects that making them all talk to one another, and work well, can be tricky. To make installing and maintaining a cluster like this easier, Cloudera, the company we work for, has put together a distribution of HADOOP called CDH. CDH or the Cloudera distribution including a patchy HADOOP, takes all the key ecosystem projects, along with HADOOP itself, and packages them together so that installation is a really easy process. And the components are all tested together, so you can be sure there's no incompatibilities between them. Of course, it's free and open source, just like Hadoop itself. While you could install everything from scratch, it's far easier to use CDH, and that's certainly what we'd recommend. In the next lesson, in fact, you'll be downloading and running a virtual machine, which has CDH installed. For more information on the Hadoop ecosystem and how each of these components works, see the instructor notes.
