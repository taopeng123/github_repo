The core Hadoop project consists of a way to store data, known as the Hadoop distributed file system, or HDFS. And a way to process data with MapReduce. The key concept is that we split the data up and store it across the collection of machines known as a cluster. Then when we want to process the data, we process it where it's actually stored.  Rather than retrieving the data from a central server, the data's already on the cluster, so we can process it in place.  You can add more machines to the cluster as the amount of data you're storing grows.  And, indeed, many people start with just a few machines, and add more as they're needed.  The machines in your cluster don't need to be anything particularly high end. Although most clusters are built using rack-mounted servers, they are typically mid-range servers, rather than top of the range equipment.
