1
00:00:01,290 --> 00:00:03,810
So, Sarah's just shown you the code. And one of

2
00:00:03,810 --> 00:00:06,510
the nice things about using hadoop streaming is that it's

3
00:00:06,510 --> 00:00:10,010
really easy to test your code outside of hadoop. So,

4
00:00:10,010 --> 00:00:12,980
let's take a look at how to do that. Our mapper

5
00:00:12,980 --> 00:00:16,870
takes input from standard input. So, in order to test

6
00:00:16,870 --> 00:00:19,060
it, we can just run it from the command line

7
00:00:19,060 --> 00:00:22,000
and type data in to test it. Here, I'm just

8
00:00:22,000 --> 00:00:26,990
typing, as standard input, a couple of lines of sample data.

9
00:00:26,990 --> 00:00:29,910
Six fields, separated by tabs. And then when

10
00:00:29,910 --> 00:00:32,560
I hit Ctrl+D, to simulate the end of

11
00:00:32,560 --> 00:00:35,260
input, you can see that the mapper outputs

12
00:00:35,260 --> 00:00:39,650
the results, just as I'd expect. Even better, we

13
00:00:39,650 --> 00:00:44,340
can just build a small sample data file, and pipe that into the mapper. So let's

14
00:00:44,340 --> 00:00:47,100
do that. I'm going to just take the

15
00:00:47,100 --> 00:00:52,140
first 50 lines of purchases.txt and save those into

16
00:00:52,140 --> 00:00:55,915
a test file, which I'll call testfile. Now,

17
00:00:55,915 --> 00:00:58,960
I can just pipe that to the mapper by

18
00:00:58,960 --> 00:01:03,420
saying cat testfile, pipe that to mapper.py. And

19
00:01:03,420 --> 00:01:06,790
that gives mapper that data a standard input, and

20
00:01:06,790 --> 00:01:08,960
as you can see, the mapper produces its

21
00:01:08,960 --> 00:01:12,360
output again to the command line. If we have

22
00:01:12,360 --> 00:01:14,370
problems, we could just go back and edit the

23
00:01:14,370 --> 00:01:17,470
mapper until it works. It's really nice and fast

24
00:01:17,470 --> 00:01:19,800
to be able to do this without having to run

25
00:01:19,800 --> 00:01:23,210
a complete hadoop job every time during the development phase.

26
00:01:24,450 --> 00:01:27,240
We can do a similar thing with the reducer. It's

27
00:01:27,240 --> 00:01:30,050
expecting a set of lines, each of which looks like

28
00:01:30,050 --> 00:01:33,130
the store name then a tab then the value. So,

29
00:01:33,130 --> 00:01:35,660
again, we can create a sample file which looks like

30
00:01:35,660 --> 00:01:38,390
that and pass it in to the reducer. But even

31
00:01:38,390 --> 00:01:42,620
nicer, we can test the entire pipeline. Remember that the mapper's

32
00:01:42,620 --> 00:01:45,840
output is sorted by the hadoop framework, and then passed

33
00:01:45,840 --> 00:01:48,950
to the reducer. So, we can simulate the entire thing

34
00:01:48,950 --> 00:01:52,410
on the Unix command line, like this. I cat testfile.

35
00:01:52,410 --> 00:01:54,810
I pipe it to the mapper. I then pass that

36
00:01:54,810 --> 00:01:58,080
to the Unix sort command, and pass that output to

37
00:01:58,080 --> 00:02:01,890
the reducer. When I run that, that simulates the entire

38
00:02:01,890 --> 00:02:05,540
map followed by shuffle and sort, followed by reduced phase,

39
00:02:05,540 --> 00:02:07,770
and as you can see, I've got the output from

40
00:02:07,770 --> 00:02:11,008
the reducer. So, now that we've tested this on the

41
00:02:11,008 --> 00:02:14,160
command line we can test it on the cluster. The best

42
00:02:14,160 --> 00:02:17,170
practice when you're developing map reduce jobs is first to

43
00:02:17,170 --> 00:02:20,550
test locally with a small data set before you run your

44
00:02:20,550 --> 00:02:24,460
code on the entire, huge, set of data. So, now

45
00:02:24,460 --> 00:02:27,020
that we've tested on the command line, we can test our

46
00:02:27,020 --> 00:02:30,460
code on the cluster. Best practice, when you're developing that

47
00:02:30,460 --> 00:02:33,300
reduced jobs, is to first test with a small data set.

48
00:02:33,300 --> 00:02:35,740
Before you run your code on your entire

49
00:02:35,740 --> 00:02:37,960
huge set of data. But, we're already, pretty,

50
00:02:37,960 --> 00:02:40,080
confident here. So, let's just run the thing

51
00:02:40,080 --> 00:02:44,300
on the wholepurchases.txt file. We'll use the HS

52
00:02:44,300 --> 00:02:47,380
alias to save some typing. I specified my

53
00:02:47,380 --> 00:02:51,670
mapper, my reducer, my input directory, and a

54
00:02:51,670 --> 00:02:54,190
new output directory which we'll call output two.

55
00:02:55,770 --> 00:02:58,289
And off hadoop goes. It starts running the job.

56
00:02:59,580 --> 00:03:03,930
It turns out that we can see what's going on on the cluster by taking a look at

57
00:03:03,930 --> 00:03:08,930
the hadoop job tracker web user interface. So, you point your

58
00:03:08,930 --> 00:03:14,160
web browser at the job tracker, which on our machine is just local host on

59
00:03:14,160 --> 00:03:19,300
port 50030. And here you can see that this just one running

60
00:03:19,300 --> 00:03:24,800
job. If we click on the job name, then that takes us to a page,

61
00:03:24,800 --> 00:03:27,390
which shows us the progress of the job, and a

62
00:03:27,390 --> 00:03:30,730
lot of other interesting information as well. We can drill

63
00:03:30,730 --> 00:03:34,120
down and look at the individual map, or reduce tasks

64
00:03:34,120 --> 00:03:37,110
just by clicking on the words map or reduce. And

65
00:03:37,110 --> 00:03:39,830
here we can see that two tasks have completed, two

66
00:03:39,830 --> 00:03:42,270
tasks are still running. If I click on one of

67
00:03:42,270 --> 00:03:45,860
the completed tasks, I can even go as far to

68
00:03:45,860 --> 00:03:50,240
look at the logs from that task. So, here's our job.

69
00:03:50,240 --> 00:03:53,090
We're now 25% of the way through our reducers.

70
00:03:53,090 --> 00:03:54,890
As you can see, we get graphs at the

71
00:03:54,890 --> 00:03:57,360
bottom of this page to show us exactly what's

72
00:03:57,360 --> 00:04:01,550
happening with the job. So, we nearly finish with

73
00:04:01,550 --> 00:04:05,370
the job, when the job's finished, hadoop fs minus

74
00:04:05,370 --> 00:04:08,590
ls of output 2 shows me the just I'd

75
00:04:08,590 --> 00:04:16,430
expect is a part dash 00000 file in that and just as we did before we can hadoop

76
00:04:16,430 --> 00:04:26,042
fs minus cat that file to see our actual results.

