1
00:00:00,570 --> 00:00:02,820
Here's our mapper code. Let's look at it line by

2
00:00:02,820 --> 00:00:06,930
line. We're going to loop around standard input, which gives

3
00:00:06,930 --> 00:00:08,750
us a line at a time. Of course, the line

4
00:00:08,750 --> 00:00:11,570
will have a new line character so let's strip that off,

5
00:00:11,570 --> 00:00:14,140
plus any other white space around the line. Since our

6
00:00:14,140 --> 00:00:17,890
line is tab delimited, let's split untab at the same

7
00:00:17,890 --> 00:00:21,710
time. This gives us an array, called data. So now

8
00:00:21,710 --> 00:00:25,840
we'll take the array, data, and assign it into individual variables

9
00:00:25,840 --> 00:00:30,190
for date, time, store, item, cost, and payment. Lastly, we'll

10
00:00:30,190 --> 00:00:33,360
print the items we care about, which is store and cost.

11
00:00:34,450 --> 00:00:36,500
As you use Hadoop more and more, you'll discover that the

12
00:00:36,500 --> 00:00:38,890
more data you have, the more likely you are to encounter

13
00:00:38,890 --> 00:00:42,440
weirdness in the data. Lines could be malformed. There could

14
00:00:42,440 --> 00:00:46,020
be strange log messages in the data, and many other cases

15
00:00:46,020 --> 00:00:48,840
that you might not even imagine. So we should make sure

16
00:00:48,840 --> 00:00:51,100
that no matter what kind of data we get, the mapper

17
00:00:51,100 --> 00:00:54,250
can continue working. You don't want to be halfway through a two

18
00:00:54,250 --> 00:00:57,510
terabyte job when it dies. So let's go back and add

19
00:00:57,510 --> 00:01:01,050
some defensive programming, to make sure things don't break, even if

20
00:01:01,050 --> 00:01:02,840
you get a strange line in the middle of your data.

