1
00:00:00,460 --> 00:00:02,469
Now we're going to talk about a pretty cool new

2
00:00:02,469 --> 00:00:05,300
tool. Something that goes between the map and the reduce

3
00:00:05,300 --> 00:00:08,530
phase, called the combiner. Before we do that, let's review

4
00:00:08,530 --> 00:00:10,780
how you may have calculated the mean in the last

5
00:00:10,780 --> 00:00:14,050
problem. And I don't know exactly what you did,

6
00:00:14,050 --> 00:00:16,440
but one thing you may have done is, something like

7
00:00:16,440 --> 00:00:21,040
this. First, you probably had your mapper go through the

8
00:00:21,040 --> 00:00:24,390
data. I should say your mappers, go through the data,

9
00:00:25,400 --> 00:00:29,500
and output, key value pairs that looked

10
00:00:29,500 --> 00:00:34,110
something like, day of week, amount spent. Then,

11
00:00:34,110 --> 00:00:39,050
for each day of the week, your reducer probably kept a total of the sum and

12
00:00:39,050 --> 00:00:42,140
a count. So actually maybe your value here,

13
00:00:42,140 --> 00:00:44,520
was the amount of money spent. And then

14
00:00:44,520 --> 00:00:48,080
also a one, to help increment the count.

15
00:00:48,080 --> 00:00:51,250
And then you probably would divide the sum

16
00:00:51,250 --> 00:00:53,560
by the count, and it looks pretty good. But

17
00:00:53,560 --> 00:00:57,230
remember, net produce is happening on this whole network

18
00:00:57,230 --> 00:01:00,300
of data nodes. Each of these little boxes, we'll

19
00:01:00,300 --> 00:01:03,470
pretend is a data node. And your data's spread

20
00:01:03,470 --> 00:01:06,740
out across all of these different machines. And now

21
00:01:06,740 --> 00:01:09,270
let's think about where each of these three steps

22
00:01:09,270 --> 00:01:12,360
took place. So, we're not going to focus on what

23
00:01:12,360 --> 00:01:16,250
happened but where it happened. Well, for step one,

24
00:01:16,250 --> 00:01:18,430
the mappers were all over the place. There were

25
00:01:18,430 --> 00:01:22,150
mappers, anywhere there is relevant data. But, for any

26
00:01:22,150 --> 00:01:25,210
given day, all of the reduction had to happen

27
00:01:25,210 --> 00:01:27,890
on a single machine, where that reducer was living.

28
00:01:27,890 --> 00:01:31,310
And actually, that's enough, we've already exposed a problem.

29
00:01:31,310 --> 00:01:34,160
Because if there's a lot of data, even if

30
00:01:34,160 --> 00:01:37,470
each individual output was simply a day and a

31
00:01:37,470 --> 00:01:41,070
number, depending on the number of records we have,

32
00:01:41,070 --> 00:01:44,360
transferring all of this, all this data to

33
00:01:44,360 --> 00:01:47,050
this machine, with reducer lift, that, that's a lot

34
00:01:47,050 --> 00:01:49,480
of bandwidth. That's going to be a lot of traffic

35
00:01:49,480 --> 00:01:51,760
on our network that we don't necessarily want, and

36
00:01:51,760 --> 00:01:53,660
it doesn't actually seem like we need to have.

37
00:01:54,850 --> 00:01:57,530
So what if we could do some of this

38
00:01:57,530 --> 00:02:02,750
reduction locally? What if on each machine, before we

39
00:02:02,750 --> 00:02:06,470
send our key-value pairs off, off to be reduced,

40
00:02:06,470 --> 00:02:08,990
what if we could do some pre-reduction? And it

41
00:02:08,990 --> 00:02:12,378
turns out we can. And that pre-reduction happens with these

42
00:02:12,378 --> 00:02:16,420
things called combiners. And before you practice using combiners

43
00:02:16,420 --> 00:02:18,320
let me try to sell you a bit more on

44
00:02:18,320 --> 00:02:20,750
their value. So as you know, when you use

45
00:02:20,750 --> 00:02:24,240
a map reduce job you get some output like this.

46
00:02:24,240 --> 00:02:27,210
This is where it's displaying how much mapping has happened

47
00:02:27,210 --> 00:02:30,230
and how much reducing. But you get this tracking URL.

48
00:02:31,440 --> 00:02:35,770
And if you open that URL in a browser you get a job page, and that job page

49
00:02:35,770 --> 00:02:38,290
looks something like this. And actually if you scroll

50
00:02:38,290 --> 00:02:41,430
down below here you'll see some more interesting statistics.

51
00:02:41,430 --> 00:02:44,910
And in fact we ran two jobs. They were

52
00:02:44,910 --> 00:02:49,490
identical jobs, except one implemented combiners, so they were

53
00:02:49,490 --> 00:02:52,720
reducing locally as much as possible, before shuttling that

54
00:02:52,720 --> 00:02:56,270
data off to that machine that held the end reducer.

55
00:02:56,270 --> 00:02:59,470
Let's compare the statistics from those two jobs. The

56
00:02:59,470 --> 00:03:02,150
one with combiners, the one without. So let's compare what

57
00:03:02,150 --> 00:03:05,320
happened on these job pages. The one without the combiner

58
00:03:05,320 --> 00:03:08,090
versus the one with combiner. So, a lot of stuff

59
00:03:08,090 --> 00:03:11,350
is the same. The mappers to the same amount,

60
00:03:11,350 --> 00:03:16,260
the input matches the output for without and with. That's

61
00:03:16,260 --> 00:03:18,380
as expected. In fact, a lot of things look pretty

62
00:03:18,380 --> 00:03:21,340
similar. But where things get really interesting is down here

63
00:03:21,340 --> 00:03:27,375
at reduce input records. So, the reducers, had to handle the whole

64
00:03:27,375 --> 00:03:32,929
4 million records that the macro had put when there is no combiner. But when

65
00:03:32,929 --> 00:03:35,800
there was a combiner, these 4 million

66
00:03:35,800 --> 00:03:39,280
mapped output records are combined into 412

67
00:03:39,280 --> 00:03:44,000
reduced input records, or reducer input records.

68
00:03:44,000 --> 00:03:47,020
That's pretty amazing. That's a huge change.

69
00:03:47,020 --> 00:03:51,220
In fact, the reducers had to shuffle far fewer bytes. And for time

70
00:03:51,220 --> 00:03:56,530
spent, with the combiner this job took 31 seconds to run versus without where

71
00:03:56,530 --> 00:04:02,020
it took 43. That's a significant difference. So you can really use combiners to

72
00:04:02,020 --> 00:04:04,520
improve the efficiency of your map reduced

73
00:04:04,520 --> 00:04:06,740
jobs. And we're going to try that next.

