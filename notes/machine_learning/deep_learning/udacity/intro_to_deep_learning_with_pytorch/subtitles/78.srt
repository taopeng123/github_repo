1
00:00:00,000 --> 00:00:02,685
Here is my solution for

2
00:00:02,685 --> 00:00:04,695
this multi-layer neural network for

3
00:00:04,695 --> 00:00:08,505
classifying handwritten digits from the MNIST dataset.

4
00:00:08,505 --> 00:00:11,850
So, here I've defined our activation function like before, so,

5
00:00:11,850 --> 00:00:15,465
again this is the sigmoid function and here I'm flattening the images.

6
00:00:15,465 --> 00:00:19,860
So, remember how to reshape your tensors.

7
00:00:19,860 --> 00:00:21,285
So, here I'm using.view.

8
00:00:21,285 --> 00:00:23,580
So, I'm just grabbing the batch size.

9
00:00:23,580 --> 00:00:25,185
So, images.shape.

10
00:00:25,185 --> 00:00:27,720
The first element zero here,

11
00:00:27,720 --> 00:00:32,925
gives you the number of batches in your images tensor.

12
00:00:32,925 --> 00:00:35,460
So, I want to keep the number of batches the same,

13
00:00:35,460 --> 00:00:38,385
but I want to flatten the rest of the dimensions.

14
00:00:38,385 --> 00:00:41,150
So, to do this, you actually can just put in negative one.

15
00:00:41,150 --> 00:00:44,600
So, I could type in 784 here but a kind of

16
00:00:44,600 --> 00:00:48,410
a shortcut way to do this is to put in negative one.

17
00:00:48,410 --> 00:00:54,980
So, basically what this does is it takes 64 as your batch size here and then when you

18
00:00:54,980 --> 00:00:58,100
put a negative one it sees this and then it just chooses

19
00:00:58,100 --> 00:01:02,270
the appropriate size to get the total number of elements.

20
00:01:02,270 --> 00:01:07,220
So, it'll work out on its own that it needs to make the second dimension,

21
00:01:07,220 --> 00:01:10,220
784 so that the number of elements

22
00:01:10,220 --> 00:01:13,675
after reshaping matches the number elements before reshaping.

23
00:01:13,675 --> 00:01:17,135
So, this is just a kind of quick way to flatten

24
00:01:17,135 --> 00:01:21,245
a tensor without having to know what the second dimension used to be.

25
00:01:21,245 --> 00:01:25,025
Then here I'm just creating our weight and bias parameters.

26
00:01:25,025 --> 00:01:33,020
So, we know that we want an input of 784 units and we want 256 hidden units.

27
00:01:33,020 --> 00:01:37,070
So, our first weight matrix is going to be 784 by 256.

28
00:01:37,070 --> 00:01:41,360
Then, we need a bias term for each of our hidden units.

29
00:01:41,360 --> 00:01:45,120
So we have 256 bias terms here in b1.

30
00:01:45,120 --> 00:01:48,830
Then, for our second weight's going from the hidden layer to

31
00:01:48,830 --> 00:01:53,570
the output layer we want 256 inputs to 10 outputs.

32
00:01:53,570 --> 00:01:55,685
Then again 10 elements in our bias.

33
00:01:55,685 --> 00:02:02,630
Before we can do a matrix multiplication of our inputs with the first set of weights,

34
00:02:02,630 --> 00:02:04,190
our first weight parameters,

35
00:02:04,190 --> 00:02:07,640
add in the bias terms and passes

36
00:02:07,640 --> 00:02:10,940
through our activation functions so that gives us the output of our hidden layer.

37
00:02:10,940 --> 00:02:15,395
Then we can use that as the input to our output layer, and again,

38
00:02:15,395 --> 00:02:19,970
a matrix multiplication with a second set of weights and the second set of bias terms.

39
00:02:19,970 --> 00:02:23,135
This gives us the output of our network. All right.

40
00:02:23,135 --> 00:02:26,210
So, if we look at the output of this network,

41
00:02:26,210 --> 00:02:29,450
we see that we get those 64.

42
00:02:29,450 --> 00:02:32,630
So, first let me print the shape just to make sure we did that right.

43
00:02:32,630 --> 00:02:40,165
So, 64 rows for one of each of our sort of input examples and then 10 values,

44
00:02:40,165 --> 00:02:42,530
so, basically it's a value that's trying to

45
00:02:42,530 --> 00:02:46,415
say this image belongs to this class like this digit.

46
00:02:46,415 --> 00:02:50,530
So, we can inspect our output tensor and see what's going on here.

47
00:02:50,530 --> 00:02:53,640
So, we see these values are just sort of all over the place.

48
00:02:53,640 --> 00:02:57,145
So, you got like six and negative 11 and so on.

49
00:02:57,145 --> 00:03:00,470
But we really want is we want our network to kind of tell

50
00:03:00,470 --> 00:03:04,160
us the probability of our different classes given some image.

51
00:03:04,160 --> 00:03:09,500
So, kind of we want to pass in an image to our network and then the output should be

52
00:03:09,500 --> 00:03:12,050
a probability distribution that tells us which are

53
00:03:12,050 --> 00:03:16,565
the most likely classes or digits that belong to this image.

54
00:03:16,565 --> 00:03:19,255
So, if it's the image of a six,

55
00:03:19,255 --> 00:03:21,830
then we want a probability distribution where most of

56
00:03:21,830 --> 00:03:24,995
the probability is in the sixth class.

57
00:03:24,995 --> 00:03:27,115
So, it's telling us that it's a number six.

58
00:03:27,115 --> 00:03:29,375
So, we want it to look something like this.

59
00:03:29,375 --> 00:03:31,520
This is like a class probability.

60
00:03:31,520 --> 00:03:33,725
So, it's telling us the probabilities of

61
00:03:33,725 --> 00:03:37,815
the different classes given this image that we're passing in.

62
00:03:37,815 --> 00:03:39,605
So, you can see that the probability for

63
00:03:39,605 --> 00:03:41,450
each of these different classes is roughly the same,

64
00:03:41,450 --> 00:03:43,985
and so it's a uniform distribution.

65
00:03:43,985 --> 00:03:46,310
This represents an untrained network,

66
00:03:46,310 --> 00:03:49,165
so it's a uniform probability distribution.

67
00:03:49,165 --> 00:03:51,050
It's because it hasn't seen any data yet,

68
00:03:51,050 --> 00:03:53,495
so it hasn't learned anything about these images.

69
00:03:53,495 --> 00:03:57,005
So, whenever you give an image to it,

70
00:03:57,005 --> 00:04:00,950
it doesn't know what it is so it's just going to give an equal probability to each class,

71
00:04:00,950 --> 00:04:03,370
regardless of the image that you pass in.

72
00:04:03,370 --> 00:04:07,355
So, what we want is we want the output of our network to be

73
00:04:07,355 --> 00:04:10,670
a probability distribution that gives us the probability

74
00:04:10,670 --> 00:04:14,960
that the image belongs to any one of our classes.

75
00:04:14,960 --> 00:04:17,945
So for this, we use the softmax function.

76
00:04:17,945 --> 00:04:20,540
So what this looks like is the exponential.

77
00:04:20,540 --> 00:04:22,670
So,you pass in your 10 values.

78
00:04:22,670 --> 00:04:24,455
So, for each of those values,

79
00:04:24,455 --> 00:04:26,765
we calculate the exponential of that value divided by

80
00:04:26,765 --> 00:04:29,555
the sum of exponentials of all the values.

81
00:04:29,555 --> 00:04:33,140
So, what this does is it actually kind of squishes each

82
00:04:33,140 --> 00:04:36,680
of the input values x between zero and one,

83
00:04:36,680 --> 00:04:39,470
and then also normalizes all the values so that

84
00:04:39,470 --> 00:04:42,870
the sum of each of the probabilities is one.

85
00:04:42,870 --> 00:04:44,940
So, the entire thing sums up to one.

86
00:04:44,940 --> 00:04:47,525
So, this gives you a proper probability distribution.

87
00:04:47,525 --> 00:04:50,225
What I want you to do here is actually implement

88
00:04:50,225 --> 00:04:54,310
a function called softmax that performs this calculation.

89
00:04:54,310 --> 00:04:57,800
So, what you're going to be doing is taking the output from

90
00:04:57,800 --> 00:05:02,705
this simple neural network and has shaped 64 by 10 and pass it through

91
00:05:02,705 --> 00:05:06,695
a dysfunction softmax and make sure it calculates

92
00:05:06,695 --> 00:05:11,660
the probability distribution for each of the different examples that we passed in.

93
00:05:11,660 --> 00:05:13,130
Right? Good luck.

