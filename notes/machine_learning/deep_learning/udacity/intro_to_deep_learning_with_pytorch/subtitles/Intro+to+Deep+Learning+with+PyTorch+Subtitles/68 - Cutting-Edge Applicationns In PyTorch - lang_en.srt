1
00:00:00,000 --> 00:00:04,185
When you talk about the pace of research and thinking about deploying models,

2
00:00:04,185 --> 00:00:07,530
are there projects that you find really exciting,

3
00:00:07,530 --> 00:00:12,075
that are happening in PyTorch uniquely or not uniquely?

4
00:00:12,075 --> 00:00:16,050
There are a lot of products I'm interested in,

5
00:00:16,050 --> 00:00:18,690
just because they're very crazy ideas.

6
00:00:18,690 --> 00:00:20,025
Those are my favorite as well.

7
00:00:20,025 --> 00:00:26,175
Yeah. So, there was this one paper written by one person,

8
00:00:26,175 --> 00:00:28,845
Andrew Brock and it's called SMASH,

9
00:00:28,845 --> 00:00:33,030
where one neural network would generate

10
00:00:33,030 --> 00:00:38,070
the weights that would be powered by for another neural network.

11
00:00:38,070 --> 00:00:38,685
Okay.

12
00:00:38,685 --> 00:00:42,870
So, it was like this neural network architects of search except that

13
00:00:42,870 --> 00:00:47,840
it's you're exploring all possible neural network architectures

14
00:00:47,840 --> 00:00:52,130
at once because the first neural network is generating the architecture and

15
00:00:52,130 --> 00:00:57,035
the weights for your neural network.

16
00:00:57,035 --> 00:00:58,970
That is the final neural network.

17
00:00:58,970 --> 00:00:59,390
I see.

18
00:00:59,390 --> 00:01:01,980
It is very funky.

19
00:01:02,680 --> 00:01:09,630
It took me a long time to even figure out like how he managed to do it in code.

20
00:01:09,630 --> 00:01:14,830
It was always in PyTorch and it looked very like, oh my god.

21
00:01:14,830 --> 00:01:18,785
It's like you give someone a calculator and they make Mario Kart out of it.

22
00:01:18,785 --> 00:01:22,085
Yeah or like a calculator programs another calculator or something.

23
00:01:22,085 --> 00:01:27,515
Yeah. So, there are several other examples.

24
00:01:27,515 --> 00:01:33,965
There's the [inaudible] from Facebook that have been closely following.

25
00:01:33,965 --> 00:01:37,490
It was more of a text-to-text processing model.

26
00:01:37,490 --> 00:01:41,635
So, you give some text and then it would generate another text.

27
00:01:41,635 --> 00:01:45,260
Initially, it was used for machine translation and

28
00:01:45,260 --> 00:01:48,590
all the standard language modeling, that kind of stuff.

29
00:01:48,590 --> 00:01:56,030
But more recently, some of the researchers Angela Fan and her collaborators,

30
00:01:56,030 --> 00:02:00,860
they published what you call hierarchical story generation.

31
00:02:00,860 --> 00:02:05,070
So, you would seed a story that like,

32
00:02:05,070 --> 00:02:09,840
"Hey, I want a story of a boy swimming in a pond."

33
00:02:09,840 --> 00:02:12,705
Then it would actually like generate a story.

34
00:02:12,705 --> 00:02:13,860
That's interesting.

35
00:02:13,860 --> 00:02:15,300
Of the dead flood.

36
00:02:15,300 --> 00:02:18,540
Yeah. A character like a vague situation.

37
00:02:18,540 --> 00:02:22,530
I find that very fun and interesting.

38
00:02:22,530 --> 00:02:24,360
That is that is also,

39
00:02:24,360 --> 00:02:27,685
I think text analysis can be really interesting when it comes to like,

40
00:02:27,685 --> 00:02:30,770
how do you actually like formalize ideas of

41
00:02:30,770 --> 00:02:35,525
understanding the context of something or where it might be going in the future.

42
00:02:35,525 --> 00:02:38,270
I think that's pretty good. I think too,

43
00:02:38,270 --> 00:02:42,320
when I look at research and some of my favorite papers are paired

44
00:02:42,320 --> 00:02:48,125
with like openly available GitHub repositories of work.

45
00:02:48,125 --> 00:02:55,010
We actually have someone [inaudible] who was doing the cycle again of formulation and his code is

46
00:02:55,010 --> 00:02:58,460
publicly available on GitHub and implemented in PyTorch and it's

47
00:02:58,460 --> 00:03:02,260
also just like very readable where you look at something you can clearly see like,

48
00:03:02,260 --> 00:03:06,050
here are the inputs, here is what's happening in as far as it being transformed,

49
00:03:06,050 --> 00:03:07,550
and here are the desired outputs.

50
00:03:07,550 --> 00:03:11,300
I think that's something that I really like to see and it's

51
00:03:11,300 --> 00:03:16,390
something that I see more with PyTorch than other languages.

