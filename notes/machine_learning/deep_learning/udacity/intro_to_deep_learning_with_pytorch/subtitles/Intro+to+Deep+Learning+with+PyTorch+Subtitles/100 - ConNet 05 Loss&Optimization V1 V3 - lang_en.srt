1
00:00:00,000 --> 00:00:03,365
Now that we've decided on the structure of our MLP,

2
00:00:03,365 --> 00:00:07,635
let's talk a bit about how this entire thing will actually learn from the MS data.

3
00:00:07,635 --> 00:00:10,515
What happens when it actually sees an input image.

4
00:00:10,515 --> 00:00:13,380
Take this input image of a two for instance.

5
00:00:13,380 --> 00:00:16,320
Say we feed this image into the network and when we do so,

6
00:00:16,320 --> 00:00:19,110
we get these ten class scores for my output layer.

7
00:00:19,110 --> 00:00:21,750
Again, a higher score means that the network is more

8
00:00:21,750 --> 00:00:25,030
certain that the input image is of that particular class.

9
00:00:25,030 --> 00:00:28,590
Four is the largest value here and negative two is the smallest.

10
00:00:28,590 --> 00:00:31,830
So, the network believes that the image input is least likely

11
00:00:31,830 --> 00:00:35,280
to be a 103 and most likely to be an eight,

12
00:00:35,280 --> 00:00:36,595
but this is incorrect.

13
00:00:36,595 --> 00:00:39,130
We can see that the correct label is two.

14
00:00:39,130 --> 00:00:42,715
So, what we can do is tell our network to learn from this mistake.

15
00:00:42,715 --> 00:00:47,540
As a network trains, we measure any mistakes that it makes using a loss function,

16
00:00:47,540 --> 00:00:51,585
whose job is to measure the difference between the predicted and true class labels.

17
00:00:51,585 --> 00:00:53,410
Then using back propagation,

18
00:00:53,410 --> 00:00:57,275
we can compute the gradient of the loss with respect to the models' weights.

19
00:00:57,275 --> 00:01:00,305
In this way, we quantify how bad a particular weight

20
00:01:00,305 --> 00:01:04,555
is and find out which weights in the network are responsible for any errors.

21
00:01:04,555 --> 00:01:06,650
Finally, using that calculation,

22
00:01:06,650 --> 00:01:09,110
we can choose an optimization function like

23
00:01:09,110 --> 00:01:13,055
gradient descent to give us a way to calculate a better weight value.

24
00:01:13,055 --> 00:01:15,755
Towards this goal, the first thing we'll need to do

25
00:01:15,755 --> 00:01:18,695
is make this output layer a bit more interpretable.

26
00:01:18,695 --> 00:01:20,870
What's commonly done is to apply

27
00:01:20,870 --> 00:01:25,600
a softtmax activation function to convert these scores into probabilities.

28
00:01:25,600 --> 00:01:28,520
To apply a softmax function to this output layer,

29
00:01:28,520 --> 00:01:32,740
we begin by evaluating the exponential function at each of the scores,

30
00:01:32,740 --> 00:01:35,200
then we add up all of the values.

31
00:01:35,200 --> 00:01:37,945
Let's denote this sum with a capital "S".

32
00:01:37,945 --> 00:01:41,020
Then we divide each of these values by the sum.

33
00:01:41,020 --> 00:01:42,810
When you plug in all of the math,

34
00:01:42,810 --> 00:01:44,525
you get these 10 values.

35
00:01:44,525 --> 00:01:47,300
Now each value yields the probability that

36
00:01:47,300 --> 00:01:50,075
the image depicts its corresponding image class.

37
00:01:50,075 --> 00:01:52,925
For instance, the network believes that the image shows an

38
00:01:52,925 --> 00:01:56,090
eight with 44.1 percent probability.

39
00:01:56,090 --> 00:01:58,310
Remember that the input to the network was

40
00:01:58,310 --> 00:02:00,905
an image of a handwritten two and yet the network

41
00:02:00,905 --> 00:02:06,125
incorrectly predicts that the image shows a two with only 16.2 percent probability.

42
00:02:06,125 --> 00:02:10,450
Now our goal is to update the weights of the network in response to this mistake,

43
00:02:10,450 --> 00:02:12,545
so that next time it sees this image,

44
00:02:12,545 --> 00:02:15,400
it predicts that two is the most likely label.

45
00:02:15,400 --> 00:02:17,750
In a perfect world, the network would predict that

46
00:02:17,750 --> 00:02:20,690
the image is 100 percent likely to be the true class.

47
00:02:20,690 --> 00:02:24,210
In order to get the model's prediction closer to the ground truth,

48
00:02:24,210 --> 00:02:26,840
we'll need to define some measure of exactly how far

49
00:02:26,840 --> 00:02:29,785
off the model currently is from perfection.

50
00:02:29,785 --> 00:02:32,720
We can use a loss function to find any errors between

51
00:02:32,720 --> 00:02:35,830
the true image classes and our predicted classes,

52
00:02:35,830 --> 00:02:37,580
then backpropagation will find out

53
00:02:37,580 --> 00:02:40,530
which model parameters are responsible for those errors.

54
00:02:40,530 --> 00:02:43,460
Since we're constructing a multi-class classifier,

55
00:02:43,460 --> 00:02:46,650
we'll use categorical cross entropy loss.

56
00:02:46,650 --> 00:02:49,330
To calculate the loss in this example,

57
00:02:49,330 --> 00:02:50,560
we begin by looking at

58
00:02:50,560 --> 00:02:55,030
the model's predicted probability of the true class which is 16.2 percent.

59
00:02:55,030 --> 00:02:58,790
Cross entropy loss looks at this probability value which in

60
00:02:58,790 --> 00:03:03,745
decimal form is 0.162 and takes the negative log loss of that value.

61
00:03:03,745 --> 00:03:08,720
In this case, we get negative log 0.162 or 1.82.

62
00:03:08,720 --> 00:03:10,385
Now for argument's sake,

63
00:03:10,385 --> 00:03:13,195
say instead that the weights of the network were slightly different.

64
00:03:13,195 --> 00:03:16,775
The model instead returned at these predicted probabilities.

65
00:03:16,775 --> 00:03:18,650
This prediction is much better than the one

66
00:03:18,650 --> 00:03:21,540
above and when we calculate the cross entropy loss,

67
00:03:21,540 --> 00:03:23,485
we get a much smaller value.

68
00:03:23,485 --> 00:03:28,430
In general, it's possible to show that the categorical cross entropy loss is defined in

69
00:03:28,430 --> 00:03:30,680
such a way that the loss is lower when

70
00:03:30,680 --> 00:03:33,760
the model's prediction agrees more with the true class label,

71
00:03:33,760 --> 00:03:37,380
and it's higher when the prediction and the true class label disagree.

72
00:03:37,380 --> 00:03:41,300
As a model trains, its goal will be to find the weights that minimize

73
00:03:41,300 --> 00:03:45,675
this loss function and therefore give us the most accurate predictions.

74
00:03:45,675 --> 00:03:48,530
So, a loss function and backpropagation give us

75
00:03:48,530 --> 00:03:51,460
a way to quantify how bad a particular network weight is,

76
00:03:51,460 --> 00:03:55,720
based on how close a predicted and the true class label are from one another.

77
00:03:55,720 --> 00:03:58,950
Next, we need a way to calculate a better weight value.

78
00:03:58,950 --> 00:04:01,070
In the previous lesson, you were encouraged to think of

79
00:04:01,070 --> 00:04:04,485
the loss function as a surface that resembles a mountain range.

80
00:04:04,485 --> 00:04:06,160
Then to minimize this function,

81
00:04:06,160 --> 00:04:09,770
we need only find a way to descend to the lowest value.

82
00:04:09,770 --> 00:04:11,869
This is the role of an optimizer.

83
00:04:11,869 --> 00:04:14,990
The standard method for minimizing the loss and optimizing

84
00:04:14,990 --> 00:04:18,155
for the best weight values is called "Gradient Descent".

85
00:04:18,155 --> 00:04:20,690
You've been introduced to a number of ways to perform

86
00:04:20,690 --> 00:04:24,634
gradient descent and each method has a corresponding optimizer.

87
00:04:24,634 --> 00:04:28,145
The surface depicted here is an example of a loss function

88
00:04:28,145 --> 00:04:32,080
and all of the optimizers are racing towards the minimum of the function.

89
00:04:32,080 --> 00:04:34,580
As you can see, some do better than others and you're

90
00:04:34,580 --> 00:04:37,360
encouraged to experiment with all of them in your code.

