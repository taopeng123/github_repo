1
00:00:00,000 --> 00:00:04,360
You've learned a lot about how to approach the task of image classification.

2
00:00:04,360 --> 00:00:07,725
In this notebook, I'll go over how to load in image data,

3
00:00:07,725 --> 00:00:09,880
define a model, and train it.

4
00:00:09,880 --> 00:00:12,220
You may already be familiar with this process from

5
00:00:12,220 --> 00:00:14,960
the earlier lesson on deep learning with PyTorch,

6
00:00:14,960 --> 00:00:18,955
but I'd encourage you to stay tuned just so you can see another example in detail.

7
00:00:18,955 --> 00:00:21,325
I'll present this code as an exercise.

8
00:00:21,325 --> 00:00:23,710
Then you'll get a chance to change this code,

9
00:00:23,710 --> 00:00:24,790
define your own model,

10
00:00:24,790 --> 00:00:28,065
and try it out on your own in a Jupyter Notebook, much like this one.

11
00:00:28,065 --> 00:00:30,460
So, the first thing I've done in this notebook is to

12
00:00:30,460 --> 00:00:33,305
load in the necessary torch and NumPy libraries.

13
00:00:33,305 --> 00:00:37,630
Next, I'm going to import the torchvision datasets and transformation libraries.

14
00:00:37,630 --> 00:00:39,400
I'll use these to actually load in

15
00:00:39,400 --> 00:00:42,835
the MNIST dataset and transform it into a Tensor datatype.

16
00:00:42,835 --> 00:00:47,065
Here transforms.ToTensor is where I define that transformation.

17
00:00:47,065 --> 00:00:51,450
The tensor datatype is just a data type that's very similar to a NumPy array,

18
00:00:51,450 --> 00:00:53,720
only it can be moved onto a GPU for

19
00:00:53,720 --> 00:00:56,515
faster calculation, which you'll learn more about later.

20
00:00:56,515 --> 00:01:00,620
You'll also see that I've set some parameters for loading in the image data.

21
00:01:00,620 --> 00:01:03,110
I can define the batch size which is the number of

22
00:01:03,110 --> 00:01:06,175
training images that will be seen in one training iteration,

23
00:01:06,175 --> 00:01:09,200
where one training iteration means one time that a network

24
00:01:09,200 --> 00:01:12,430
make some mistakes and learn from them using back propagation.

25
00:01:12,430 --> 00:01:15,550
The number of workers is if you want to load data in parallel.

26
00:01:15,550 --> 00:01:18,430
For most cases, zero will work fine here.

27
00:01:18,430 --> 00:01:23,415
Now I'm going to load in training and test data using datasets.MNIST.

28
00:01:23,415 --> 00:01:25,460
I'm going to download each set and I'm going to

29
00:01:25,460 --> 00:01:28,685
transform it into a tensor datatype which I defined here.

30
00:01:28,685 --> 00:01:32,670
I'll put the downloaded data into a directory named data.

31
00:01:32,670 --> 00:01:36,260
Finally, I'm going to create training and test loaders.

32
00:01:36,260 --> 00:01:39,140
These loaders taking our data that we defined above,

33
00:01:39,140 --> 00:01:41,270
our batch size and number of workers.

34
00:01:41,270 --> 00:01:43,640
The train and test loaders give

35
00:01:43,640 --> 00:01:46,615
us a way to iterate through this data one batch at a time.

36
00:01:46,615 --> 00:01:49,560
Downloading the data may take a minute or two.

37
00:01:49,560 --> 00:01:51,715
After I've downloaded my data,

38
00:01:51,715 --> 00:01:54,590
I do the first step in any image analysis task,

39
00:01:54,590 --> 00:01:56,055
I visualize the data.

40
00:01:56,055 --> 00:01:58,640
Here I'm grabbing one batch of images and

41
00:01:58,640 --> 00:02:01,525
their correct labels and I'm plotting 20 of them.

42
00:02:01,525 --> 00:02:05,635
Here you can see a variety of MNIST images and their labels.

43
00:02:05,635 --> 00:02:08,120
This step allows me to check and make sure that

44
00:02:08,120 --> 00:02:10,730
these images look how I expect them to look.

45
00:02:10,730 --> 00:02:13,095
I can even look at an image in more detail.

46
00:02:13,095 --> 00:02:18,520
Here I'm just looking at one image in our set and I'm displaying the gray scale values.

47
00:02:18,520 --> 00:02:21,335
You can see that these values are normalized

48
00:02:21,335 --> 00:02:23,930
and the brightest pixels are close to a value of one.

49
00:02:23,930 --> 00:02:26,330
The black pixels have a value of zero.

50
00:02:26,330 --> 00:02:30,710
Next comes the really interesting part which is defining the MLP model.

51
00:02:30,710 --> 00:02:34,130
We've talked about defining the input hidden and output layers

52
00:02:34,130 --> 00:02:37,430
and so I'll leave most of this section for you to fill out.

53
00:02:37,430 --> 00:02:39,960
I do want to point out a couple of things here.

54
00:02:39,960 --> 00:02:41,740
First the init function.

55
00:02:41,740 --> 00:02:45,260
To define any neural network in PyTorch you have to define and

56
00:02:45,260 --> 00:02:48,890
name any layers that have learned weight values in the Init function,

57
00:02:48,890 --> 00:02:52,940
in this case, any fully-connected linear layers that you define.

58
00:02:52,940 --> 00:02:58,160
I've defined a sample first input layer which I've named FC1 for you.

59
00:02:58,160 --> 00:03:04,250
This has 784 or 28 by 28 inputs and a number of hidden nodes.

60
00:03:04,250 --> 00:03:07,755
This is the number of outputs that this layer will produce.

61
00:03:07,755 --> 00:03:10,010
For now, I've left this as one and this will have

62
00:03:10,010 --> 00:03:12,380
to be changed to create a working solution.

63
00:03:12,380 --> 00:03:16,145
Next, you have to define the feedforward behavior of your network.

64
00:03:16,145 --> 00:03:21,215
This is just how an input X will be passed through various layers and transformed.

65
00:03:21,215 --> 00:03:24,440
I'm assuming the past in X will be a grayscale image like

66
00:03:24,440 --> 00:03:28,335
an MNIST image and I've provided some starting code in here for you.

67
00:03:28,335 --> 00:03:32,705
First, I've made sure to flatten the input image X by using the view function.

68
00:03:32,705 --> 00:03:35,270
View takes in a number of rows and columns and

69
00:03:35,270 --> 00:03:38,120
then squishes and input into that desired shape.

70
00:03:38,120 --> 00:03:40,760
In this case, the number of columns will be 28 by

71
00:03:40,760 --> 00:03:44,480
28 or 784 and by putting a negative one here,

72
00:03:44,480 --> 00:03:49,120
this function will automatically fit all of the x values into this column shape.

73
00:03:49,120 --> 00:03:53,930
The end result is that this X will be a vector of 784 values.

74
00:03:53,930 --> 00:03:56,300
Then I'm passing this flattened vector to

75
00:03:56,300 --> 00:03:58,810
our first fully-connected layer defined up here.

76
00:03:58,810 --> 00:04:00,690
I just call this layer by name,

77
00:04:00,690 --> 00:04:04,395
pass in our input and apply a relu activation function.

78
00:04:04,395 --> 00:04:06,680
A relu should be applied generally to

79
00:04:06,680 --> 00:04:09,140
the output of every hidden layer so that those outputs

80
00:04:09,140 --> 00:04:13,950
are consistent positive values and finally I return the transformed X.

81
00:04:13,950 --> 00:04:15,485
Now to complete this model,

82
00:04:15,485 --> 00:04:18,410
you should add to the init end forward functions so that

83
00:04:18,410 --> 00:04:22,135
the final returned X is a list of class scores.

84
00:04:22,135 --> 00:04:25,115
Next, I'm going to describe how you might go about training

85
00:04:25,115 --> 00:04:28,530
your defined model and complete this task on your own.

