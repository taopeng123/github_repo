1
00:00:00,000 --> 00:00:05,955
欢迎回来下面介绍如何创建使用 LogSoftmax 输出的模型

2
00:00:05,955 --> 00:00:09,915
和之前使用 nn.sequential 创建的模型很像

3
00:00:09,915 --> 00:00:12,360
网络层级是线性转换、

4
00:00:12,360 --> 00:00:14,160
ReLU、线性转换 、ReLU、

5
00:00:14,160 --> 00:00:16,860
线性转换

6
00:00:16,860 --> 00:00:20,055
然后将这个传入 LogSoftmax 模块

7
00:00:20,055 --> 00:00:22,410
要注意

8
00:00:22,410 --> 00:00:25,500
将 LogSoftmax 里的维度设为 1

9
00:00:25,500 --> 00:00:31,215
它就会针对列计算结果 而不是行

10
00:00:31,215 --> 00:00:34,970
行对应的是样本

11
00:00:34,970 --> 00:00:37,640
我们将一批样本传入网络中

12
00:00:37,640 --> 00:00:40,750
每行都是一个样本

13
00:00:40,750 --> 00:00:42,800
要注意

14
00:00:42,800 --> 00:00:46,160
对每个样本计算 softmax 函数

15
00:00:46,160 --> 00:00:52,240
而不是针对批次中的每个特征计算该函数

16
00:00:52,240 --> 00:00:56,060
在这里将损失/条件

17
00:00:56,060 --> 00:00:58,490
定义为负对数似然损失

18
00:00:58,490 --> 00:01:01,250
从 trainloader 获取图像和标签

19
00:01:01,250 --> 00:01:04,310
扁平化图像 传入模型中以获得对数

20
00:01:04,310 --> 00:01:07,380
这里不再是对数

21
00:01:07,380 --> 00:01:09,844
而是对数概率

22
00:01:09,844 --> 00:01:11,450
称为 logps

23
00:01:11,450 --> 00:01:14,345
这里也更改下

24
00:01:14,345 --> 00:01:17,320
好了我们计算出了损失

25
00:01:17,320 --> 00:01:19,969
现在我们知道如何计算损失了

26
00:01:19,969 --> 00:01:23,195
但是如何用损失进行反向传播呢？

27
00:01:23,195 --> 00:01:26,090
PyTorch 提供了一个非常有用的模块 叫做 Autograd

28
00:01:26,090 --> 00:01:30,460
它可以自动计算张量的梯度

29
00:01:30,460 --> 00:01:32,550
它的工作原理是

30
00:01:32,550 --> 00:01:35,210
PyTorch 将跟踪你对张量执行的所有操作

31
00:01:35,210 --> 00:01:39,635
然后你可以告诉它进行反向传播

32
00:01:39,635 --> 00:01:41,630
反向执行每个操作

33
00:01:41,630 --> 00:01:47,015
并计算相对于输入参数的梯度

34
00:01:47,015 --> 00:01:53,825
你需要告诉 PyTorch 你想对特定的张量使用 autograd

35
00:01:53,825 --> 00:01:59,025
例如 创建一个张量 x = torch.zeros

36
00:01:59,025 --> 00:02:00,590
将其设为标量 例如 1

37
00:02:00,590 --> 00:02:03,845
然后设置为 requires_grad = True

38
00:02:03,845 --> 00:02:09,620
这部分告诉 PyTorch 跟踪对张量 x 执行的操作

39
00:02:09,620 --> 00:02:13,540
如果你想获得梯度 它将为你计算梯度

40
00:02:13,540 --> 00:02:16,100
如果你创建了一个张量

41
00:02:16,100 --> 00:02:19,310
但是不想计算它的梯度

42
00:02:19,310 --> 00:02:21,790
则将这个设为 False

43
00:02:21,790 --> 00:02:26,195
还可以使用 torch.no_grad

44
00:02:26,195 --> 00:02:28,400
对在这个上下文中执行的所有操作

45
00:02:28,400 --> 00:02:31,255
关闭所有梯度

46
00:02:31,255 --> 00:02:35,000
还可以使用 torch.set_grad_enabled

47
00:02:35,000 --> 00:02:39,060
全局性地开启或关闭所有梯度 在这里设置 true 或 false 即可

48
00:02:39,060 --> 00:02:44,520
我们在 PyTorch 中创建张量

49
00:02:44,520 --> 00:02:49,290
设置 requires_grad = True 然后对该张量执行操作

50
00:02:49,290 --> 00:02:53,360
操作完毕后 输入 .backwards

51
00:02:53,360 --> 00:02:56,060
如果使用张量 x

52
00:02:56,060 --> 00:02:59,000
然后计算出其他张量 z 执行 z.backward

53
00:02:59,000 --> 00:03:04,595
它将反向经过操作并计算 x 的总梯度

54
00:03:04,595 --> 00:03:07,370
例如 我创建了这个随机张量

55
00:03:07,370 --> 00:03:12,320
随机的 2x2 张量 像这样变成方形

56
00:03:12,320 --> 00:03:15,890
如果你查看 y

57
00:03:15,890 --> 00:03:18,320
y 是第二个方形张量

58
00:03:18,320 --> 00:03:20,405
如果查看 y.grad_fn

59
00:03:20,405 --> 00:03:25,125
它会显示这个 grad 函数是一个幂

60
00:03:25,125 --> 00:03:28,340
PyTorch 跟踪了这个操作

61
00:03:28,340 --> 00:03:31,915
知道最后一个操作是幂运算

62
00:03:31,915 --> 00:03:36,335
现在我们可以对 y 取均值并获得另一个张量 z

63
00:03:36,335 --> 00:03:40,145
这是一个标量张量 我们缩减了 y

64
00:03:40,145 --> 00:03:42,710
y 是一个 2 x 2 矩阵/数组

65
00:03:42,710 --> 00:03:45,850
然后对其取均值 得出 z

66
00:03:45,850 --> 00:03:49,220
张量的梯度显示在属性 grad 中

67
00:03:49,220 --> 00:03:53,870
现在我们可以查看张量 x 的梯度

68
00:03:53,870 --> 00:03:56,330
我们仅进行了前向传播

69
00:03:56,330 --> 00:03:59,410
尚未计算梯度 因此为 None

70
00:03:59,410 --> 00:04:01,275
现在如果执行 z.backward

71
00:04:01,275 --> 00:04:05,550
它将反向经过我们执行的这小部分操作

72
00:04:05,550 --> 00:04:07,430
我们进行了幂运算和取均值运算

73
00:04:07,430 --> 00:04:10,795
我们反向传播并计算 x 的梯度

74
00:04:10,795 --> 00:04:12,740
结果你会发现

75
00:04:12,740 --> 00:04:15,290
z 相对于 x 的梯度应该是 x/2

76
00:04:15,290 --> 00:04:18,380
查看梯度

77
00:04:18,380 --> 00:04:21,680
看看 x/2 它们是一样的

78
00:04:21,680 --> 00:04:26,435
从数学角度来看 梯度等于预期结果

79
00:04:26,435 --> 00:04:29,210
这就是在 autograd 和 PyTorch 中

80
00:04:29,210 --> 00:04:31,430
使用梯度的一般流程

81
00:04:31,430 --> 00:04:32,810
在计算损失时 我们可以使用它计算梯度

82
00:04:32,810 --> 00:04:36,580
很有用 对不对

83
00:04:36,580 --> 00:04:42,350
损失取决于权重和偏差参数

84
00:04:42,350 --> 00:04:44,960
我们需要权重梯度来执行梯度下降法

85
00:04:44,960 --> 00:04:48,170
我们可以将权重设为需要梯度的张量

86
00:04:48,170 --> 00:04:52,960
然后进行前向传播以计算损失

87
00:04:52,960 --> 00:04:57,230
获得损失后 进行反向传播以计算权重的梯度

88
00:04:57,230 --> 00:04:59,990
获得这些梯度后 可以执行梯度下降步骤

89
00:04:59,990 --> 00:05:02,930
我来演示下代码

90
00:05:02,930 --> 00:05:07,535
像之前一样定义模型 这里是 LogSoftmax 输出

91
00:05:07,535 --> 00:05:10,220
使用负对数似然损失

92
00:05:10,220 --> 00:05:13,625
从 trainloader 中获取图像和标签 扁平化图像

93
00:05:13,625 --> 00:05:15,950
然后从模型中获得对数概率

94
00:05:15,950 --> 00:05:18,800
传入损失函数里

95
00:05:18,800 --> 00:05:20,930
获得实际损失

96
00:05:20,930 --> 00:05:23,750
现在查看模型权重

97
00:05:23,750 --> 00:05:29,600
Model[0] 对应的是第一个线性转换的参数

98
00:05:29,600 --> 00:05:31,715
查看权重梯度

99
00:05:31,715 --> 00:05:34,370
然后从损失开始进行反向传播

100
00:05:34,370 --> 00:05:38,260
再次查看权重梯度

101
00:05:38,260 --> 00:05:40,570
在反向传播之前

102
00:05:40,570 --> 00:05:42,695
没有任何梯度 这是因为我们尚未计算它

103
00:05:42,695 --> 00:05:45,425
但是在反向传播之后

104
00:05:45,425 --> 00:05:47,630
我们计算了梯度

105
00:05:47,630 --> 00:05:51,715
我们可以在梯度下降步骤中使用这些梯度训练网络

106
00:05:51,715 --> 00:05:54,770
现在你知道如何计算损失

107
00:05:54,770 --> 00:05:57,830
如何使用这些损失计算梯度

108
00:05:57,830 --> 00:06:00,980
但是在开始训练之前 还有一步

109
00:06:00,980 --> 00:06:04,850
你需要知道如何使用这些梯度更新权重

110
00:06:04,850 --> 00:06:09,230
接下来 我们将使用优化器 优化器来自 PyTorch 的 optim 软件包

111
00:06:09,230 --> 00:06:14,345
例如 我们可以通过输入 optim.SGD 使用随机梯度下降法

112
00:06:14,345 --> 00:06:18,184
定义方式是

113
00:06:18,184 --> 00:06:22,395
从 PyTorch 导入模块 optim 在这里输入 optim.SGD

114
00:06:22,395 --> 00:06:23,940
传入模型参数

115
00:06:23,940 --> 00:06:25,895
这些是我们希望优化器更新的参数

116
00:06:25,895 --> 00:06:29,855
然后设定学习速率

117
00:06:29,855 --> 00:06:32,245
这样就创建了优化器

118
00:06:32,245 --> 00:06:36,110
训练流程包含四个不同的步骤

119
00:06:36,110 --> 00:06:39,440
首先对网络进行前向传播

120
00:06:39,440 --> 00:06:43,220
然后使用该网络输出计算损失

121
00:06:43,220 --> 00:06:45,350
使用 loss.backwards

122
00:06:45,350 --> 00:06:48,540
对网络进行反向传播并计算梯度

123
00:06:48,540 --> 00:06:52,715
然后使用优化器更新权重

124
00:06:52,715 --> 00:06:56,090
我在这里只演示训练一步

125
00:06:56,090 --> 00:07:00,410
然后你将编写实际训练过程 循环地训练网络

126
00:07:00,410 --> 00:07:04,190
首先

127
00:07:04,190 --> 00:07:05,795
从 trainloader 获取图像和标签

128
00:07:05,795 --> 00:07:08,350
然后扁平化图像

129
00:07:08,350 --> 00:07:11,910
接下来清理梯度

130
00:07:11,910 --> 00:07:14,970
PyTorch 默认地会累积梯度

131
00:07:14,970 --> 00:07:18,185
如果你在网络中传播多次

132
00:07:18,185 --> 00:07:21,830
即 多次执行前向传播和反向传播

133
00:07:21,830 --> 00:07:24,630
并不断计算梯度

134
00:07:24,630 --> 00:07:27,100
PyTorch 将不断将这些梯度加起来

135
00:07:27,100 --> 00:07:29,850
如果不清理梯度

136
00:07:29,850 --> 00:07:33,680
那么在当前训练中

137
00:07:33,680 --> 00:07:36,020
会残留上个训练步骤的梯度

138
00:07:36,020 --> 00:07:38,725
导致网络无法正常训练

139
00:07:38,725 --> 00:07:40,290
因此在每个训练流程之前

140
00:07:40,290 --> 00:07:45,710
你需要调用 zero_grad()

141
00:07:45,710 --> 00:07:48,560
输入 optimizer.zero_grad

142
00:07:48,560 --> 00:07:52,120
它会在优化器里清理所有参数中的所有梯度

143
00:07:52,120 --> 00:07:54,480
让我们能够正常训练网络

144
00:07:54,480 --> 00:07:59,355
这点很容易被忽略

145
00:07:59,355 --> 00:08:00,570
但是这一步很重要

146
00:08:00,570 --> 00:08:03,220
请务必记住

147
00:08:03,220 --> 00:08:06,090
接着 执行前向传播和反向传播

148
00:08:06,090 --> 00:08:07,725
并更新权重

149
00:08:07,725 --> 00:08:09,010
获得输出

150
00:08:09,010 --> 00:08:12,110
使用图像对模型进行前向传播

151
00:08:12,110 --> 00:08:15,720
使用模型的输出和标签计算损失

152
00:08:15,720 --> 00:08:20,360
然后反向传播 最后执行优化器步骤

153
00:08:20,360 --> 00:08:23,675
这是初始权重

154
00:08:23,675 --> 00:08:26,150
然后计算梯度

155
00:08:26,150 --> 00:08:28,010
梯度是这样的

156
00:08:28,010 --> 00:08:30,350
然后执行优化器步骤并更新权重

157
00:08:30,350 --> 00:08:32,330
权重更改了

158
00:08:32,330 --> 00:08:35,870
你将遍历训练集

159
00:08:35,870 --> 00:08:39,560
对于训练集中的每批数据

160
00:08:39,560 --> 00:08:42,185
你将执行相同的训练流程

161
00:08:42,185 --> 00:08:44,060
获取数据

162
00:08:44,060 --> 00:08:46,180
清理梯度

163
00:08:46,180 --> 00:08:51,320
将图像/输入传入网络中 获得输出

164
00:08:51,320 --> 00:08:52,925
使用标签计算损失

165
00:08:52,925 --> 00:08:56,285
然后用损失反向传播并更新权重

166
00:08:56,285 --> 00:09:01,640
现在该你来为此模型实现训练循环了

167
00:09:01,640 --> 00:09:05,870
你要将遍历数据集

168
00:09:05,870 --> 00:09:10,440
从 trainloader 中获取图像和标签

169
00:09:10,440 --> 00:09:11,710
然后对每批数据执行训练流程

170
00:09:11,710 --> 00:09:16,000
计算网络输出

171
00:09:16,000 --> 00:09:18,190
计算损失 对损失进行反向传播

172
00:09:18,190 --> 00:09:20,120
然后更新权重

173
00:09:20,120 --> 00:09:23,435
经过整个数据集 1 次称为 1 个周期

174
00:09:23,435 --> 00:09:26,980
我将周期设成了 5 次

175
00:09:26,980 --> 00:09:30,005
当然 你也可以改为其他次数

176
00:09:30,005 --> 00:09:31,565
计算损失后

177
00:09:31,565 --> 00:09:34,880
我们可以递增这个值以进行跟踪

178
00:09:34,880 --> 00:09:36,620
最后就能查看损失

179
00:09:36,620 --> 00:09:38,510
调用 running_loss

180
00:09:38,510 --> 00:09:40,610
输出训练损失

181
00:09:40,610 --> 00:09:42,065
当你继续使用数据训练网络时

182
00:09:42,065 --> 00:09:44,825
如果操作正确的话

183
00:09:44,825 --> 00:09:47,555
应该看到损失开始降低

184
00:09:47,555 --> 00:09:49,985
自己尝试一下吧 

185
00:09:49,985 --> 00:09:52,530
如果需要帮助 请参阅我的解决方案加油！

