1
00:00:00,000 --> 00:00:05,040
Is there a story you have for an error or some feature that,

2
00:00:05,040 --> 00:00:07,755
it was like a bug that was found early on?

3
00:00:07,755 --> 00:00:12,675
Yeah. This was before public release.

4
00:00:12,675 --> 00:00:15,075
I think we were in- so,

5
00:00:15,075 --> 00:00:21,210
public release was 0.1.12 and I think this was

6
00:00:21,210 --> 00:00:27,374
like 0.1.6 or something - Something really early - and Justin Johnson,

7
00:00:27,374 --> 00:00:29,175
who was interning at a fair,

8
00:00:29,175 --> 00:00:30,510
who's a student at Stanford, also runs-

9
00:00:30,510 --> 00:00:32,790
Yes, I think I've seen some of his computer vision videos.

10
00:00:32,790 --> 00:00:38,775
Yeah. So, he co-runs the CS231n course on ConvNets at Stanford.

11
00:00:38,775 --> 00:00:44,335
So, he was doing his research and then he was like,

12
00:00:44,335 --> 00:00:45,840
"My networks aren't training,

13
00:00:45,840 --> 00:00:47,310
and I started investigating.

14
00:00:47,310 --> 00:00:51,620
It turns out if you have a non-contiguous tensor and send it through a linear layer,

15
00:00:51,620 --> 00:00:53,075
it'll just give you garbage really."

16
00:00:53,075 --> 00:00:53,945
Just garbage.

17
00:00:53,945 --> 00:00:55,655
And we were like,

18
00:00:55,655 --> 00:00:58,040
"Oh man, that just sounds bad."

19
00:00:58,040 --> 00:01:00,760
So, we had pretty robust unit testing,

20
00:01:00,760 --> 00:01:07,730
but we forgot to add non-contiguous checks to all of our unit tests.

21
00:01:07,730 --> 00:01:11,480
So, that was a particularly fun one.

22
00:01:11,480 --> 00:01:12,965
Yeah, that's a big bug.

23
00:01:12,965 --> 00:01:17,240
That's good though that someone notices it before going fully public.

24
00:01:17,240 --> 00:01:20,030
One of the reasons why I like PyTorch is,

25
00:01:20,030 --> 00:01:21,530
especially for our teaching,

26
00:01:21,530 --> 00:01:24,540
it just merges so nicely with Python,

27
00:01:24,540 --> 00:01:26,960
which a lot of our students already know and it's something that's

28
00:01:26,960 --> 00:01:30,920
like fairly intuitive to pick up as a programming language.

29
00:01:30,920 --> 00:01:34,675
But I think about Python and I also think

30
00:01:34,675 --> 00:01:36,635
there's a trade off there where

31
00:01:36,635 --> 00:01:39,380
the readability comes at a cost of it being a little bit slow.

32
00:01:39,380 --> 00:01:41,900
So, maybe you could talk a bit about that decision.

33
00:01:41,900 --> 00:01:45,069
Sure. Initially, when we wrote PyTorch,

34
00:01:45,069 --> 00:01:51,915
the autograd and- the entire internal autograd engine is all written in Python.

35
00:01:51,915 --> 00:01:57,540
Then, one of our aims for PyTorch was that it should be very imperative,

36
00:01:57,540 --> 00:01:59,030
very usable, very Pythonic,

37
00:01:59,030 --> 00:02:02,705
but at the same time as fast as any other framework out there.

38
00:02:02,705 --> 00:02:05,765
So, we put special focus on that.

39
00:02:05,765 --> 00:02:11,360
One of the consequences of that was large parts of PyTorch live in C++,

40
00:02:11,360 --> 00:02:14,740
except whatever is user facing is still in Python,

41
00:02:14,740 --> 00:02:19,140
so that you can attach your debugger, you can print.

42
00:02:19,140 --> 00:02:21,530
All of those are still very hackable.

43
00:02:21,530 --> 00:02:24,765
But, yeah, it gives you the best of both worlds where

44
00:02:24,765 --> 00:02:30,500
the main parts that the userland would need to know are in Python,

45
00:02:30,500 --> 00:02:34,280
but of the critical parts that are internals,

46
00:02:34,280 --> 00:02:35,645
they're all in C++.

47
00:02:35,645 --> 00:02:36,680
Okay. That makes sense.

