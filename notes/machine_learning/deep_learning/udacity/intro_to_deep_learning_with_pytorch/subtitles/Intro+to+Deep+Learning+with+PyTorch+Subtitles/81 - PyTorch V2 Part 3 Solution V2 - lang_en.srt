1
00:00:00,000 --> 00:00:05,955
Hi and welcome back. Here's my solution for this model that uses a LogSoftmax output.

2
00:00:05,955 --> 00:00:09,915
It is a pretty similar to what I built before with an index sequential.

3
00:00:09,915 --> 00:00:12,360
So, we just use a linear transformation,

4
00:00:12,360 --> 00:00:14,160
ReLU, linear transformation, ReLU,

5
00:00:14,160 --> 00:00:16,860
another linear transformation for output and then we can pass this

6
00:00:16,860 --> 00:00:20,055
to our LogSoftmax module.

7
00:00:20,055 --> 00:00:22,410
So, what I'm doing here is I'm making sure I set

8
00:00:22,410 --> 00:00:25,500
the dimension to one for LogSoftmax and this

9
00:00:25,500 --> 00:00:31,215
makes it so that it calculates the function across the columns instead of the rows.

10
00:00:31,215 --> 00:00:34,970
So, if you remember, the rows correspond to our examples.

11
00:00:34,970 --> 00:00:37,640
So, we have a batch of examples that we're passing

12
00:00:37,640 --> 00:00:40,750
to our network and each row is one of those examples.

13
00:00:40,750 --> 00:00:42,800
So, we want to make sure that we're covering

14
00:00:42,800 --> 00:00:46,160
the softmax function across each of our examples and

15
00:00:46,160 --> 00:00:52,240
not across each individual feature in our batches.

16
00:00:52,240 --> 00:00:56,060
Here, I'm just defining our loss or criterion as

17
00:00:56,060 --> 00:00:58,490
the negative log likelihood loss and

18
00:00:58,490 --> 00:01:01,250
again get our images and labels from our train loader,

19
00:01:01,250 --> 00:01:04,310
flatten them, pass it through our model to get the logits.

20
00:01:04,310 --> 00:01:07,380
So, this is actually not the largest anymore,

21
00:01:07,380 --> 00:01:09,844
this is like a log probability,

22
00:01:09,844 --> 00:01:11,450
so we call it like logps,

23
00:01:11,450 --> 00:01:14,345
and then you do that.

24
00:01:14,345 --> 00:01:17,320
There you go. You see we get our nice loss.

25
00:01:17,320 --> 00:01:19,969
Now, we know how to calculate a loss,

26
00:01:19,969 --> 00:01:23,195
but how do we actually use it to perform backpropagation?

27
00:01:23,195 --> 00:01:26,090
So, PyTorch towards actually has this really great module called

28
00:01:26,090 --> 00:01:30,460
Autograd that automatically calculates the gradients of our tensors.

29
00:01:30,460 --> 00:01:32,550
So, the way it works is that,

30
00:01:32,550 --> 00:01:35,210
PyTorch will keep track of all the operations you do on

31
00:01:35,210 --> 00:01:39,635
a tensor and then when you can tell it to do a backwards pass,

32
00:01:39,635 --> 00:01:41,630
to go backwards through each of

33
00:01:41,630 --> 00:01:47,015
those operations and calculate the gradients with respect to the input parameters.

34
00:01:47,015 --> 00:01:53,825
In general, you need to tell PyTorch that you want to use autograd on a specific tensor.

35
00:01:53,825 --> 00:01:59,025
So, in this case, you would create some tensor like x equals torch.zeros,

36
00:01:59,025 --> 00:02:00,590
just to make it a scalar,

37
00:02:00,590 --> 00:02:03,845
say one and then give it requires grad equals true.

38
00:02:03,845 --> 00:02:09,620
So, this tells PyTorch to track the operations on this tensor x,

39
00:02:09,620 --> 00:02:13,540
so that if you want to get the gradient then it will calculate it for you.

40
00:02:13,540 --> 00:02:16,100
So, in general, if you're creating a tensor and

41
00:02:16,100 --> 00:02:19,310
you don't want to calculate the gradient for it,

42
00:02:19,310 --> 00:02:21,790
you want to make sure this is set to false.

43
00:02:21,790 --> 00:02:26,195
You can also use this context torch.no grad to make sure

44
00:02:26,195 --> 00:02:28,400
all the gradients are shut off for all of

45
00:02:28,400 --> 00:02:31,255
the operations that you're doing while you're in this context.

46
00:02:31,255 --> 00:02:35,000
Then, you can also turn on or off gradients globally with

47
00:02:35,000 --> 00:02:39,060
torch.set grad enabled and give it true or false, depending on what you want to do.

48
00:02:39,060 --> 00:02:44,520
So, the way this works in PyTorch is that you basically create your tensor and again,

49
00:02:44,520 --> 00:02:49,290
you set requires grad equals true and then you just perform some operations on it.

50
00:02:49,290 --> 00:02:53,360
Then, once you are done with those operations, you type in.backwards.

51
00:02:53,360 --> 00:02:56,060
So, if you use x, this tensor x,

52
00:02:56,060 --> 00:02:59,000
then calculate some other tensor z then if you do z.backward,

53
00:02:59,000 --> 00:03:04,595
it'll go backwards through your operations and calculate the total gradient for x.

54
00:03:04,595 --> 00:03:07,370
So, for example, if I just create this random tensor,

55
00:03:07,370 --> 00:03:12,320
random two-by-two tensor, and then I can square it like this.

56
00:03:12,320 --> 00:03:15,890
What it does, you can actually see if you look at y,

57
00:03:15,890 --> 00:03:18,320
so y is our secondary or squared tensor.

58
00:03:18,320 --> 00:03:20,405
If you look at y.grad function,

59
00:03:20,405 --> 00:03:25,125
then it actually shows us that this grad function is a power.

60
00:03:25,125 --> 00:03:28,340
So, PyTorch just track this and it

61
00:03:28,340 --> 00:03:31,915
knows that the last operation done was a power operation.

62
00:03:31,915 --> 00:03:36,335
So, now, we can take the mean of y and get another tensor z.

63
00:03:36,335 --> 00:03:40,145
So, now this is just a scalar tensor, we've reduced y,

64
00:03:40,145 --> 00:03:42,710
y is a two-by-two matrix,

65
00:03:42,710 --> 00:03:45,850
two by two array and then we take in the mean of it to get z.

66
00:03:45,850 --> 00:03:49,220
Ingredients for tensor show up in this attribute grad,

67
00:03:49,220 --> 00:03:53,870
so we can actually look at what's the gradient of our tensor x right now,

68
00:03:53,870 --> 00:03:56,330
and we've only done this forward pass,

69
00:03:56,330 --> 00:03:59,410
we haven't actually calculated the gradient yet and so it's just none.

70
00:03:59,410 --> 00:04:01,275
So, now if we do z.backward,

71
00:04:01,275 --> 00:04:05,550
it's going to go backwards through this tiny little set of operations that we've done.

72
00:04:05,550 --> 00:04:07,430
So, we did a power and then a mean and let's go

73
00:04:07,430 --> 00:04:10,795
backwards through this and calculate the gradient for x.

74
00:04:10,795 --> 00:04:12,740
So, if you actually work out the math,

75
00:04:12,740 --> 00:04:15,290
you find out that the gradient of z with respect to x should

76
00:04:15,290 --> 00:04:18,380
be x over two and if we look at the gradient,

77
00:04:18,380 --> 00:04:21,680
then we can also look at x divided by two then they are the same.

78
00:04:21,680 --> 00:04:26,435
So, our gradient is equal to what it should be mathematically,

79
00:04:26,435 --> 00:04:29,210
and this is the general process for working with gradients,

80
00:04:29,210 --> 00:04:31,430
and autograd, and PyTorch.

81
00:04:31,430 --> 00:04:32,810
Why this is useful,

82
00:04:32,810 --> 00:04:36,580
is because we can use this to get our gradients when we calculate the loss.

83
00:04:36,580 --> 00:04:42,350
So, if remember, our loss depends on our weight and bias parameters.

84
00:04:42,350 --> 00:04:44,960
We need the gradients of our weights to do gradient descent.

85
00:04:44,960 --> 00:04:48,170
So, what we can do is we can set up our weights as

86
00:04:48,170 --> 00:04:52,960
tensors that require gradients and then do a forward pass to calculate our loss.

87
00:04:52,960 --> 00:04:57,230
With the loss, you do a backwards pass which calculates the gradients for your weights,

88
00:04:57,230 --> 00:04:59,990
and then with those gradients, you can do your gradient descent step.

89
00:04:59,990 --> 00:05:02,930
Now, I'll show you how that looks in code.

90
00:05:02,930 --> 00:05:07,535
So, here, I'm defining our model like I did before with LogSoftmax output,

91
00:05:07,535 --> 00:05:10,220
then using the negative log-likelihood loss,

92
00:05:10,220 --> 00:05:13,625
get our images and labels from our train loader, flatten it,

93
00:05:13,625 --> 00:05:15,950
and then we can get our log probabilities from

94
00:05:15,950 --> 00:05:18,800
our model and then pass that into our criterion,

95
00:05:18,800 --> 00:05:20,930
which gives us the actual loss.

96
00:05:20,930 --> 00:05:23,750
So, now, if we look at our models weights,

97
00:05:23,750 --> 00:05:29,600
so model zero gives us the parameters for this first linear transformation.

98
00:05:29,600 --> 00:05:31,715
So, we can look at the weight and then we can look at the gradient,

99
00:05:31,715 --> 00:05:34,370
then we'll do our backwards pass starting from

100
00:05:34,370 --> 00:05:38,260
the loss and then we can look at the weight gradients again.

101
00:05:38,260 --> 00:05:40,570
So, we see before the backward pass,

102
00:05:40,570 --> 00:05:42,695
we don't have any because we haven't actually

103
00:05:42,695 --> 00:05:45,425
calculated it yet but then after the backwards pass,

104
00:05:45,425 --> 00:05:47,630
we have calculated our gradients.

105
00:05:47,630 --> 00:05:51,715
So, we can use these gradients in gradient descent to train our network. All right.

106
00:05:51,715 --> 00:05:54,770
So, now you know how to calculate losses and you know

107
00:05:54,770 --> 00:05:57,830
how to use those losses to calculate gradients.

108
00:05:57,830 --> 00:06:00,980
So, there's one piece left before we can start training.

109
00:06:00,980 --> 00:06:04,850
So, you need to see how to use those gradients to actually update our weights,

110
00:06:04,850 --> 00:06:09,230
and for that we use optimizers and these come from PyTorch's Optim package.

111
00:06:09,230 --> 00:06:14,345
So, for example, we can use stochastic gradient descent with optim.SGD.

112
00:06:14,345 --> 00:06:18,184
The way this is defined is we import

113
00:06:18,184 --> 00:06:22,395
this module optim from PyTorch and then we'd say optim.SGD,

114
00:06:22,395 --> 00:06:23,940
we give it our model parameters.

115
00:06:23,940 --> 00:06:25,895
So, these are the parameters that we want

116
00:06:25,895 --> 00:06:29,855
this optimizer to actually update and then we give it a learning rate,

117
00:06:29,855 --> 00:06:32,245
and this creates our optimizer for us.

118
00:06:32,245 --> 00:06:36,110
So, the training pass consists of four different steps.

119
00:06:36,110 --> 00:06:39,440
So, first, we're going to make a forward pass through the network

120
00:06:39,440 --> 00:06:43,220
then we're going to use that network output to calculate the loss,

121
00:06:43,220 --> 00:06:45,350
then we'll perform a backwards pass through

122
00:06:45,350 --> 00:06:48,540
the network with loss.backwards and this will calculate the gradients.

123
00:06:48,540 --> 00:06:52,715
Then, we'll make a step with our optimizer that updates the weights.

124
00:06:52,715 --> 00:06:56,090
I'll show you how this works with one training step and then you're going

125
00:06:56,090 --> 00:07:00,410
to write it up for real and a loop that is going to train a network.

126
00:07:00,410 --> 00:07:04,190
So, first, we're going to start by getting our images

127
00:07:04,190 --> 00:07:05,795
and labels like we normally do from

128
00:07:05,795 --> 00:07:08,350
our train loader and then we're going to flatten them.

129
00:07:08,350 --> 00:07:11,910
Then next, what we want to do is actually clear the gradients.

130
00:07:11,910 --> 00:07:14,970
So, PyTorch by default accumulates gradients.

131
00:07:14,970 --> 00:07:18,185
That means that if you actually do

132
00:07:18,185 --> 00:07:21,830
multiple passes and multiple backwards like multiple forward passes,

133
00:07:21,830 --> 00:07:24,630
multiple backwards passes, and you keep calculating your gradient,

134
00:07:24,630 --> 00:07:27,100
it's going to keep summing up those gradients.

135
00:07:27,100 --> 00:07:29,850
So, if you don't clear gradients,

136
00:07:29,850 --> 00:07:33,680
then you're going to be getting gradients from the previous training step in

137
00:07:33,680 --> 00:07:36,020
your current training step and it's going to end up where

138
00:07:36,020 --> 00:07:38,725
your network is just not training properly.

139
00:07:38,725 --> 00:07:40,290
So, for this in general,

140
00:07:40,290 --> 00:07:45,710
you're going to be calling zero grad before every training passes.

141
00:07:45,710 --> 00:07:48,560
So, you just say optimizer.zero grad and this will just clean

142
00:07:48,560 --> 00:07:52,120
out all the gradients and all the parameters in your optimizer,

143
00:07:52,120 --> 00:07:54,480
and it'll allow you to train appropriately.

144
00:07:54,480 --> 00:07:59,355
So, this is one of the things in PyTorch that is easy to forget,

145
00:07:59,355 --> 00:08:00,570
but it's really important.

146
00:08:00,570 --> 00:08:03,220
So, try your hardest to remember to do this part,

147
00:08:03,220 --> 00:08:06,090
and then we do our forward pass,

148
00:08:06,090 --> 00:08:07,725
backward pass, then update the weights.

149
00:08:07,725 --> 00:08:09,010
So, we get our output,

150
00:08:09,010 --> 00:08:12,110
so we do a forward pass through our model with our images,

151
00:08:12,110 --> 00:08:15,720
then we calculate the loss using the output of the model and our labels,

152
00:08:15,720 --> 00:08:20,360
then we do a backwards pass and then finally we take an optimizer step.

153
00:08:20,360 --> 00:08:23,675
So, if we look at our initial weights,

154
00:08:23,675 --> 00:08:26,150
so it looks like this and then we can calculate our gradient,

155
00:08:26,150 --> 00:08:28,010
and so the gradient looks like and then if

156
00:08:28,010 --> 00:08:30,350
we take an optimizer step and update our weights,

157
00:08:30,350 --> 00:08:32,330
then our weights have changed.

158
00:08:32,330 --> 00:08:35,870
So, in general, what has worked is you're going to be looping

159
00:08:35,870 --> 00:08:39,560
through your training set and then for each batch out of your training set,

160
00:08:39,560 --> 00:08:42,185
you'll do the same training pass.

161
00:08:42,185 --> 00:08:44,060
So, you'll get your data,

162
00:08:44,060 --> 00:08:46,180
then clear the gradients,

163
00:08:46,180 --> 00:08:51,320
pass those images or your input through your network to get your output, from that,

164
00:08:51,320 --> 00:08:52,925
in the labels, calculate your loss,

165
00:08:52,925 --> 00:08:56,285
and then do a backwards pass on the loss and then update your weights.

166
00:08:56,285 --> 00:09:01,640
So, now, it's your turn to implement the training loop for this model.

167
00:09:01,640 --> 00:09:05,870
So, the idea here is that we're going to be looping through our data-set,

168
00:09:05,870 --> 00:09:10,440
so grabbing images and labels from train loader and then on each of those batches,

169
00:09:10,440 --> 00:09:11,710
you'll be doing the training pass,

170
00:09:11,710 --> 00:09:16,000
and so you'll do this pass where you calculate the output of the network,

171
00:09:16,000 --> 00:09:18,190
calculate the loss, do backwards pass on loss,

172
00:09:18,190 --> 00:09:20,120
and then update your weights.

173
00:09:20,120 --> 00:09:23,435
Each pass through the entire training set is called an

174
00:09:23,435 --> 00:09:26,980
epoch and so here I just have it set for five epochs.

175
00:09:26,980 --> 00:09:30,005
So, you can change this number if you want to go more or less.

176
00:09:30,005 --> 00:09:31,565
Once you calculate the loss,

177
00:09:31,565 --> 00:09:34,880
we can accumulate it to keep track,

178
00:09:34,880 --> 00:09:36,620
so we're going to be looking at a loss.

179
00:09:36,620 --> 00:09:38,510
So, this is running loss and so we'll be

180
00:09:38,510 --> 00:09:40,610
printing out the training loss as it's going along.

181
00:09:40,610 --> 00:09:42,065
So, if it's working,

182
00:09:42,065 --> 00:09:44,825
you should see the loss start falling,

183
00:09:44,825 --> 00:09:47,555
start dropping as you're going through the data.

184
00:09:47,555 --> 00:09:49,985
Try this out yourself and if you need some help,

185
00:09:49,985 --> 00:09:52,530
be sure to check out my solution. Cheers.

