1
00:00:00,000 --> 00:00:03,955
你已经试过定义和训练你自己的分类器

2
00:00:03,955 --> 00:00:06,220
现在我将演示我的创建方式

3
00:00:06,220 --> 00:00:08,980
加载并预处理数据后

4
00:00:08,980 --> 00:00:11,210
我定义了一个完整的 CNN

5
00:00:11,210 --> 00:00:14,670
我保留了我的初始卷积层 conv1

6
00:00:14,670 --> 00:00:18,620
它会接受输入图像并输出有 16 个特征图的图层

7
00:00:18,620 --> 00:00:22,470
我再定义两个卷积层

8
00:00:22,470 --> 00:00:27,440
每个都使输出深度翻倍 直到输出深度达到 64

9
00:00:27,440 --> 00:00:29,795
首先 图像深度是 3

10
00:00:29,795 --> 00:00:33,890
然后是 16 32 最后是 64

11
00:00:33,890 --> 00:00:37,820
每个层级的卷积核大小是 3x3

12
00:00:37,820 --> 00:00:39,395
填充是 1

13
00:00:39,395 --> 00:00:42,310
此外 我保留了一个最大池化层

14
00:00:42,310 --> 00:00:45,245
它会将任何 x,y 维度下采样到一半大小

15
00:00:45,245 --> 00:00:51,070
我还添加了丢弃层 概率是 0.25 以防止过拟合

16
00:00:51,070 --> 00:00:54,360
最后 我还添加了几个全连接层

17
00:00:54,360 --> 00:00:57,230
第一个全连接层会接受

18
00:00:57,230 --> 00:01:00,360
最后下采样的叠加特征图

19
00:01:00,360 --> 00:01:02,810
原始输入图像是 32x32x3

20
00:01:02,810 --> 00:01:06,080
当它们经过每个卷积层和池化层后

21
00:01:06,080 --> 00:01:08,990
x,y 维度将缩小

22
00:01:08,990 --> 00:01:11,930
深度将扩展

23
00:01:11,930 --> 00:01:14,060
在 forward 函数中可以看到

24
00:01:14,060 --> 00:01:16,310
我在每个卷积层之后应用了一个池化层

25
00:01:16,310 --> 00:01:20,010
因此这个图像将缩小到 16x16

26
00:01:20,010 --> 00:01:21,440
然后缩小到 8x8

27
00:01:21,440 --> 00:01:24,755
在最后一个池化层之后缩小到 4x4

28
00:01:24,755 --> 00:01:28,950
第三个卷积层使深度变成 64

29
00:01:28,950 --> 00:01:30,930
因此获得这些值

30
00:01:30,930 --> 00:01:33,320
4x4 表示最终 x,y 大小

31
00:01:33,320 --> 00:01:35,230
64 表示深度

32
00:01:35,230 --> 00:01:39,100
第一个全连接层的输入数量应该是 64

33
00:01:39,100 --> 00:01:41,760
然后输出 500 个值

34
00:01:41,760 --> 00:01:46,369
这 500 个输出将传入最后一个分类层级

35
00:01:46,369 --> 00:01:50,495
生成 10 个类别分数

36
00:01:50,495 --> 00:01:53,955
我们看看如何在 forward 函数中使用所有这些层级

37
00:01:53,955 --> 00:01:58,430
首先我按顺序添加一系列卷积层和池化层

38
00:01:58,430 --> 00:02:01,190
将输入图像传入第一个卷积层

39
00:02:01,190 --> 00:02:04,160
应用激活函数 然后应用池化层

40
00:02:04,160 --> 00:02:07,660
第二个和第三个卷积层也一样

41
00:02:07,660 --> 00:02:09,690
将最后生成的这个 x

42
00:02:09,690 --> 00:02:11,945
扁平化为向量形状

43
00:02:11,945 --> 00:02:15,840
从而能够将其当做输入传入一个全连接层

44
00:02:15,840 --> 00:02:19,190
我在这个扁平化层级和每个全连接层之间

45
00:02:19,190 --> 00:02:21,615
添加了一个丢弃层 以防止过拟合

46
00:02:21,615 --> 00:02:26,265
然后将扁平化的输入图像 x 传入第一个全连接层

47
00:02:26,265 --> 00:02:27,790
和所有隐藏层一样

48
00:02:27,790 --> 00:02:30,095
我应用了一个 ReLu 激活函数

49
00:02:30,095 --> 00:02:34,015
最后还有一个丢弃层和最后的全连接层

50
00:02:34,015 --> 00:02:37,510
生成的 x 应该是一个包含 10 个类别分数的列表

51
00:02:37,510 --> 00:02:41,050
最后实例化这个 model 并将其移到 GPU 上

52
00:02:41,050 --> 00:02:43,940
我在下面输出了 init 函数中的每个层级

53
00:02:43,940 --> 00:02:47,230
确保它们与预期相符

54
00:02:47,230 --> 00:02:50,235
这里显示了每个层级的输入和输出数量

55
00:02:50,235 --> 00:02:51,795
核大小 步长和填充大小

56
00:02:51,795 --> 00:02:53,545
一切都符合预期

57
00:02:53,545 --> 00:02:56,935
总结下 对于定义的每个卷积层

58
00:02:56,935 --> 00:03:00,115
我都在后面应用了 ReLu 函数和最大池化层

59
00:03:00,115 --> 00:03:01,730
在这一系列层级之后

60
00:03:01,730 --> 00:03:05,435
扁平化表示结果并传入全连接层

61
00:03:05,435 --> 00:03:10,245
添加一些丢弃层 最后生成 10 个类别分数

62
00:03:10,245 --> 00:03:12,265
这就是完整的模型

63
00:03:12,265 --> 00:03:13,990
现在开始训练

64
00:03:13,990 --> 00:03:16,340
我将使用标准交叉熵损失

65
00:03:16,340 --> 00:03:18,835
它适合这种分类任务

66
00:03:18,835 --> 00:03:20,975
并采用随机梯度下降法

67
00:03:20,975 --> 00:03:22,755
然后开始训练网络

68
00:03:22,755 --> 00:03:25,320
我将训练周期设为 30

69
00:03:25,320 --> 00:03:27,590
我通过观察训练和验证损失的下降情况

70
00:03:27,590 --> 00:03:30,420
确定了这个周期数

71
00:03:30,420 --> 00:03:34,090
可以看出 我甚至可以在第 20 个周期左右就停止训练

72
00:03:34,090 --> 00:03:37,250
因为这时候验证损失停止下降了

73
00:03:37,250 --> 00:03:38,780
我在这里保存了模型

74
00:03:38,780 --> 00:03:40,810
它的验证损失最低

75
00:03:40,810 --> 00:03:43,050
然后加载并测试该模型

76
00:03:43,050 --> 00:03:44,640
在测试时

77
00:03:44,640 --> 00:03:47,850
总体准确率约为 70%

78
00:03:47,850 --> 00:03:50,880
还好 至少比随意猜测要准得多

79
00:03:50,880 --> 00:03:53,025
这个任务真心不容易

80
00:03:53,025 --> 00:03:55,415
恭喜你终于学了这么多

81
00:03:55,415 --> 00:03:58,655
关于如何编写神经网络的知识

82
00:03:58,655 --> 00:04:01,565
当然 模型在某些类别上表现更好

83
00:04:01,565 --> 00:04:05,050
可以思考下为何会这样

84
00:04:05,050 --> 00:04:09,655
总之 我的模型似乎在分类汽车时比分类动物时效果要好

85
00:04:09,655 --> 00:04:12,770
可能是因为动物的颜色和大小变化很大

86
00:04:12,770 --> 00:04:15,350
如果有更多此类图像数据

87
00:04:15,350 --> 00:04:18,245
我也许能改善此模型

88
00:04:18,245 --> 00:04:21,260
添加另一个卷积层或许也有帮助

89
00:04:21,260 --> 00:04:24,165
看看能否从这些图像中提取更多复杂的图案

90
00:04:24,165 --> 00:04:28,140
我在这里进一步可视化哪些图像分类正确 哪些分类错误

91
00:04:28,140 --> 00:04:32,000
对于如何进一步优化 CNN 有很多思考余地

92
00:04:32,000 --> 00:04:33,125
建议你思考一下

93
00:04:33,125 --> 00:04:34,965
这是很好的学习机会

94
00:04:34,965 --> 00:04:37,065
我要提一下 在 2015 年

95
00:04:37,065 --> 00:04:39,200
有一场在线竞赛

96
00:04:39,200 --> 00:04:42,740
数据科学家对分类此数据库中的图像展开竞争

97
00:04:42,740 --> 00:04:44,905
获胜结构是 CNN

98
00:04:44,905 --> 00:04:48,055
它获得了 95% 以上的测试准确率

99
00:04:48,055 --> 00:04:51,695
在 GPU 上的训练时间约为 90 个小时

100
00:04:51,695 --> 00:04:53,935
不建议你在课堂上尝试这么训练

101
00:04:53,935 --> 00:04:58,270
这也证明了 要获得很高的准确率的确不易

