1
00:00:00,000 --> 00:00:02,790
加载数据并定义模型后

2
00:00:02,790 --> 00:00:06,835
下面将定义损失和优化函数

3
00:00:06,835 --> 00:00:09,745
对于这样的简单分类问题

4
00:00:09,745 --> 00:00:14,200
建议使用交叉熵损失和梯度下降法

5
00:00:14,200 --> 00:00:16,380
同时 建议你阅读

6
00:00:16,380 --> 00:00:20,279
关于各种损失函数和优化器的文档

7
00:00:20,279 --> 00:00:22,960
下面我们来看看损失文档

8
00:00:22,960 --> 00:00:25,570
文档中介绍了几种不同的损失

9
00:00:25,570 --> 00:00:28,100
这部分是交叉熵损失

10
00:00:28,100 --> 00:00:32,355
文档提到交叉熵函数

11
00:00:32,355 --> 00:00:37,395
会对输出层执行 softmax 函数 然后执行负对数损失

12
00:00:37,395 --> 00:00:40,760
所以 模型只需生成类别分数

13
00:00:40,760 --> 00:00:43,310
然后这个损失函数

14
00:00:43,310 --> 00:00:46,715
会使用 softmax 函数将它们变成概率并计算损失

15
00:00:46,715 --> 00:00:49,985
文档详细介绍了这些计算流程

16
00:00:49,985 --> 00:00:52,670
这里还提到

17
00:00:52,670 --> 00:00:55,310
损失等于一批图像的平均值

18
00:00:55,310 --> 00:00:59,510
如果模型在训练中观察了一批 20 张图像

19
00:00:59,510 --> 00:01:03,290
那么返回的损失将是这 20 张图像的平均损失

20
00:01:03,290 --> 00:01:04,760
现在回到 notebook

21
00:01:04,760 --> 00:01:07,040
损失和优化函数定义了

22
00:01:07,040 --> 00:01:09,955
网络在训练过程中更新权重的方式

23
00:01:09,955 --> 00:01:12,795
下面是实际训练循环

24
00:01:12,795 --> 00:01:15,450
这是训练周期数

25
00:01:15,450 --> 00:01:17,630
周期数表示我们希望模型

26
00:01:17,630 --> 00:01:19,930
遍历整个训练数据集的次数

27
00:01:19,930 --> 00:01:23,960
1 个周期表示仅查看每张训练图像一次

28
00:01:23,960 --> 00:01:27,320
对于此数据集 建议至少设为 20 个周期

29
00:01:27,320 --> 00:01:30,290
在尝试不同的模型架构时

30
00:01:30,290 --> 00:01:32,630
有必要从很小的周期数开始

31
00:01:32,630 --> 00:01:35,495
训练最终模型时 再增加周期数

32
00:01:35,495 --> 00:01:39,870
然后遍历每个周期并跟踪训练损失

33
00:01:39,870 --> 00:01:42,800
在这个周期循环里是批次循环

34
00:01:42,800 --> 00:01:46,490
在批次循环里 训练加载器将加载一批训练数据

35
00:01:46,490 --> 00:01:50,270
我们可以查看该批次中的每个图像及其真实标签

36
00:01:50,270 --> 00:01:51,680
在开始批次循环时

37
00:01:51,680 --> 00:01:54,430
我们使用 optimizer.zero_grad 清除

38
00:01:54,430 --> 00:01:57,565
PyTorch 积累的所有梯度计算结果

39
00:01:57,565 --> 00:02:01,060
然后传入模型并执行前向流程

40
00:02:01,060 --> 00:02:03,655
模型从训练加载器中获得输入图像

41
00:02:03,655 --> 00:02:08,270
然后返回预测的类别分数

42
00:02:08,270 --> 00:02:09,320
称之为 output

43
00:02:09,320 --> 00:02:11,810
接下来使用定义的损失函数

44
00:02:11,810 --> 00:02:15,270
对比预测输出和真实标签 (target)

45
00:02:15,270 --> 00:02:17,540
这些标签也来自训练加载器

46
00:02:17,540 --> 00:02:23,415
我们对比一批输出和目标标签 然后计算交叉熵损失

47
00:02:23,415 --> 00:02:25,850
要完成反向传播步骤

48
00:02:25,850 --> 00:02:28,730
我们执行反向流程并计算损失的梯度

49
00:02:28,730 --> 00:02:32,015
然后执行一个优化步骤

50
00:02:32,015 --> 00:02:36,755
这一步负责更新网络的权重值

51
00:02:36,755 --> 00:02:39,595
最后 计算训练损失

52
00:02:39,595 --> 00:02:43,610
这个 loss.item 是一批数据的平均损失值

53
00:02:43,610 --> 00:02:47,715
在此例中 我们希望记录一批数据的累积损失

54
00:02:47,715 --> 00:02:49,035
暂时不需要平均损失

55
00:02:49,035 --> 00:02:52,970
因此我将乘以批次大小 data.size(0)

56
00:02:52,970 --> 00:02:56,810
然后 在完成这个循环并经过一整个周期后

57
00:02:56,810 --> 00:02:59,760
我将计算该周期的平均损失

58
00:02:59,760 --> 00:03:01,190
我将用累积损失

59
00:03:01,190 --> 00:03:05,310
除以训练集中的图像总数

60
00:03:05,310 --> 00:03:09,560
返回平均训练损失 我将在每个周期之后输出该损失

61
00:03:09,560 --> 00:03:12,620
注意 这只是计算平均损失的一种方式

62
00:03:12,620 --> 00:03:14,365
你可能会在示例代码中

63
00:03:14,365 --> 00:03:16,065
看到其他计算方式

64
00:03:16,065 --> 00:03:20,380
例如 你可以在这个批次循环的任何时刻计算平均损失

65
00:03:20,380 --> 00:03:23,540
前提是将每个输入图像的损失加起来

66
00:03:23,540 --> 00:03:27,550
然后用累积损失除以网络看到的图像数量

67
00:03:27,550 --> 00:03:31,040
运行这个训练循环多个周期后

68
00:03:31,040 --> 00:03:34,730
你将能够使用之前未见过的测试数据测试模型

69
00:03:34,730 --> 00:03:38,810
我提供了一些代码 使你能够遍历测试加载器中的测试数据

70
00:03:38,810 --> 00:03:41,245
并将训练的模型应用到该数据上

71
00:03:41,245 --> 00:03:43,490
这段代码将帮助你了解

72
00:03:43,490 --> 00:03:45,920
对于数据集中的每个类别 模型表现如何

73
00:03:45,920 --> 00:03:48,200
接下来你将有机会查看这个练习的代码

74
00:03:48,200 --> 00:03:52,500
以及创建图像分类 MLP 的练习答案

75
00:03:52,500 --> 00:03:54,665
建议你在尝试自己构建网络之后

76
00:03:54,665 --> 00:03:57,530
再查看答案

