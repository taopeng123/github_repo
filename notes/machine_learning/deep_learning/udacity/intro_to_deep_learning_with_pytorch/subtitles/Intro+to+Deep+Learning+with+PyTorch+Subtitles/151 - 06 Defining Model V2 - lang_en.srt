1
00:00:00,000 --> 00:00:04,745
All right. So, we have our mini batches of data and now it's time to define our model.

2
00:00:04,745 --> 00:00:08,115
This is a little diagram of what the model will look like.

3
00:00:08,115 --> 00:00:12,915
We'll have our character's put into our input layer and then a stack of LSTM cells.

4
00:00:12,915 --> 00:00:17,970
These LSTM cells make up our hidden recurrent layer and when they look at a mini batch of

5
00:00:17,970 --> 00:00:20,130
data as input they'll look at one character at

6
00:00:20,130 --> 00:00:22,920
a time and produce an output and a hidden state.

7
00:00:22,920 --> 00:00:25,500
So, we will pass an input character into

8
00:00:25,500 --> 00:00:28,575
our first LSTM cell which produces a hidden state.

9
00:00:28,575 --> 00:00:30,250
Then at the next time step,

10
00:00:30,250 --> 00:00:33,450
we'll look at the next character in our sequence and pass that into

11
00:00:33,450 --> 00:00:37,495
this LSTM cell which will see the previous hidden state as input.

12
00:00:37,495 --> 00:00:42,170
You have so far seen this behavior in a one layer RNN but in this case we

13
00:00:42,170 --> 00:00:46,520
plan on using a two-layer model that has stacked LSTM layers and

14
00:00:46,520 --> 00:00:50,270
that means that the output of this LSTM layer is going to go to the next one as

15
00:00:50,270 --> 00:00:52,790
input and each of these cells is sharing

16
00:00:52,790 --> 00:00:56,330
its hidden state with the next cell in the unrolled series.

17
00:00:56,330 --> 00:00:59,930
Finally, the output of the last LSTM layer will include

18
00:00:59,930 --> 00:01:03,635
some character class scores that will be the length of our vocabulary.

19
00:01:03,635 --> 00:01:07,730
We'll put this through a Softmax activation function which we'll use to get

20
00:01:07,730 --> 00:01:11,660
the probability distribution for predicting the most likely next character.

21
00:01:11,660 --> 00:01:14,320
So, to start you off on this task,

22
00:01:14,320 --> 00:01:17,015
you've been given some skeleton code for creating a model.

23
00:01:17,015 --> 00:01:19,640
First, we're going to check to see if a GPU is

24
00:01:19,640 --> 00:01:23,370
available for training then you'll see this class character RNN.

25
00:01:23,370 --> 00:01:26,270
You can see that this character RNN class has

26
00:01:26,270 --> 00:01:29,190
our usual init and forward functions and later

27
00:01:29,190 --> 00:01:31,910
you've been given some code to initialize the hidden state of

28
00:01:31,910 --> 00:01:35,070
an LSTM layer and I'll go over this in a moment.

29
00:01:35,070 --> 00:01:38,240
You can definitely take a look at this given code and how we're

30
00:01:38,240 --> 00:01:41,770
creating our initial character dictionaries but you won't need to change it.

31
00:01:41,770 --> 00:01:45,170
We also have several parameters that are going to be passed in when a character

32
00:01:45,170 --> 00:01:49,555
RNN is instantiated and I've saved some of these as class variables.

33
00:01:49,555 --> 00:01:52,130
So, using these input parameters and variables,

34
00:01:52,130 --> 00:01:55,990
it will be up to you to create our model layers and complete the forward function.

35
00:01:55,990 --> 00:02:00,420
You'll first create an LSTM layer which you can read about in the documentation here.

36
00:02:00,420 --> 00:02:04,510
We can see that an LSTM layer is created using our usual parameters;

37
00:02:04,510 --> 00:02:06,330
an input size, hidden size,

38
00:02:06,330 --> 00:02:08,930
number of layers, and a batch first parameter.

39
00:02:08,930 --> 00:02:11,140
We'll also add a dropout value.

40
00:02:11,140 --> 00:02:14,210
This introduces a dropout layer in between the outputs

41
00:02:14,210 --> 00:02:17,855
of LSTM layers if you've decided to stack multiple layers.

42
00:02:17,855 --> 00:02:20,210
So, after you define an LSTM layer,

43
00:02:20,210 --> 00:02:22,410
I'll ask you to define two more layers;

44
00:02:22,410 --> 00:02:27,320
one dropout layer and a final fully-connected layer for getting our desired output size.

45
00:02:27,320 --> 00:02:28,740
Once you've defined these layers,

46
00:02:28,740 --> 00:02:31,000
you'll move on to define the forward function.

47
00:02:31,000 --> 00:02:33,770
This takes in an input x and hidden state.

48
00:02:33,770 --> 00:02:35,900
You'll pass this input through the layers of

49
00:02:35,900 --> 00:02:38,975
the model and return a final output and hidden state.

50
00:02:38,975 --> 00:02:41,540
You'll have to make sure to shape the LSTM output

51
00:02:41,540 --> 00:02:44,350
so that it can be fed into the last fully connected layer.

52
00:02:44,350 --> 00:02:45,940
Okay. Then at the bottom here,

53
00:02:45,940 --> 00:02:49,720
you'll see this function for initializing the hidden state of an LSTM.

54
00:02:49,720 --> 00:02:54,775
An LSTM has a hidden and a cell state that are saved as a tuple hidden.

55
00:02:54,775 --> 00:02:57,290
The shape of the hidden and cell state is

56
00:02:57,290 --> 00:02:59,840
defined first by the number of layers in our model,

57
00:02:59,840 --> 00:03:01,420
the batch size of our input,

58
00:03:01,420 --> 00:03:04,730
and then the hidden dimension that we specified in model creation.

59
00:03:04,730 --> 00:03:07,610
In this function, we're initializing the hidden weights all to

60
00:03:07,610 --> 00:03:10,250
zero and moving them to GPU if it's available.

61
00:03:10,250 --> 00:03:13,290
Okay, so all the code that you see you don't need to change,

62
00:03:13,290 --> 00:03:16,690
you just need to define the model layers and feedforward behavior.

63
00:03:16,690 --> 00:03:18,555
If you've implemented this correctly,

64
00:03:18,555 --> 00:03:21,065
you should be able to set your model hyperparameters

65
00:03:21,065 --> 00:03:24,230
and proceed with training and generating some sample text.

66
00:03:24,230 --> 00:03:26,360
Try this out on your own then next,

67
00:03:26,360 --> 00:03:27,800
check out my solution.

