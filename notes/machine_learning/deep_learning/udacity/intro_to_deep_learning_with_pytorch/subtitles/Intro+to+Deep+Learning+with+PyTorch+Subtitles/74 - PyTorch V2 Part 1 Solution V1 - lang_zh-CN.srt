1
00:00:00,000 --> 00:00:03,540
下面说说我是如何

2
00:00:03,540 --> 00:00:08,190
计算这个小型简单神经网络输出的

3
00:00:08,190 --> 00:00:12,330
我们需要将 features 与 weights 相乘

4
00:00:12,330 --> 00:00:14,070
即 features * weights

5
00:00:14,070 --> 00:00:18,225
它们是张量 基本上和 NumPy 数组一样

6
00:00:18,225 --> 00:00:19,845
如果你之前用过 NumPy   应该就会很熟悉

7
00:00:19,845 --> 00:00:22,770
features 与 weights 相乘时

8
00:00:22,770 --> 00:00:25,320
两者里的第一个元素互相相乘

9
00:00:25,320 --> 00:00:27,420
然后是两者里的第二个元素互相相乘

10
00:00:27,420 --> 00:00:29,895
依次进行 

11
00:00:29,895 --> 00:00:33,060
得出新的张量

12
00:00:33,060 --> 00:00:37,020
然后运行 torch.sum 得出一个和

13
00:00:37,020 --> 00:00:41,100
加上偏差项 传入激活函数并得出 y

14
00:00:41,100 --> 00:00:44,720
再介绍一种方法 同样是 features * weights

15
00:00:44,720 --> 00:00:46,295
创建另一个张量

16
00:00:46,295 --> 00:00:48,710
而张量有一个 .sum 方法

17
00:00:48,710 --> 00:00:53,645
你可以传入张量   运行 .sum    它会对该张量里的所有值求和

18
00:00:53,645 --> 00:00:56,550
因此 我们可以运行 torch.sum

19
00:00:56,550 --> 00:00:58,605
或者采用此方法

20
00:00:58,605 --> 00:01:01,430
对张量运行 .sum 并求和

21
00:01:01,430 --> 00:01:04,645
同样 传入激活函数

22
00:01:04,645 --> 00:01:06,440
这部分的作用是

23
00:01:06,440 --> 00:01:11,080
用两种方式进行元素级乘法运算并求和

24
00:01:11,080 --> 00:01:13,180
相乘 然后求和

25
00:01:13,180 --> 00:01:17,420
但实际上可以使用矩阵乘法进行相同的运算

26
00:01:17,420 --> 00:01:19,520
大多数情况下

27
00:01:19,520 --> 00:01:22,070
都建议使用矩阵乘法

28
00:01:22,070 --> 00:01:24,365
因为矩阵乘法更高效

29
00:01:24,365 --> 00:01:30,145
这些线性代数运算已经使用现代库加快了速度

30
00:01:30,145 --> 00:01:32,985
例如在 GPU 上运行的 CUDA

31
00:01:32,985 --> 00:01:37,550
要在 PyTorch 中对 features 和 weights 这两个张量进行矩阵乘法运算

32
00:01:37,550 --> 00:01:40,310
我们可以采用两种方法

33
00:01:40,310 --> 00:01:44,875
分别是 torch.mm 和 torch.matmul

34
00:01:44,875 --> 00:01:48,590
对于 torch.mm  矩阵乘法更简单

35
00:01:48,590 --> 00:01:52,760
并且对传入的张量要求更严格

36
00:01:52,760 --> 00:01:56,420
对于 torch.matmul 它实际上支持广播

37
00:01:56,420 --> 00:02:02,029
如果你传入大小/形状很奇怪的张量

38
00:02:02,029 --> 00:02:05,665
那么你可能会获得意料之外的输出结果

39
00:02:05,665 --> 00:02:09,495
因此我更愿意使用 torch.mm

40
00:02:09,495 --> 00:02:12,520
它能按照我的预期方式进行运算

41
00:02:12,520 --> 00:02:15,710
如果我操作错了 它就报错

42
00:02:15,710 --> 00:02:19,085
而不是继续运算

43
00:02:19,085 --> 00:02:21,665
但是 如果我们尝试

44
00:02:21,665 --> 00:02:25,175
对 features 和 weights 使用 torch.mm  就会出错

45
00:02:25,175 --> 00:02:28,975
这里显示 RuntimeError: size mismatch

46
00:02:28,975 --> 00:02:33,485
表示我们向 torch.mm 传入了两个张量

47
00:02:33,485 --> 00:02:35,810
但是大小不匹配 它不能进行矩阵乘法运算

48
00:02:35,810 --> 00:02:39,815
并在这里列出了大小

49
00:02:39,815 --> 00:02:42,155
第一个张量

50
00:02:42,155 --> 00:02:45,905
m1 是 [1 x 5] 第二个张量也是 [1 x 5]

51
00:02:45,905 --> 00:02:51,360
如果你还记得线性代数课程的知识

52
00:02:51,360 --> 00:02:53,425
就会发现在进行矩阵乘法运算时

53
00:02:53,425 --> 00:02:56,690
第一个矩阵的列数

54
00:02:56,690 --> 00:03:00,520
必须等于第二个矩阵的行数

55
00:03:00,520 --> 00:03:04,245
因此 weights 张量

56
00:03:04,245 --> 00:03:07,765
必须是 [5 x 1] 而不是 [1 x 5]

57
00:03:07,765 --> 00:03:11,339
在构建神经网络时

58
00:03:11,339 --> 00:03:13,365
如果要查看张量的形状

59
00:03:13,365 --> 00:03:15,210
你可以使用 tensor.shape

60
00:03:15,210 --> 00:03:18,500
我们以后会频繁用到这个方法 不仅在 PyTorch 里

61
00:03:18,500 --> 00:03:22,890
在 TensorFlow 和其他深度学习框架中也不例外

62
00:03:22,890 --> 00:03:25,760
在构建神经网络时 最常见的错误就是形状错误

63
00:03:25,760 --> 00:03:28,910
所以 在设计神经网络架构时

64
00:03:28,910 --> 00:03:32,240
很重要的步骤就是

65
00:03:32,240 --> 00:03:35,780
使张量的形状保持匹配

66
00:03:35,780 --> 00:03:38,014
也就是说 在调试过程中

67
00:03:38,014 --> 00:03:39,980
很大一部分工作是

68
00:03:39,980 --> 00:03:42,635
查看传入网络中的张量的形状

69
00:03:42,635 --> 00:03:45,170
记得用 tensor.shape()

70
00:03:45,170 --> 00:03:47,690
要变形张量

71
00:03:47,690 --> 00:03:49,790
通常可以采用

72
00:03:49,790 --> 00:03:51,860
三种不同的方法

73
00:03:51,860 --> 00:03:53,045
也就是这三种方法

74
00:03:53,045 --> 00:03:56,075
Reshape() resize() 和 view()

75
00:03:56,075 --> 00:03:58,205
原理基本相通 即

76
00:03:58,205 --> 00:04:01,105
对于 weights.reshape

77
00:04:01,105 --> 00:04:04,805
传入想要的新形状

78
00:04:04,805 --> 00:04:05,930
在这个练习中

79
00:04:05,930 --> 00:04:08,900
我们要将 weights 变成 5x1 的矩阵

80
00:04:08,900 --> 00:04:11,630
输入 .reshape(5, 1)

81
00:04:11,630 --> 00:04:15,320
reshape 的作用是

82
00:04:15,320 --> 00:04:18,750
返回一个新的张量 其中的数据和 weights 的一样

83
00:04:18,750 --> 00:04:22,595
即在内存地址中存储的相同数据

84
00:04:22,595 --> 00:04:24,275
它将创建一个新的张量

85
00:04:24,275 --> 00:04:27,775
形状是你要求的形状

86
00:04:27,775 --> 00:04:31,775
但内存中的实际数据没有改变

87
00:04:31,775 --> 00:04:33,740
这种情况有时候会发生

88
00:04:33,740 --> 00:04:38,570
有时候 它会返回克隆版本

89
00:04:38,570 --> 00:04:41,090
也就是说 它将数据复制到内存的另一个部分

90
00:04:41,090 --> 00:04:44,215
然后返回在此内存部分存储的张量

91
00:04:44,215 --> 00:04:47,050
也就是说

92
00:04:47,050 --> 00:04:50,080
复制数据比直接更改张量形状（不克隆数据）

93
00:04:50,080 --> 00:04:54,080
效率要低

94
00:04:54,080 --> 00:04:55,260
要这么做

95
00:04:55,260 --> 00:04:58,050
我们可以使用 resize_   末尾有个下划线

96
00:04:58,050 --> 00:05:02,884
下划线表示这个方法是原地操作

97
00:05:02,884 --> 00:05:04,235
原地操作是指

98
00:05:04,235 --> 00:05:06,220
根本不更改数据

99
00:05:06,220 --> 00:05:08,140
只是更改

100
00:05:08,140 --> 00:05:12,575
位于该内存地址中的数据对应的张量

101
00:05:12,575 --> 00:05:16,435
resize_ 方法的问题在于

102
00:05:16,435 --> 00:05:20,560
如果请求的形状比原始张量的元素多或少

103
00:05:20,560 --> 00:05:23,470
你可能会丢失数据

104
00:05:23,470 --> 00:05:26,305
或者使用未初始化的内存

105
00:05:26,305 --> 00:05:30,005
创建虚假的数据

106
00:05:30,005 --> 00:05:32,510
通常这时候

107
00:05:32,510 --> 00:05:35,780
如果有一个方法 

108
00:05:35,780 --> 00:05:38,030
能在你把原始数量的元素更改为不同数量的元素的时候

109
00:05:38,030 --> 00:05:41,555
进行报错 是不是很好

110
00:05:41,555 --> 00:05:43,645
我们其实有这么个方法 就是 .view

111
00:05:43,645 --> 00:05:46,870
.view 是我频繁使用的一个方法

112
00:05:46,870 --> 00:05:49,535
它会返回一个新张量

113
00:05:49,535 --> 00:05:52,985
包含的数据和 weights 在内存中的一样

114
00:05:52,985 --> 00:05:55,345
对的

115
00:05:55,345 --> 00:05:57,620
它只是返回一个新的张量

116
00:05:57,620 --> 00:06:00,155
不会更改内存中的任何数据

117
00:06:00,155 --> 00:06:03,755
如果你想获取新的尺寸

118
00:06:03,755 --> 00:06:08,250
使张量具有新的形状和不同数量的元素

119
00:06:08,250 --> 00:06:09,885
就会报错

120
00:06:09,885 --> 00:06:12,910
使用 .view 可以确保

121
00:06:12,910 --> 00:06:15,035
在更改 weights 的形状时

122
00:06:15,035 --> 00:06:17,905
始终获得相同数量的元素

123
00:06:17,905 --> 00:06:21,740
这就是我在变形张量时 会用到这个方法的原因

124
00:06:21,740 --> 00:06:23,555
好了 总结下

125
00:06:23,555 --> 00:06:27,110
如果你要将 weights 变形为 5 行和 1 列

126
00:06:27,110 --> 00:06:30,420
可以输入 weights.view(5, 1)

127
00:06:30,420 --> 00:06:33,155
现在 你已经知道如何改变张量的形状

128
00:06:33,155 --> 00:06:37,280
以及如何进行矩阵乘法运算

129
00:06:37,280 --> 00:06:39,830
我希望你能使用矩阵乘法

130
00:06:39,830 --> 00:06:43,330
计算这个小型神经网络的输出

