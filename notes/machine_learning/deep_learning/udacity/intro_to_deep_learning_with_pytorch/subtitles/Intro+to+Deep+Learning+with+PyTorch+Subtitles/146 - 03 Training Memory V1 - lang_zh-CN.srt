1
00:00:00,000 --> 00:00:02,560
上次我们定义了一个模型

2
00:00:02,560 --> 00:00:06,380
下面我将实例化该模型并使用训练数据训练它

3
00:00:06,380 --> 00:00:09,120
首先指定模型超参数

4
00:00:09,120 --> 00:00:11,160
输入和输出大小为 1

5
00:00:11,160 --> 00:00:14,550
一次处理和生成一个序列

6
00:00:14,550 --> 00:00:17,310
然后指定隐藏维度

7
00:00:17,310 --> 00:00:20,250
表示希望 RNN 层级生成的特征数量

8
00:00:20,250 --> 00:00:23,065
设为 32 但是对于这样的小型数据集来说

9
00:00:23,065 --> 00:00:25,105
甚至可以设为更小的值

10
00:00:25,105 --> 00:00:27,345
暂时将 n_layers 设为 1

11
00:00:27,345 --> 00:00:29,880
表示不叠加任何 RNN 层级

12
00:00:29,880 --> 00:00:31,915
创建并输出此 RNN

13
00:00:31,915 --> 00:00:33,890
结果应该是我期望的值

14
00:00:33,890 --> 00:00:36,620
这是 RNN 层级的输入大小和隐藏维度

15
00:00:36,620 --> 00:00:40,600
这是线性层级的输入特征数量和输出数量

16
00:00:40,600 --> 00:00:44,090
在开始训练之前 定义损失和优化函数

17
00:00:44,090 --> 00:00:46,730
我们将训练模型生成数据点

18
00:00:46,730 --> 00:00:49,830
这些数据点是坐标值

19
00:00:49,830 --> 00:00:52,640
为了比较预测数据点和真实数据点

20
00:00:52,640 --> 00:00:54,680
我们将使用回归损失

21
00:00:54,680 --> 00:00:57,670
因为它只是一个数量

22
00:00:57,670 --> 00:00:58,970
而不是类别概率

23
00:00:58,970 --> 00:01:00,980
我将使用均方误差损失

24
00:01:00,980 --> 00:01:03,650
它会衡量两个点之间的距离

25
00:01:03,650 --> 00:01:06,990
我将使用 Adam 优化器 它是递归模型的标准优化器

26
00:01:06,990 --> 00:01:09,655
传入参数和学习速率

27
00:01:09,655 --> 00:01:12,530
下面是 train 函数

28
00:01:12,530 --> 00:01:15,320
它将接受一个 rnn 和训练步数

29
00:01:15,320 --> 00:01:19,315
以及何时输出损失信息的参数

30
00:01:19,315 --> 00:01:21,560
在此函数开头

31
00:01:21,560 --> 00:01:23,205
初始化隐藏状态

32
00:01:23,205 --> 00:01:27,470
一开始设为 None隐藏状态默认全为 0

33
00:01:27,470 --> 00:01:29,785
然后是批次循环

34
00:01:29,785 --> 00:01:31,705
这个循环不太符合常规

35
00:01:31,705 --> 00:01:33,440
根据训练步数

36
00:01:33,440 --> 00:01:35,880
动态生成数据

37
00:01:35,880 --> 00:01:40,635
在这几行一次生成 20 个正弦波值序列

38
00:01:40,635 --> 00:01:42,990
和在开头生成数据一样

39
00:01:42,990 --> 00:01:45,500
在这里获取输入 x 以及目标 y

40
00:01:45,500 --> 00:01:48,110
y 应该比 x 晚一个时间步

41
00:01:48,110 --> 00:01:51,140
在这里将数据转换为张量

42
00:01:51,140 --> 00:01:55,275
unsqueeze 张量 x 的第一个维度 使其批次大小为 1

43
00:01:55,275 --> 00:01:58,280
然后将输入张量传入 rnn 模型

44
00:01:58,280 --> 00:02:03,020
rnn 接受的是输入张量 x 和初始隐藏状态

45
00:02:03,020 --> 00:02:06,820
生成预测参数和新的隐藏状态

46
00:02:06,820 --> 00:02:08,875
下面是很重要的部分

47
00:02:08,875 --> 00:02:11,870
当我们在下个时间步再次循环时

48
00:02:11,870 --> 00:02:14,860
我希望将此新隐藏状态当做输入传入 rnn 中

49
00:02:14,860 --> 00:02:19,660
将生成的隐藏状态中的值复制到新的变量中

50
00:02:19,660 --> 00:02:23,330
这样可以将隐藏状态与其历史记录分离开

51
00:02:23,330 --> 00:02:26,810
不用反向经过一系列累积的隐藏状态

52
00:02:26,810 --> 00:02:29,540
所以在下个时间步（序列的下个数据点）

53
00:02:29,540 --> 00:02:33,790
将它传入 rnn 中

54
00:02:33,790 --> 00:02:36,180
然后是正常的训练命令

55
00:02:36,180 --> 00:02:38,630
清零累积的梯度

56
00:02:38,630 --> 00:02:42,720
计算损失 执行反向传播和优化步骤

57
00:02:42,720 --> 00:02:45,320
在下面输出损失

58
00:02:45,320 --> 00:02:47,770
并显示输入和预测输出

59
00:02:47,770 --> 00:02:50,375
最后 此函数返回训练过的 rnn

60
00:02:50,375 --> 00:02:53,030
你可以保存该模型

61
00:02:53,030 --> 00:02:55,115
我们来运行下代码

62
00:02:55,115 --> 00:02:56,840
训练在上面定义的 rnn

63
00:02:56,840 --> 00:02:59,710
并训练 75 步

64
00:02:59,710 --> 00:03:02,480
每隔 15 步输出结果

65
00:03:02,480 --> 00:03:05,390
这里输出了均方误差损失

66
00:03:05,390 --> 00:03:09,080
并显示了红色输入值和蓝色输出值之间的区别

67
00:03:09,080 --> 00:03:11,870
我们希望蓝色输出值

68
00:03:11,870 --> 00:03:14,560
比红色输入值晚一个时间步

69
00:03:14,560 --> 00:03:16,500
所以一开始完全错了

70
00:03:16,500 --> 00:03:20,230
在 15 个时间步之后 损失降低了很多

71
00:03:20,230 --> 00:03:22,760
蓝色虚线更接近红色虚线

72
00:03:22,760 --> 00:03:27,565
继续训练之后 蓝色预测虚线与已知目标更接近

73
00:03:27,565 --> 00:03:29,370
在 75 个周期之后

74
00:03:29,370 --> 00:03:30,840
损失很低了

75
00:03:30,840 --> 00:03:34,310
蓝色虚线和已知目标/真实输出很相似

76
00:03:34,310 --> 00:03:37,565
查看同一时间步的红色输入点

77
00:03:37,565 --> 00:03:38,890
和蓝色输入点

78
00:03:38,890 --> 00:03:40,520
应该看到蓝色输入

79
00:03:40,520 --> 00:03:43,895
比红色输入晚了一个时间步很接近了

80
00:03:43,895 --> 00:03:47,000
如果训练更多个时间步或添加更多层级

81
00:03:47,000 --> 00:03:50,680
RNN 的效果应该会更好

82
00:03:50,680 --> 00:03:51,945
在此视频中

83
00:03:51,945 --> 00:03:55,850
我希望能演示简单 RNN 的基本结构

84
00:03:55,850 --> 00:04:00,010
并演示如何在训练过程中记录隐藏状态及表示记忆功能

85
00:04:00,010 --> 00:04:02,990
你可以对其他数据执行非常相似的操作

86
00:04:02,990 --> 00:04:07,010
例如全球气温或股价 这些数据更复杂

87
00:04:07,010 --> 00:04:08,990
查看给定这些数据

88
00:04:08,990 --> 00:04:11,120
能否预测未来情形会很有趣

89
00:04:11,120 --> 00:04:13,100
这只是一个例子

90
00:04:13,100 --> 00:04:15,135
你可以在我们的程序 GitHub 中查看此代码

91
00:04:15,135 --> 00:04:16,520
下方提供了链接

92
00:04:16,520 --> 00:04:17,780
建议你尝试不同的模型参数

93
00:04:17,780 --> 00:04:20,390
直到你充分了解

94
00:04:20,390 --> 00:04:22,010
RNN 输入和输出的维度

95
00:04:22,010 --> 00:04:25,570
明白超参数会如何变化以及如何训练模型

96
00:04:25,570 --> 00:04:29,670
接下来 我和 Matt 将介绍如何生成文本

