1
00:00:00,000 --> 00:00:03,140
在 Google 搜索框中输入 mlp for mnist 后

2
00:00:03,140 --> 00:00:04,515
出现了一些搜索结果

3
00:00:04,515 --> 00:00:07,440
包括几条代码实现

4
00:00:07,440 --> 00:00:12,215
第一条结果来自官方 keras GitHub 代码库 看起来很可靠

5
00:00:12,215 --> 00:00:13,965
滚动浏览代码

6
00:00:13,965 --> 00:00:17,400
他们导入了 MNIST 数据库

7
00:00:17,400 --> 00:00:21,450
并将每个输入图像扁平化为大小是 784 的向量

8
00:00:21,450 --> 00:00:26,740
他们使用一个隐藏层将输入转换成 512 个节点

9
00:00:26,740 --> 00:00:29,670
并且应用了激活函数 即 ReLu 函数

10
00:00:29,670 --> 00:00:35,090
还有一个隐藏层 也是 512 个节点

11
00:00:35,090 --> 00:00:37,790
中间还有一些丢弃层

12
00:00:37,790 --> 00:00:41,445
这个 0.2 表示丢弃率是 20%

13
00:00:41,445 --> 00:00:45,315
即在训练过程中节点被关闭的概率是 20%

14
00:00:45,315 --> 00:00:49,475
丢弃层可以用来避免过拟合数据

15
00:00:49,475 --> 00:00:52,625
最后还有一个全连接层

16
00:00:52,625 --> 00:00:56,380
作用是为几个类别生成输出向量 长度为 10

17
00:00:56,380 --> 00:00:58,170
当我看到这种源代码时

18
00:00:58,170 --> 00:01:00,745
我首先会思考这些代码合理吗

19
00:01:00,745 --> 00:01:03,860
向网络中添加的隐藏层越多

20
00:01:03,860 --> 00:01:07,090
网络能检测到的规律越复杂

21
00:01:07,090 --> 00:01:09,965
但是我不希望引入不必要的复杂性

22
00:01:09,965 --> 00:01:13,130
我的直觉是对于小图像

23
00:01:13,130 --> 00:01:14,960
两个隐藏层听起来很合理

24
00:01:14,960 --> 00:01:19,835
这只是手写数字分类任务的一种解决方案

25
00:01:19,835 --> 00:01:23,805
对于这种问题 下一步是

26
00:01:23,805 --> 00:01:28,445
继续搜索 看看能否找到你感兴趣的其他结构

27
00:01:28,445 --> 00:01:31,250
找到一两个你喜欢的模型后

28
00:01:31,250 --> 00:01:34,185
在代码中尝试它们 看看它们的效果如何

29
00:01:34,185 --> 00:01:36,620
这个模型对我来说足够好了

30
00:01:36,620 --> 00:01:38,990
我将继续根据这个结构

31
00:01:38,990 --> 00:01:42,270
定义一个有两个隐藏层的 MLP

