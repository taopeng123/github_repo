1
00:00:00,000 --> 00:00:02,730
我们已经学习了

2
00:00:02,730 --> 00:00:06,415
如何训练 MLP 对 MNIST 数据集中的手写数字进行分类

3
00:00:06,415 --> 00:00:08,765
并且获得了很高的测试准确率

4
00:00:08,765 --> 00:00:11,405
MLP 很适合解决此类问题

5
00:00:11,405 --> 00:00:14,970
下面的链接部分提供了 MNIST 分类任务的其他解决方案

6
00:00:14,970 --> 00:00:19,000
以及它们在测试数据集上的错误率

7
00:00:19,000 --> 00:00:21,650
你会发现最佳算法或测试错误率最低的方案

8
00:00:21,650 --> 00:00:26,795
使用的是卷积神经网络 (CNN)

9
00:00:26,795 --> 00:00:29,710
实际上 对于大多数图像分类任务来说

10
00:00:29,710 --> 00:00:32,615
CNN 和 MLP 不能相提并论

11
00:00:32,615 --> 00:00:34,420
CNN 效果好多了

12
00:00:34,420 --> 00:00:36,795
MNIST 数据库是一个特例

13
00:00:36,795 --> 00:00:39,190
因为它非常整洁并且经过预处理

14
00:00:39,190 --> 00:00:45,245
所有数字图像大小大致相等 并且在 28x28 像素网格中居中

15
00:00:45,245 --> 00:00:47,660
假设要分类的数字图像

16
00:00:47,660 --> 00:00:50,480
不是这么整洁

17
00:00:50,480 --> 00:00:53,785
数字可能会出现中网格中的任何位置

18
00:00:53,785 --> 00:00:56,730
有时候很小 有时候很大

19
00:00:56,730 --> 00:00:59,830
这对 MLP 来说将更有挑战

20
00:00:59,830 --> 00:01:02,760
在现实中 图像数据很混乱

21
00:01:02,760 --> 00:01:05,670
对于这类图像 CNN 的效果比 MLP 强很多

22
00:01:05,670 --> 00:01:08,270
解释下原因

23
00:01:08,270 --> 00:01:11,080
为了将图像传入 MLP 中

24
00:01:11,080 --> 00:01:13,665
必须首先将图像转换为向量

25
00:01:13,665 --> 00:01:16,340
MLP 然后将转换后的图像

26
00:01:16,340 --> 00:01:19,045
当做简单的数字向量 没有特殊结构

27
00:01:19,045 --> 00:01:20,930
它不知道这些数字

28
00:01:20,930 --> 00:01:23,925
原先是位于网格空间里的数字

29
00:01:23,925 --> 00:01:27,320
相反 CNN 可以专门处理这种情形

30
00:01:27,320 --> 00:01:31,175
CNN 能够分析多维数据中的规律

31
00:01:31,175 --> 00:01:35,750
与 MLP 不同，CNN 可以判断出相互靠近的图像像素

32
00:01:35,750 --> 00:01:40,475
比相互离得更远的像素关系更紧密

33
00:01:40,475 --> 00:01:44,970
在此部分 我和 Alexis 将讨论 CNN 的数学原理

34
00:01:44,970 --> 00:01:47,850
我们将介绍构成 CNN 的不同层级

35
00:01:47,850 --> 00:01:50,300
并讲解它们在处理输入

36
00:01:50,300 --> 00:01:53,640
以及形成完整神经网络中的角色

