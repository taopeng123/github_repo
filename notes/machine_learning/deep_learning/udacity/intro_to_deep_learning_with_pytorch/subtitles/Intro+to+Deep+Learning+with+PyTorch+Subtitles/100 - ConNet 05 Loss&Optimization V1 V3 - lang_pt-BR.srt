1
00:00:00,399 --> 00:00:03,336
Agora que decidimos a estrutura
da nossa MLP,

2
00:00:03,370 --> 00:00:05,371
vamos falar
de como toda esta coisa

3
00:00:05,405 --> 00:00:07,640
vai aprender
a partir dos dados MNIST.

4
00:00:07,674 --> 00:00:10,476
O que acontece
quando ela vê uma imagem.

5
00:00:10,510 --> 00:00:13,312
Veja esta imagem de um 2.

6
00:00:13,346 --> 00:00:15,514
Nós colocamos esta imagem
na rede

7
00:00:15,548 --> 00:00:18,951
e recebemos estas 10 pontuações
de classe da camada de output.

8
00:00:18,985 --> 00:00:22,121
Uma pontuação mais alta
significa que a rede está mais certa

9
00:00:22,155 --> 00:00:24,824
de que a imagem é
daquela classe em particular.

10
00:00:24,858 --> 00:00:27,059
4 é o maior valor aqui.

11
00:00:27,093 --> 00:00:28,894
E -2 é o menor.

12
00:00:28,928 --> 00:00:33,132
A rede acredita que é menos provável
que a imagem seja um 3

13
00:00:33,166 --> 00:00:35,034
e mais provável que seja um 8.

14
00:00:35,068 --> 00:00:36,502
Mas isto está incorreto.

15
00:00:36,536 --> 00:00:38,904
Nós podemos ver
que o rótulo correto é 2.

16
00:00:38,938 --> 00:00:42,675
Então podemos fazer a rede
aprender com este erro.

17
00:00:42,709 --> 00:00:44,010
Enquanto a rede treina,

18
00:00:44,044 --> 00:00:47,313
nós medimos os erros que ela comete
usando a função de perda,

19
00:00:47,347 --> 00:00:49,115
cujo trabalho é medir
a diferença

20
00:00:49,149 --> 00:00:51,484
entre a classe prevista
e a verdadeira.

21
00:00:51,518 --> 00:00:55,521
E, com retropropagação,
computamos o gradiente da perda

22
00:00:55,555 --> 00:00:57,256
sobre os pesos dos modelos.

23
00:00:57,290 --> 00:01:00,793
Deste modo, calculamos
o quanto um peso é ruim

24
00:01:00,827 --> 00:01:04,263
e descobrimos quais pesos na rede
são responsáveis por cada erro.

25
00:01:04,297 --> 00:01:06,599
Por último,
usando este cálculo,

26
00:01:06,633 --> 00:01:08,734
podemos escolher
uma função de otimização,

27
00:01:08,768 --> 00:01:10,336
como a gradiente descendente,

28
00:01:10,370 --> 00:01:12,905
para nos dar o melhor valor
de um peso.

29
00:01:12,939 --> 00:01:15,808
Para atingir este objetivo,
a primeira coisa a se fazer

30
00:01:15,842 --> 00:01:18,477
é tornar esta camada de output
mais interpretável.

31
00:01:18,511 --> 00:01:22,815
O que costuma ser feito é aplicar
uma função de ativação softmax

32
00:01:22,849 --> 00:01:25,551
para converter as pontuações
em probabilidades.

33
00:01:25,585 --> 00:01:28,688
Para aplicar uma função softmax
nesta camada de output,

34
00:01:28,722 --> 00:01:31,357
começamos avaliando
a função exponencial

35
00:01:31,391 --> 00:01:32,792
em cada uma das pontuações.

36
00:01:32,826 --> 00:01:35,261
Depois adicionamos os valores.

37
00:01:35,295 --> 00:01:37,863
Vamos denotar a soma
com um S maiúsculo.

38
00:01:37,897 --> 00:01:40,933
Depois dividimos cada valor
pela soma.

39
00:01:40,967 --> 00:01:44,403
Depois de fazer as contas,
terá estes dez valores.

40
00:01:44,437 --> 00:01:47,106
Agora cada valor produz
a probabilidade

41
00:01:47,140 --> 00:01:50,076
de que a imagem retratada
seja a imagem prevista.

42
00:01:50,110 --> 00:01:54,380
Por exemplo, a rede acredita
que há 44,1% de probabilidade

43
00:01:54,414 --> 00:01:55,948
de a imagem mostrar um 8.

44
00:01:55,982 --> 00:01:59,819
Lembre-se de que a imagem era
de um 2 escrito à mão.

45
00:01:59,853 --> 00:02:02,221
E a rede prevê erroneamente

46
00:02:02,255 --> 00:02:05,992
que esta é uma probabilidade
de apenas 16,2%.

47
00:02:06,026 --> 00:02:08,828
Nosso objetivo é atualizar
os pesos da rede

48
00:02:08,862 --> 00:02:10,563
em resposta a este erro,

49
00:02:10,597 --> 00:02:12,598
para da próxima vez que vir
esta imagem,

50
00:02:12,632 --> 00:02:15,301
ela preveja que 2 seja
o rótulo mais provável.

51
00:02:15,335 --> 00:02:17,603
Em um mundo perfeito,
a rede preveria

52
00:02:17,637 --> 00:02:20,635
que a imagem tem 100%
de probabilidade de ser classe 2.

53
00:02:20,669 --> 00:02:24,143
Para a previsão do modelo
se aproximar da verdade,

54
00:02:24,177 --> 00:02:25,711
precisamos definir uma medição

55
00:02:25,745 --> 00:02:29,515
de a qual distância o modelo
está da perfeição.

56
00:02:29,549 --> 00:02:32,351
Podemos usar uma função de perda
para achar qualquer erro

57
00:02:32,385 --> 00:02:35,755
entre a classe verdadeira
e a classe prevista.

58
00:02:35,789 --> 00:02:37,723
E a retropropagação vai achar

59
00:02:37,757 --> 00:02:40,523
os parâmetros responsáveis
por estes erros.

60
00:02:40,557 --> 00:02:43,529
Como estamos construindo
um classificador multiclasse,

61
00:02:43,563 --> 00:02:46,365
usaremos a perda
de entropia cruzada.

62
00:02:46,927 --> 00:02:49,235
Para calcular
a perda neste exemplo,

63
00:02:49,269 --> 00:02:55,241
começamos com a previsão do modelo
para a classe correta, que é 16,2%.

64
00:02:55,275 --> 00:02:58,344
A perda de entropia cruzada olha
este valor de probabilidade,

65
00:02:58,378 --> 00:03:01,180
que em decimal é 0,162,

66
00:03:01,214 --> 00:03:03,983
e tira a perda de log negativo
deste valor.

67
00:03:04,017 --> 00:03:07,019
Neste caso, receberemos
um log negativo .162,

68
00:03:07,053 --> 00:03:08,454
ou 1.82.

69
00:03:08,488 --> 00:03:13,359
E aqui diz que o peso da rede é
um pouco diferente.

70
00:03:13,393 --> 00:03:16,696
E o modelo retorna
estas probabilidades.

71
00:03:16,730 --> 00:03:19,365
Esta previsão é bem melhor
do que a de cima.

72
00:03:19,399 --> 00:03:21,785
E quando calculamos a perda
de entropia cruzada,

73
00:03:21,819 --> 00:03:23,536
recebemos um valor bem menor.

74
00:03:23,570 --> 00:03:25,438
Em geral, é possível mostrar

75
00:03:25,472 --> 00:03:27,606
que a perda de entropia
cruzada categórica

76
00:03:27,640 --> 00:03:30,476
é definida de um jeito
que a perda é menor

77
00:03:30,510 --> 00:03:33,812
quando a previsão concorda mais
com a classe verdadeira do rótulo.

78
00:03:33,846 --> 00:03:37,216
E é maior quando a previsão
e a classe verdadeira discordam.

79
00:03:37,250 --> 00:03:38,784
Enquanto um modelo treina,

80
00:03:38,818 --> 00:03:42,521
seu objetivo será achar os pesos
que minimizem a função de perda

81
00:03:42,555 --> 00:03:45,524
e nos deem as previsões
mais precisas.

82
00:03:45,558 --> 00:03:48,260
Uma função de perda
e a retropropagação

83
00:03:48,294 --> 00:03:51,764
nos ajudam a quantificar
o quanto o peso de uma rede é ruim,

84
00:03:51,798 --> 00:03:55,501
baseado na distância entre a classe
prevista e a classe verdadeira.

85
00:03:55,535 --> 00:03:58,704
Agora precisamos calcular
o melhor valor de peso.

86
00:03:58,738 --> 00:04:02,074
Na aula anterior, o encorajamos
a pensar na função de perda

87
00:04:02,108 --> 00:04:04,577
como a superfície
de uma cadeia de montanhas.

88
00:04:04,611 --> 00:04:06,212
Para minimizar esta função,

89
00:04:06,246 --> 00:04:09,548
precisamos achar um jeito
de descer para o vale mais baixo.

90
00:04:09,582 --> 00:04:11,851
Esta é a função
de um otimizador.

91
00:04:11,885 --> 00:04:14,988
O padrão para minimizar a perda
e otimizar

92
00:04:15,021 --> 00:04:16,321
para os melhores
valores de pesos

93
00:04:16,355 --> 00:04:18,157
é chamado
de "gradiente descendente".

94
00:04:18,191 --> 00:04:21,894
Você foi apresentado a vários modos
de fazer o gradiente descendente.

95
00:04:21,928 --> 00:04:24,397
E cada método tem
um otimizador correspondente.

96
00:04:24,431 --> 00:04:28,167
A superfície retratada aqui é
um exemplo de uma função de perda.

97
00:04:28,201 --> 00:04:30,169
E todos os otimizadores
estão correndo

98
00:04:30,203 --> 00:04:31,902
para o mínimo da função.

99
00:04:31,935 --> 00:04:34,340
Veja que alguns se saem melhor
que outros,

100
00:04:34,374 --> 00:04:37,527
e você deve experimentar
todos eles no seu código.

