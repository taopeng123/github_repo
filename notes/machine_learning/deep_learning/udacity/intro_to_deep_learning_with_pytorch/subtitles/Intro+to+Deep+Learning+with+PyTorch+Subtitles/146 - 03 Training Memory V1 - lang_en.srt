1
00:00:00,000 --> 00:00:02,560
Last time, we defined a model, and next,

2
00:00:02,560 --> 00:00:06,380
I want to actually instantiate it and train it using our training data.

3
00:00:06,380 --> 00:00:09,120
First, I'll specify my model hyperparameters.

4
00:00:09,120 --> 00:00:11,160
The input and output will just be one,

5
00:00:11,160 --> 00:00:14,550
it's just one sequence at a time that we're processing and outputting,

6
00:00:14,550 --> 00:00:17,310
then I'll specify a hidden dimension which is just the number

7
00:00:17,310 --> 00:00:20,250
of features expect to generate with the RNN layer.

8
00:00:20,250 --> 00:00:23,065
I'll set this to 32, but for a small data set like this,

9
00:00:23,065 --> 00:00:25,105
I may even be able to go smaller.

10
00:00:25,105 --> 00:00:27,345
I'll set n_layers to one for now.

11
00:00:27,345 --> 00:00:29,880
So, I'm not stacking any RNN layers.

12
00:00:29,880 --> 00:00:31,915
I'll create this RNN and printed out.

13
00:00:31,915 --> 00:00:33,890
I should see the variables that I expect.

14
00:00:33,890 --> 00:00:36,620
My RNN layer with an input size and hidden dimension,

15
00:00:36,620 --> 00:00:40,600
and a linear layer with an input number of features and output number.

16
00:00:40,600 --> 00:00:44,090
Before training, I'm defining my loss and optimization functions.

17
00:00:44,090 --> 00:00:46,730
Now in this case, we're training our model to generate

18
00:00:46,730 --> 00:00:49,830
data points that are going to be basically coordinate values.

19
00:00:49,830 --> 00:00:52,640
So to compare a predicted and ground truth point like this,

20
00:00:52,640 --> 00:00:54,680
we'll use a regression loss because this is

21
00:00:54,680 --> 00:00:57,670
just a quantity rather than something like a class probability.

22
00:00:57,670 --> 00:00:58,970
So for the loss function,

23
00:00:58,970 --> 00:01:00,980
I'm going to use mean squared error loss which will

24
00:01:00,980 --> 00:01:03,650
just measure the distance between two points.

25
00:01:03,650 --> 00:01:06,990
I'll use an Adam optimizer which is standard for recurrent models,

26
00:01:06,990 --> 00:01:09,655
passing in my parameters and our learning rate.

27
00:01:09,655 --> 00:01:12,530
Next, I have a function train that's going to take

28
00:01:12,530 --> 00:01:15,320
in and RNN a number of steps to train for,

29
00:01:15,320 --> 00:01:19,315
and the parameter that will determine when it will print out law statistics.

30
00:01:19,315 --> 00:01:21,560
Now, at the very start of this function,

31
00:01:21,560 --> 00:01:23,205
I'm initializing my hidden state.

32
00:01:23,205 --> 00:01:27,470
At first, this is going to be nothing and it will default to a hidden state of all zeros.

33
00:01:27,470 --> 00:01:29,785
Then let's take a look at our batch loop.

34
00:01:29,785 --> 00:01:31,705
Now, this is a little unconventional,

35
00:01:31,705 --> 00:01:33,440
but I'm just generating data on the fly

36
00:01:33,440 --> 00:01:35,880
here according to how many steps we will train for.

37
00:01:35,880 --> 00:01:40,635
So, in these lines, I'm just generating a sequence of 20 sine wave values at a time.

38
00:01:40,635 --> 00:01:42,990
As we saw when I generated data at the start.

39
00:01:42,990 --> 00:01:45,500
Here, I'm getting my input x and a target

40
00:01:45,500 --> 00:01:48,110
y that's just shifted by one time step in the future.

41
00:01:48,110 --> 00:01:51,140
Here, I'm converting this data into tensors and

42
00:01:51,140 --> 00:01:55,275
squeezing the first dimension of our x_tensor to give it a batch size of one.

43
00:01:55,275 --> 00:01:58,280
Then I can pass my input tensor into my RNN model.

44
00:01:58,280 --> 00:02:03,020
So this is taking in my x input tensor and my initial hidden state at first.

45
00:02:03,020 --> 00:02:06,820
It produces a predicted output and a new hidden state.

46
00:02:06,820 --> 00:02:08,875
Next is an important part.

47
00:02:08,875 --> 00:02:11,870
I want to feed this new hidden state into the RNN as

48
00:02:11,870 --> 00:02:14,860
input at the next time step when we loop around once more.

49
00:02:14,860 --> 00:02:19,660
So I'm just copying the values from this produced hidden state into a new variable.

50
00:02:19,660 --> 00:02:23,330
This essentially detaches the hidden state from its history and I will

51
00:02:23,330 --> 00:02:26,810
not have to backpropagate through a series of accumulated hidden states.

52
00:02:26,810 --> 00:02:29,540
So this is what's going to be passed as input to

53
00:02:29,540 --> 00:02:33,790
the RNN at the next time step or next point in our sequence.

54
00:02:33,790 --> 00:02:36,180
So then, I have the usual training commands,

55
00:02:36,180 --> 00:02:38,630
I zero out any accumulated gradients.

56
00:02:38,630 --> 00:02:42,720
Calculate the loss, and perform a backpropagation in optimization step.

57
00:02:42,720 --> 00:02:45,320
Down here, I have some code to print out the loss

58
00:02:45,320 --> 00:02:47,770
and show what our input and predicted outputs are.

59
00:02:47,770 --> 00:02:50,375
Finally, this function returns a trained RNN

60
00:02:50,375 --> 00:02:53,030
which will be useful if you want to save a model for example.

61
00:02:53,030 --> 00:02:55,115
So, let's run this code.

62
00:02:55,115 --> 00:02:56,840
I'll choose to train our RNN,

63
00:02:56,840 --> 00:02:59,710
and that we defined above for 75 steps.

64
00:02:59,710 --> 00:03:02,480
I'll print out the result every 15 steps.

65
00:03:02,480 --> 00:03:05,390
We can see the mean squared error loss here and the

66
00:03:05,390 --> 00:03:09,080
difference between our red input in our blue output values.

67
00:03:09,080 --> 00:03:11,870
Recall that we want the blue output values to be

68
00:03:11,870 --> 00:03:14,560
one times step in the future when compared to the red ones.

69
00:03:14,560 --> 00:03:16,500
So it starts out pretty incorrect.

70
00:03:16,500 --> 00:03:20,230
Then we can see the loss decreases quite a lot after the first 15 steps.

71
00:03:20,230 --> 00:03:22,760
Our blue line is getting closer to our red one.

72
00:03:22,760 --> 00:03:27,565
As we train the blue predicted line gets closer to what we know our target is,

73
00:03:27,565 --> 00:03:29,370
at the end of 75 steps,

74
00:03:29,370 --> 00:03:30,840
our loss is pretty low.

75
00:03:30,840 --> 00:03:34,310
Our blue line looks very similar to what we know or output should be.

76
00:03:34,310 --> 00:03:37,565
If we look at the same time step for a red input dot,

77
00:03:37,565 --> 00:03:38,890
and a blue input dot,

78
00:03:38,890 --> 00:03:40,520
we we shouldn't see that the blue input is

79
00:03:40,520 --> 00:03:43,895
one time-step shifted in the future. It's pretty close.

80
00:03:43,895 --> 00:03:47,000
You could imagine getting even better performance after training for

81
00:03:47,000 --> 00:03:50,680
more steps or if you wanted to add more layers to your RNN.

82
00:03:50,680 --> 00:03:51,945
So, in this video,

83
00:03:51,945 --> 00:03:55,850
I wanted to demonstrate the basic structure of a simple RNN and show you how

84
00:03:55,850 --> 00:04:00,010
to keep track of the hidden state and represent memory over time as you train.

85
00:04:00,010 --> 00:04:02,990
You could imagine doing something very similar with data about

86
00:04:02,990 --> 00:04:07,010
world temperature or stock prices which are a little bit more complicated than this.

87
00:04:07,010 --> 00:04:08,990
But it will be really interesting to see if you could

88
00:04:08,990 --> 00:04:11,120
predict the future given that kind of data.

89
00:04:11,120 --> 00:04:13,100
Okay, so this is just an example,

90
00:04:13,100 --> 00:04:15,135
you can check out this code in our program GitHub,

91
00:04:15,135 --> 00:04:16,520
which is linked to below.

92
00:04:16,520 --> 00:04:17,780
I encourage you to play around with

93
00:04:17,780 --> 00:04:20,390
these model parameters until you have a good handle on

94
00:04:20,390 --> 00:04:22,010
the dimensions of an RNN input and

95
00:04:22,010 --> 00:04:25,570
output and how hyperparameters might change, how this model trains.

96
00:04:25,570 --> 00:04:29,670
Next, Matt and I will go over an exercise in generating text.

