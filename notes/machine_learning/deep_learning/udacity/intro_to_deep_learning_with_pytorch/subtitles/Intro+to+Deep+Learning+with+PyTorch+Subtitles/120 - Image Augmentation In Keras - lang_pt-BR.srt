1
00:00:00,167 --> 00:00:03,967
Quando construímos um algoritmo
para classificar objetos em imagens,

2
00:00:04,004 --> 00:00:08,067
há muitas informações
irrelevantes.

3
00:00:08,100 --> 00:00:10,767
Nós queremos
que o algoritmo determine

4
00:00:10,800 --> 00:00:13,934
se um objeto está presente
na imagem ou não.

5
00:00:13,967 --> 00:00:16,434
O tamanho do objeto
não importa,

6
00:00:16,467 --> 00:00:17,867
nem o ângulo,

7
00:00:17,900 --> 00:00:21,100
nem se eu o mover
para a direita da imagem.

8
00:00:21,134 --> 00:00:24,067
Isso continuará sendo
uma imagem com um abacate.

9
00:00:24,100 --> 00:00:27,734
Nós queremos
que o algoritmo aprenda

10
00:00:27,767 --> 00:00:31,600
uma representação invariante
da imagem.

11
00:00:31,634 --> 00:00:34,400
Nós não queremos que o modelo
altere a previsão

12
00:00:34,434 --> 00:00:36,800
baseado no tamanho do objeto.

13
00:00:36,834 --> 00:00:39,667
Isso é chamado
de "invariância de escala".

14
00:00:39,700 --> 00:00:43,530
Também não queremos que o ângulo
do objeto seja relevante.

15
00:00:43,767 --> 00:00:46,600
Isso é chamado
de "invariância de rotação".

16
00:00:46,634 --> 00:00:48,967
Se eu mover a imagem
para a esquerda

17
00:00:49,000 --> 00:00:50,500
ou para a direita,

18
00:00:50,534 --> 00:00:53,234
ela continuará sendo
a imagem de um abacate.

19
00:00:53,267 --> 00:00:56,634
Isso é chamado
de "invariância de tradução".

20
00:00:56,667 --> 00:00:59,600
As CNNs já possuem algumas

21
00:00:59,634 --> 00:01:02,434
invariâncias de tradução.

22
00:01:02,467 --> 00:01:05,667
Para ver isso, precisaremos
nos lembrar de como calculamos

23
00:01:05,700 --> 00:01:07,400
as camadas max pooling.

24
00:01:07,434 --> 00:01:09,700
Na localização de cada janela,

25
00:01:09,734 --> 00:01:13,700
pegamos o maior pixel
de uma janela.

26
00:01:13,734 --> 00:01:17,767
Esse valor máximo pode ocorrer
em qualquer lugar da janela.

27
00:01:17,800 --> 00:01:21,134
O valor do nó max pooling
será o mesmo.

28
00:01:21,167 --> 00:01:23,900
Podemos traduzir a imagem
um pouco para a esquerda,

29
00:01:23,934 --> 00:01:26,834
para a direita,
para cima ou para baixo,

30
00:01:26,867 --> 00:01:30,634
desde que o valor máximo
fique dentro da janela.

31
00:01:30,667 --> 00:01:34,534
Aplicando muitas camadas
max pooling em uma sequência,

32
00:01:34,567 --> 00:01:37,634
cada uma seguida
de uma camada convolucional,

33
00:01:37,667 --> 00:01:40,734
podemos traduzir o objeto
que estiver à esquerda,

34
00:01:40,767 --> 00:01:43,467
em cima da imagem
ou embaixo da imagem

35
00:01:43,500 --> 00:01:47,134
e, ainda assim, a rede
conseguirá compreendê-lo.

36
00:01:47,167 --> 00:01:50,067
Esse é um problema
não trivial.

37
00:01:50,100 --> 00:01:54,067
O computador só vê
a matriz dos pixels.

38
00:01:54,100 --> 00:01:56,500
Transformar a escala,
a rotação

39
00:01:56,534 --> 00:01:58,534
ou a posição
de um objeto na imagem

40
00:01:58,567 --> 00:02:01,167
afetará muito
os valores dos pixels.

41
00:02:01,200 --> 00:02:05,634
Nós podemos ver
a diferença nas imagens,

42
00:02:05,667 --> 00:02:10,400
mas como você se sairia
se tivesse um arranjo numérico?

43
00:02:10,434 --> 00:02:12,500
Felizmente há uma boa técnica

44
00:02:12,534 --> 00:02:16,700
que torna os algoritmos
estatisticamente invariantes,

45
00:02:16,734 --> 00:02:20,134
mas isso soaria como trapaça.

46
00:02:20,167 --> 00:02:22,167
A ideia é a seguinte:

47
00:02:22,200 --> 00:02:26,067
se quisermos que a CNN
seja invariante de rotação,

48
00:02:26,100 --> 00:02:29,367
adicionamos imagens
ao conjunto de treinamento,

49
00:02:29,400 --> 00:02:33,467
fazendo rotações aleatórias
nas imagens de treinamento.

50
00:02:33,500 --> 00:02:35,967
Se quisermos mais
invariâncias de tradução,

51
00:02:36,000 --> 00:02:38,600
adicionamos mais imagens,

52
00:02:38,634 --> 00:02:43,267
criando traduções aleatórias
das imagens de treinamento.

53
00:02:43,300 --> 00:02:46,767
Assim expandimos
o conjunto de treinamento

54
00:02:46,800 --> 00:02:49,100
aumentando os dados.

55
00:02:49,134 --> 00:02:53,867
O aumento de dados ajuda
a evitar sobreajuste.

56
00:02:53,900 --> 00:02:57,734
Isso é porque o modelo vê
muitas imagens novas,

57
00:02:57,767 --> 00:03:00,500
assim,
ele generalizará melhor,

58
00:03:00,534 --> 00:03:04,501
e teremos melhor desempenho
no conjunto de dados de teste.

59
00:03:04,534 --> 00:03:06,434
Vamos aumentar
os dados de treinamento

60
00:03:06,467 --> 00:03:09,799
para vermos se os dez conjuntos
de treinamento do vídeo anterior

61
00:03:09,832 --> 00:03:12,767
podem melhorar
a precisão de teste.

62
00:03:12,800 --> 00:03:16,934
Usaremos o notebook Jupyter,
que pode ser baixado abaixo.

