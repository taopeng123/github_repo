1
00:00:00,000 --> 00:00:04,620
Hi again. So, here's my solution for the train pass that I had you implement.

2
00:00:04,620 --> 00:00:07,620
So, here we're just defining our model like normal and

3
00:00:07,620 --> 00:00:10,110
then our negative log-likelihood loss using

4
00:00:10,110 --> 00:00:13,095
stochastic gradient descent and pass in our parameters.

5
00:00:13,095 --> 00:00:15,510
Then, here is our training pass.

6
00:00:15,510 --> 00:00:18,150
So, for each image in labels in trainloader,

7
00:00:18,150 --> 00:00:23,190
we're going to flatten it and then zero out the gradients using optimizer. zero_ grad.

8
00:00:23,190 --> 00:00:26,970
Pass our images forward through the model and the output and then from that,

9
00:00:26,970 --> 00:00:28,820
we can calculate our loss and then do

10
00:00:28,820 --> 00:00:31,130
a backward pass and then finally with the gradients,

11
00:00:31,130 --> 00:00:32,960
we can do this optimizer step.

12
00:00:32,960 --> 00:00:36,260
So, if I run this and we wait a little bit for to train,

13
00:00:36,260 --> 00:00:40,305
we can actually see the loss dropping over time, right?

14
00:00:40,305 --> 00:00:41,925
So, after five epochs,

15
00:00:41,925 --> 00:00:43,380
we see that the first one,

16
00:00:43,380 --> 00:00:47,430
it starts out fairly high at 1.9 but after five epochs,

17
00:00:47,430 --> 00:00:51,950
continuous drop as we're training and we see it much lower after five epochs.

18
00:00:51,950 --> 00:00:54,950
So, if we kept training then our network would learn the data

19
00:00:54,950 --> 00:00:58,445
better and better and the training loss would be even smaller.

20
00:00:58,445 --> 00:01:00,740
So, now with our training network,

21
00:01:00,740 --> 00:01:05,655
we can actually see what our network thinks it's seen in these images.

22
00:01:05,655 --> 00:01:08,700
So, for here, we can pass in an image.

23
00:01:08,700 --> 00:01:11,630
In this case, it's the image of a number two and

24
00:01:11,630 --> 00:01:14,610
then this is what our network is predicting now.

25
00:01:14,610 --> 00:01:19,190
So, you can see pretty easily that it's putting most of the probability,

26
00:01:19,190 --> 00:01:23,480
most of its prediction into the class for the digit two.

27
00:01:23,480 --> 00:01:29,460
So we try it again and put in passes in number eight and again, it's predicting eight.

28
00:01:29,460 --> 00:01:31,655
So, we've managed to actually train

29
00:01:31,655 --> 00:01:34,970
our network to make accurate predictions for our digits.

30
00:01:34,970 --> 00:01:37,730
So next step, you'll write the code for training

31
00:01:37,730 --> 00:01:41,420
a neutral network on a more complex dataset and you'll be doing the whole thing,

32
00:01:41,420 --> 00:01:46,070
defining the model, running the training loop, all that. Cheers.

