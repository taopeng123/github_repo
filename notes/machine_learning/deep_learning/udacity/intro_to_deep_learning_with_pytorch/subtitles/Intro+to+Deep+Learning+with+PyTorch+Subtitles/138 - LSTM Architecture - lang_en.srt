1
00:00:00,000 --> 00:00:02,740
So in order to study the architecture of an LSTM,

2
00:00:02,740 --> 00:00:05,429
let's quickly recall the architecture of an RNN.

3
00:00:05,429 --> 00:00:10,379
Basically what we do is we take our event E_t and our memory M_t-1,

4
00:00:10,380 --> 00:00:12,460
coming from the previous point in time,

5
00:00:12,460 --> 00:00:15,150
and we apply a simple tanh or

6
00:00:15,150 --> 00:00:20,184
sigmoid activation function to obtain the output and then your memory M_t.

7
00:00:20,184 --> 00:00:21,539
So to be more specific,

8
00:00:21,539 --> 00:00:26,039
we join these two vectors and multiply them by a matrix W and add a bias b,

9
00:00:26,039 --> 00:00:28,920
and then squish this with the tanh function,

10
00:00:28,920 --> 00:00:31,149
and that gives us the output M_t.

11
00:00:31,149 --> 00:00:35,490
This output is a prediction and also the memory that we carry to the next node.

12
00:00:35,490 --> 00:00:37,710
The LSTM architecture is very similar,

13
00:00:37,710 --> 00:00:40,679
except with a lot more nodes inside and with two inputs

14
00:00:40,679 --> 00:00:43,990
and outputs since it keeps track of the long- and short-term memories.

15
00:00:43,990 --> 00:00:45,679
And as I said, the short-term memory is,

16
00:00:45,679 --> 00:00:47,550
again, the output or prediction.

17
00:00:47,549 --> 00:00:51,274
Don't get scared. These are actually not as complicated as they look.

18
00:00:51,274 --> 00:00:53,298
We'll break them down in the next few videos.

