1
00:00:00,000 --> 00:00:03,365
确定 MLP 的结构后

2
00:00:03,365 --> 00:00:07,635
我们看看整个模型是如何从 MNIST 数据中学习规律的

3
00:00:07,635 --> 00:00:10,515
当它看到一张输入图像时 会发生什么？

4
00:00:10,515 --> 00:00:13,380
以这个输入图像 2 为例

5
00:00:13,380 --> 00:00:16,320
假设将这个图像传入网络中

6
00:00:16,320 --> 00:00:19,110
输出层生成了 10 个类别分数

7
00:00:19,110 --> 00:00:21,750
更高的分数表示网络更确信

8
00:00:21,750 --> 00:00:25,030
输入图像是特定类别

9
00:00:25,030 --> 00:00:28,590
最大值是 4 最小值是 -2

10
00:00:28,590 --> 00:00:31,830
因此网络认为该图像最不可能是手写 3

11
00:00:31,830 --> 00:00:35,280
最有可能是 8

12
00:00:35,280 --> 00:00:36,595
但这不正确

13
00:00:36,595 --> 00:00:39,130
我们知道正确标签是 2

14
00:00:39,130 --> 00:00:42,715
因此我们可以让网络从这个错误中汲取教训

15
00:00:42,715 --> 00:00:47,540
在训练网络时 我们使用损失函数衡量任何错误

16
00:00:47,540 --> 00:00:51,585
损失函数的作用是衡量预测类别标签和真实标签之间的差异

17
00:00:51,585 --> 00:00:53,410
然后通过反向传播

18
00:00:53,410 --> 00:00:57,275
计算损失相对于模型权重的梯度

19
00:00:57,275 --> 00:01:00,305
这样就可以量化特定权重有多糟糕

20
00:01:00,305 --> 00:01:04,555
并找到造成任何错误的权重

21
00:01:04,555 --> 00:01:06,650
最后根据该计算结果

22
00:01:06,650 --> 00:01:09,110
选择一个优化函数 例如梯度下降法

23
00:01:09,110 --> 00:01:13,055
从而计算更合适的权重值

24
00:01:13,055 --> 00:01:15,755
为了达到这个目标

25
00:01:15,755 --> 00:01:18,695
首先我们需要让这个输出层更容易解读

26
00:01:18,695 --> 00:01:20,870
常见做法是

27
00:01:20,870 --> 00:01:25,600
使用 softtmax 激活函数将这些分数转换成概率

28
00:01:25,600 --> 00:01:28,520
要向输出层应用 softmax 函数

29
00:01:28,520 --> 00:01:32,740
首先计算每个分数的指数函数值

30
00:01:32,740 --> 00:01:35,200
然后将所有这些值相加

31
00:01:35,200 --> 00:01:37,945
相加得到的和用大写的 S 表示

32
00:01:37,945 --> 00:01:41,020
然后用每个值除以和

33
00:01:41,020 --> 00:01:42,810
代入所有数字后

34
00:01:42,810 --> 00:01:44,525
得出这 10 个值

35
00:01:44,525 --> 00:01:47,300
每个值表示

36
00:01:47,300 --> 00:01:50,075
图像属于相应类别的概率

37
00:01:50,075 --> 00:01:52,925
例如网络认为

38
00:01:52,925 --> 00:01:56,090
图像是 8 的概率为 44.1%

39
00:01:56,090 --> 00:01:58,310
该输入图像是一张手写数字 2

40
00:01:58,310 --> 00:02:00,905
但是网络却错误地预测出

41
00:02:00,905 --> 00:02:06,125
图像是 2 的概率只有 16.2%

42
00:02:06,125 --> 00:02:10,450
我们的目标是根据这个错误更新网络权重

43
00:02:10,450 --> 00:02:12,545
以便下次看到该图像时

44
00:02:12,545 --> 00:02:15,400
网络能预测出 2 是概率最高的标签

45
00:02:15,400 --> 00:02:17,750
在完美情形下 网络将预测

46
00:02:17,750 --> 00:02:20,690
图像属于真实类别的概率是 100%

47
00:02:20,690 --> 00:02:24,210
为了使预测结果更接近真实情况

48
00:02:24,210 --> 00:02:26,840
我们需要定义一些衡量指标

49
00:02:26,840 --> 00:02:29,785
表示模型目前与完美结果之间的差异有多大

50
00:02:29,785 --> 00:02:32,720
我们可以使用损失函数

51
00:02:32,720 --> 00:02:35,830
表示真实类别与预测类别之间的差异

52
00:02:35,830 --> 00:02:37,580
然后反向传播将发现

53
00:02:37,580 --> 00:02:40,530
哪些模型参数导致了这些差异

54
00:02:40,530 --> 00:02:43,460
因为我们要构建多层分类器

55
00:02:43,460 --> 00:02:46,650
所以将使用分类交叉熵损失

56
00:02:46,650 --> 00:02:49,330
要计算这个示例的损失

57
00:02:49,330 --> 00:02:50,560
我们首先看看

58
00:02:50,560 --> 00:02:55,030
模型预测真实类别的概率是多少 结果是 16.2%

59
00:02:55,030 --> 00:02:58,790
交叉熵损失将对这个概率值

60
00:02:58,790 --> 00:03:03,745
（小数形式为 0.162）取负对数损失值

61
00:03:03,745 --> 00:03:08,720
结果为 -log(0.162) = 1.82

62
00:03:08,720 --> 00:03:10,385
假设网络的权重

63
00:03:10,385 --> 00:03:13,195
稍微有所不同

64
00:03:13,195 --> 00:03:16,775
模型返回了这些预测概率

65
00:03:16,775 --> 00:03:18,650
这些预测结果比上面的好多了

66
00:03:18,650 --> 00:03:21,540
计算交叉熵损失后

67
00:03:21,540 --> 00:03:23,485
结果小多了

68
00:03:23,485 --> 00:03:28,430
通常 分类交叉熵损失具有特定的规律

69
00:03:28,430 --> 00:03:30,680
当模型的预测值与真实类别标签更接近时

70
00:03:30,680 --> 00:03:33,760
分类交叉熵损失更低

71
00:03:33,760 --> 00:03:37,380
当预测值与真实类别标签不一致时 损失更高

72
00:03:37,380 --> 00:03:41,300
在模型训练过程中 交叉熵损失的目标是

73
00:03:41,300 --> 00:03:45,675
寻找最小化该损失函数的权重 从而提供最准确的预测

74
00:03:45,675 --> 00:03:48,530
损失函数和反向传播可以根据

75
00:03:48,530 --> 00:03:51,460
预测标签与真实标签之间的差异

76
00:03:51,460 --> 00:03:55,720
量化特定的网络权重质量

77
00:03:55,720 --> 00:03:58,950
接下来我们需要一种计算更佳权重值的方式

78
00:03:58,950 --> 00:04:01,070
在上节课 我们建议

79
00:04:01,070 --> 00:04:04,485
将损失函数看做类似于山峦的曲面

80
00:04:04,485 --> 00:04:06,160
要最小化此函数

81
00:04:06,160 --> 00:04:09,770
我们只需找到来到最低点的道路

82
00:04:09,770 --> 00:04:11,869
这就是优化器的作用

83
00:04:11,869 --> 00:04:14,990
最小化损失并优化为最佳权重值的标准方式

84
00:04:14,990 --> 00:04:18,155
称为梯度下降法

85
00:04:18,155 --> 00:04:20,690
我们已经介绍过几种梯度下降方式

86
00:04:20,690 --> 00:04:24,634
每个方法都有对应的优化器

87
00:04:24,634 --> 00:04:28,145
图中描述的曲面是一种损失函数

88
00:04:28,145 --> 00:04:32,080
所有优化器都朝着该函数的最低点

89
00:04:32,080 --> 00:04:34,580
某些优化器的效果更好

90
00:04:34,580 --> 00:04:37,360
建议在代码中尝试一下这些优化器

