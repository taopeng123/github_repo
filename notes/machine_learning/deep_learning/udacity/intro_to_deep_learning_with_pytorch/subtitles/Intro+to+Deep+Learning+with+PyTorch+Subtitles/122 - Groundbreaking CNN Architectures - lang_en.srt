1
00:00:00,000 --> 00:00:04,605
ImageNet is a database of over 10 million hand labeled images,

2
00:00:04,605 --> 00:00:08,180
drawn from 1,000 different image categories.

3
00:00:08,180 --> 00:00:11,490
Since 2010, the ImageNet project has held

4
00:00:11,490 --> 00:00:15,630
the ImageNet Large Scale Visual Recognition Competition,

5
00:00:15,630 --> 00:00:18,630
an annual competition where teams try to build

6
00:00:18,630 --> 00:00:23,005
the best CNN for object recognition and classification.

7
00:00:23,005 --> 00:00:26,165
The first breakthrough was in 2012.

8
00:00:26,165 --> 00:00:28,635
The network called AlexNet,

9
00:00:28,635 --> 00:00:32,715
was developed by a team at the University of Toronto.

10
00:00:32,715 --> 00:00:36,610
Using the best GPUs available in 2012,

11
00:00:36,610 --> 00:00:40,475
the AlexNet team trained the network in about a week.

12
00:00:40,475 --> 00:00:44,995
AlexNet pioneered the use of the ReLU activation function,

13
00:00:44,995 --> 00:00:49,220
and dropout as a technique for avoiding overfitting.

14
00:00:49,220 --> 00:00:55,655
In 2014, two different groups nearly tied in the ImageNet competition.

15
00:00:55,655 --> 00:00:59,710
One of those networks was called VGGNet,

16
00:00:59,710 --> 00:01:03,100
often referred to as just VGG,

17
00:01:03,100 --> 00:01:07,905
and it came from the Visual Geometry Group at Oxford University.

18
00:01:07,905 --> 00:01:12,560
VGG has two versions termed VGG16,

19
00:01:12,560 --> 00:01:18,935
and VGG 19, with 16 and 19 total layers respectively.

20
00:01:18,935 --> 00:01:22,860
Both versions have a simple and elegant architecture,

21
00:01:22,860 --> 00:01:26,685
which is just a long sequence of three by three convolutions,

22
00:01:26,685 --> 00:01:29,460
broken up by two by two pooling layers,

23
00:01:29,460 --> 00:01:33,130
and finished with three fully-connected layers.

24
00:01:33,130 --> 00:01:39,650
VGG pioneered the exclusive use of small three by three convolution windows,

25
00:01:39,650 --> 00:01:45,295
to contrast AlexNets much larger 11 by 11 windows.

26
00:01:45,295 --> 00:01:51,955
In 2015, the ImageNet winner was a network from Microsoft Research called ResNet.

27
00:01:51,955 --> 00:01:57,940
ResNet is like VGG and not the same structure is repeated again and again,

28
00:01:57,940 --> 00:01:59,600
for layer after layer.

29
00:01:59,600 --> 00:02:05,215
Also like VGG, ResNet has different versions that vary in their number of layers.

30
00:02:05,215 --> 00:02:10,490
The largest having a groundbreaking 152 layers.

31
00:02:10,490 --> 00:02:14,400
Previous researchers tried to make their CNNs this deep,

32
00:02:14,400 --> 00:02:17,810
but they ran into a problem where as they were adding layers,

33
00:02:17,810 --> 00:02:20,100
performance increased up to a point,

34
00:02:20,100 --> 00:02:22,910
after which performance quickly declined.

35
00:02:22,910 --> 00:02:27,350
This is partially due to what's known as the vanishing gradient's problem,

36
00:02:27,350 --> 00:02:31,715
which arises when we go to train the network through backpropagation.

37
00:02:31,715 --> 00:02:37,475
The main idea is that the gradient signal has to be pushed through the entire network.

38
00:02:37,475 --> 00:02:39,625
The deeper the network becomes,

39
00:02:39,625 --> 00:02:44,550
the more likely that the signal gets weakened before it gets where it needs to go.

40
00:02:44,550 --> 00:02:50,665
The ResNet team added connections to their very deep CNN that skip layers.

41
00:02:50,665 --> 00:02:54,390
So, the gradient signal has a shorter route to travel.

42
00:02:54,390 --> 00:03:01,650
ResNet achieves superhuman performance in classifying images in the ImageNet database.

