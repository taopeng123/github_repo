1
00:00:00,000 --> 00:00:01,685
So, in our last example,

2
00:00:01,685 --> 00:00:05,250
I got pretty good classification accuracy just by looking at how during

3
00:00:05,250 --> 00:00:07,320
training the cross entropy loss that

4
00:00:07,320 --> 00:00:09,855
measured the difference between predicted and true classes,

5
00:00:09,855 --> 00:00:11,145
got smaller and smaller.

6
00:00:11,145 --> 00:00:14,295
I estimated when a good time to stop training would be,

7
00:00:14,295 --> 00:00:17,185
when the training loss had plateaued it's decreasing.

8
00:00:17,185 --> 00:00:18,460
But as you just learned,

9
00:00:18,460 --> 00:00:22,990
there's a way to use a validation dataset to programmatically know when to stop training.

10
00:00:22,990 --> 00:00:25,250
So, I'm going to show you how to add that encode.

11
00:00:25,250 --> 00:00:26,755
So, the first thing I'm going to do,

12
00:00:26,755 --> 00:00:31,450
is to create a validation dataset much like we created training and test sets.

13
00:00:31,450 --> 00:00:35,790
In fact, I'm actually going to take a percentage of data from the training set.

14
00:00:35,790 --> 00:00:38,550
I'll set a variable valid size to take 20 percent of

15
00:00:38,550 --> 00:00:41,440
the training set data and turn it into validation data.

16
00:00:41,440 --> 00:00:45,155
There should be a good enough size since the MNIST dataset is very large.

17
00:00:45,155 --> 00:00:46,910
Then I'm going to use something called,

18
00:00:46,910 --> 00:00:50,960
subset random sampler to help me do the work of splitting the training data.

19
00:00:50,960 --> 00:00:54,289
First, I'll record how many training images there are,

20
00:00:54,289 --> 00:00:56,900
and I'll determine which indices in the training set I'll

21
00:00:56,900 --> 00:01:00,229
access to create both sets training and validation.

22
00:01:00,229 --> 00:01:04,930
I'll list out all the possible indices by grabbing the length of the entire training set.

23
00:01:04,930 --> 00:01:07,670
So, these indices are going to be the values that point to

24
00:01:07,670 --> 00:01:10,600
each of the 70,000 images in the training set.

25
00:01:10,600 --> 00:01:15,080
Then, I'm going to shuffle these indices so that any index I select out of this list,

26
00:01:15,080 --> 00:01:17,080
will reference a random piece of data.

27
00:01:17,080 --> 00:01:18,995
Then, I'm defining a split boundary,

28
00:01:18,995 --> 00:01:21,140
and this is just going to be the number of examples

29
00:01:21,140 --> 00:01:23,420
that I want to include in the validation set.

30
00:01:23,420 --> 00:01:26,615
So, it's going to be 20 percent of our training data.

31
00:01:26,615 --> 00:01:31,125
Then I'll use this to get an 80-20 split between training and validation data.

32
00:01:31,125 --> 00:01:33,860
Finally, I'm using subset random sampler to

33
00:01:33,860 --> 00:01:37,155
create data samplers for this training and validation data.

34
00:01:37,155 --> 00:01:41,370
This adds one more argument to our train loader and validation loaders.

35
00:01:41,370 --> 00:01:44,930
So, previously, I had only training and test data loaders,

36
00:01:44,930 --> 00:01:48,740
and now I've split the training data into two sets by essentially shuffling it and

37
00:01:48,740 --> 00:01:53,170
selecting 20 percent for validation set using a specific data sampler.

38
00:01:53,170 --> 00:01:56,375
So, now, we officially have a validation set loader

39
00:01:56,375 --> 00:02:00,665
and I'm going to scroll down to my training loop to actually use this validation set.

40
00:02:00,665 --> 00:02:03,305
Okay. Here's our training loop and this time

41
00:02:03,305 --> 00:02:05,720
in addition to keeping track of the training loss,

42
00:02:05,720 --> 00:02:08,210
I'm also going to track the validation loss.

43
00:02:08,210 --> 00:02:11,210
I'll actually want to start training whenever I reach an epic for

44
00:02:11,210 --> 00:02:14,470
which the training loss decreases but the validation loss does not.

45
00:02:14,470 --> 00:02:17,990
So, I'm going to track the change in the validation loss,

46
00:02:17,990 --> 00:02:21,140
and specifically, I'll track the minimum validation loss over time,

47
00:02:21,140 --> 00:02:24,230
so that I can compare it to the current validation loss and

48
00:02:24,230 --> 00:02:27,995
see if it's increased or decreased from the minimum over a given epic.

49
00:02:27,995 --> 00:02:29,320
So, within our epic loop,

50
00:02:29,320 --> 00:02:33,885
we have our usual training batch loop and we also have a validation batch loop.

51
00:02:33,885 --> 00:02:37,630
This is looping through all the data and labels in the validation set.

52
00:02:37,630 --> 00:02:41,680
It's applying our model to that data and recording the loss as usual.

53
00:02:41,680 --> 00:02:44,840
Notice, we're not performing the back propagation step here.

54
00:02:44,840 --> 00:02:47,295
That is reserved only for our training data.

55
00:02:47,295 --> 00:02:50,360
Then, I've added a little bit to my print statement and I'm

56
00:02:50,360 --> 00:02:53,570
printing out the average validation loss after each epic as well.

57
00:02:53,570 --> 00:02:55,280
Also at the end of each epic,

58
00:02:55,280 --> 00:02:57,260
I'm going to check the validation loss and

59
00:02:57,260 --> 00:02:59,720
see if it's less than the currently recorded minimum.

60
00:02:59,720 --> 00:03:02,180
If it is, I'm going to save the model because that

61
00:03:02,180 --> 00:03:04,400
means the validation loss has decreased,

62
00:03:04,400 --> 00:03:07,050
and I'll store that as the new minimum value.

63
00:03:07,050 --> 00:03:10,870
You may have noticed that I set the initial minimum to infinity.

64
00:03:10,870 --> 00:03:15,575
This high-value guarantees that this loss will update after the first epic.

65
00:03:15,575 --> 00:03:18,575
Also take note of this line which allows me to save the model

66
00:03:18,575 --> 00:03:22,075
and its current parameters by name model.pt.

67
00:03:22,075 --> 00:03:25,250
Okay. Then I've run this for 50 epics and I can see

68
00:03:25,250 --> 00:03:28,920
both the training loss and the validation loss after each epic.

69
00:03:28,920 --> 00:03:30,980
We can see that both the training and

70
00:03:30,980 --> 00:03:34,895
validation loss are decreasing for the first 30 or so epics.

71
00:03:34,895 --> 00:03:37,190
So, the model is being saved after each of

72
00:03:37,190 --> 00:03:39,695
these points where the validation loss decreases.

73
00:03:39,695 --> 00:03:43,620
We can also see this decrease slowing down right around here.

74
00:03:43,620 --> 00:03:47,580
In fact, our last model was saved after epic number 37.

75
00:03:47,580 --> 00:03:51,140
By saving the model at the point where the validation and training loss

76
00:03:51,140 --> 00:03:55,000
diverge am preventing my model from overfitting the training data,

77
00:03:55,000 --> 00:03:57,200
this is also an issue of efficiency.

78
00:03:57,200 --> 00:04:02,510
We should see that our validation loss stays the same for the last 10 to 15 epics here.

79
00:04:02,510 --> 00:04:05,720
So, the lack of decrease here is actually indicating to

80
00:04:05,720 --> 00:04:09,125
me that the best model is really reached even around epic 30,

81
00:04:09,125 --> 00:04:11,105
but definitely by epic 37.

82
00:04:11,105 --> 00:04:15,155
So, next step is to see how this model performs on our test data.

83
00:04:15,155 --> 00:04:17,840
Here, I'm actually loading the model that we saved earlier

84
00:04:17,840 --> 00:04:20,630
by name into our instantiated model,

85
00:04:20,630 --> 00:04:22,895
and I'm testing it as usual.

86
00:04:22,895 --> 00:04:27,005
Passing our test data into our model and recording the class accuracies.

87
00:04:27,005 --> 00:04:29,865
You can see we get 97 percent overall accuracy.

88
00:04:29,865 --> 00:04:32,270
This is really about the same as our last model,

89
00:04:32,270 --> 00:04:33,640
the one without validation.

90
00:04:33,640 --> 00:04:36,950
So, even though we train that model for 15 more epics,

91
00:04:36,950 --> 00:04:38,735
the results are about the same.

92
00:04:38,735 --> 00:04:42,320
This makes sense because this validation loss really isn't changing much.

93
00:04:42,320 --> 00:04:45,625
So, whether we save the model after epic 37 or 50,

94
00:04:45,625 --> 00:04:47,595
the model should be pretty similar.

95
00:04:47,595 --> 00:04:52,800
This behavior is also occurring because most of these images are very similar.

96
00:04:52,800 --> 00:04:56,765
The images are very processed and all of the digits look the same.

97
00:04:56,765 --> 00:04:58,735
So, in our non validation case,

98
00:04:58,735 --> 00:05:01,790
it didn't matter so much that our model trained for much longer,

99
00:05:01,790 --> 00:05:03,125
but in some cases,

100
00:05:03,125 --> 00:05:05,780
you will get overfitting and choosing a model

101
00:05:05,780 --> 00:05:09,160
based on validation loss will be even more important.

102
00:05:09,160 --> 00:05:12,230
So, model validation can be a really helpful way to

103
00:05:12,230 --> 00:05:15,270
select the best model and decide when to stop training.

