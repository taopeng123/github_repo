1
00:00:00,000 --> 00:00:03,500
Now, the goal of this model is to train it so that it can take in

2
00:00:03,500 --> 00:00:07,835
one character and produce a next character and that's what this next step,

3
00:00:07,835 --> 00:00:09,590
Making Predictions is all about.

4
00:00:09,590 --> 00:00:12,675
We basically want to create functions that can take in

5
00:00:12,675 --> 00:00:15,955
a character and have our network predict the next character.

6
00:00:15,955 --> 00:00:17,850
Then, we want to take that character,

7
00:00:17,850 --> 00:00:21,675
pass it back in, and get more and more predicted next characters.

8
00:00:21,675 --> 00:00:24,435
We'll keep doing this until we generate a bunch of text.

9
00:00:24,435 --> 00:00:27,770
So, you've been given this predict function which will help with this.

10
00:00:27,770 --> 00:00:29,895
This function takes in a model and

11
00:00:29,895 --> 00:00:33,210
occurring character and its job is to basically give us

12
00:00:33,210 --> 00:00:34,770
back the encoded value of

13
00:00:34,770 --> 00:00:39,330
the predictive next character and the hidden state that's produced by our model.

14
00:00:39,330 --> 00:00:42,100
So, let's see what it's actually doing step-by-step.

15
00:00:42,100 --> 00:00:47,040
It's taking in our input character and converting it into it's encoded integer value.

16
00:00:47,040 --> 00:00:49,080
Then, as part of pre-processing,

17
00:00:49,080 --> 00:00:50,210
we're turning that into

18
00:00:50,210 --> 00:00:54,905
a one-hot encoded representation and then converting these inputs into a tensor.

19
00:00:54,905 --> 00:00:57,410
These inputs we can then pass to our model,

20
00:00:57,410 --> 00:00:59,060
and then you'll see a couple of steps that are really

21
00:00:59,060 --> 00:01:01,285
similar to what we saw in our training loop.

22
00:01:01,285 --> 00:01:03,470
We put our inputs on a GPU if it's

23
00:01:03,470 --> 00:01:07,180
available and we detach our hidden state from its history here.

24
00:01:07,180 --> 00:01:10,160
Then, we pass in the inputs and the hidden state to

25
00:01:10,160 --> 00:01:13,820
our model which returns an output and a new hidden state.

26
00:01:13,820 --> 00:01:16,600
Next, we're processing the output a little more.

27
00:01:16,600 --> 00:01:21,790
We're applying a softmax function to get p probabilities for the likely next character.

28
00:01:21,790 --> 00:01:24,800
So, p is a probability distribution over

29
00:01:24,800 --> 00:01:28,930
all the possible mixed characters given the input character x.

30
00:01:28,930 --> 00:01:31,595
Now, we can generate more sensible characters by

31
00:01:31,595 --> 00:01:34,785
only considering the k most probable characters.

32
00:01:34,785 --> 00:01:38,900
So, here we're giving you a couple of lines of code to use top k sampling,

33
00:01:38,900 --> 00:01:41,880
which finds us the k most likely next characters.

34
00:01:41,880 --> 00:01:44,660
Then, here we're adding an element of randomness,

35
00:01:44,660 --> 00:01:48,280
something that selects from among those top likely next characters.

36
00:01:48,280 --> 00:01:51,950
So, then we have a most likely next character and we're actually returning

37
00:01:51,950 --> 00:01:56,170
the encoded value of that character and the hidden state produced by our model,

38
00:01:56,170 --> 00:01:59,510
but we'll basically want to call the predict function several times,

39
00:01:59,510 --> 00:02:01,320
generating one character's output,

40
00:02:01,320 --> 00:02:05,145
then passing that in as input and predicting the next and next characters.

41
00:02:05,145 --> 00:02:07,725
That brings me to our next function sample.

42
00:02:07,725 --> 00:02:12,385
Sample will take in our trained model and the size of text that we want to generate.

43
00:02:12,385 --> 00:02:14,235
It will also take in prime,

44
00:02:14,235 --> 00:02:18,440
which is going to be a set of characters that we want to start our model off with.

45
00:02:18,440 --> 00:02:21,380
Lastly, we will take in a value for top k which will

46
00:02:21,380 --> 00:02:24,950
just return our k most probable characters in our predict function.

47
00:02:24,950 --> 00:02:29,430
So, in here, we're starting off by moving our model to GPU if it's available,

48
00:02:29,430 --> 00:02:33,950
and here we're also initializing the hidden state with a batch size of one because,

49
00:02:33,950 --> 00:02:36,080
for one character that we're inputting at a time,

50
00:02:36,080 --> 00:02:37,520
the batch size will be one.

51
00:02:37,520 --> 00:02:40,830
In this way, prediction is quite different than training a model.

52
00:02:40,830 --> 00:02:44,560
Then, you'll see that we're getting each character in our prime word.

53
00:02:44,560 --> 00:02:46,855
The prime word basically helps us answer the question,

54
00:02:46,855 --> 00:02:49,135
how do we start to generate text?

55
00:02:49,135 --> 00:02:50,960
We shouldn't just start out randomly.

56
00:02:50,960 --> 00:02:54,975
So, what is usually done is to provide a prime word or a set of characters.

57
00:02:54,975 --> 00:02:57,320
Here the default prime set is just the,

58
00:02:57,320 --> 00:03:02,315
T-H-E, but you can pass in any set of characters that you want as the prime.

59
00:03:02,315 --> 00:03:04,505
The sample function first processes

60
00:03:04,505 --> 00:03:08,035
these characters in sequence adding them to a list of characters.

61
00:03:08,035 --> 00:03:11,179
It then calls predict on these characters passing in our model,

62
00:03:11,179 --> 00:03:14,044
each character and hidden state and this returns

63
00:03:14,044 --> 00:03:17,485
the next character after our prime sequence and the hidden state.

64
00:03:17,485 --> 00:03:20,870
So, here we have all our prime characters in the default case.

65
00:03:20,870 --> 00:03:22,175
This is going to be T, H,

66
00:03:22,175 --> 00:03:25,970
and E and then we're going to append the next most likely character.

67
00:03:25,970 --> 00:03:29,805
So, we're basically building up a list of characters here,

68
00:03:29,805 --> 00:03:32,660
then we're going to generate more and more characters.

69
00:03:32,660 --> 00:03:37,765
In this loop, we're passing in our model and the last character in our character list.

70
00:03:37,765 --> 00:03:40,450
This returns the next character and the hidden state.

71
00:03:40,450 --> 00:03:44,380
This character is appended to our list and the cycle starts all over again.

72
00:03:44,380 --> 00:03:47,930
So, predict is generating a next likely character which is

73
00:03:47,930 --> 00:03:52,005
appended to our list and then that goes back as input into our predict function.

74
00:03:52,005 --> 00:03:54,500
The effect is that we're getting next and next and

75
00:03:54,500 --> 00:03:57,740
next characters and adding them to our characters list,

76
00:03:57,740 --> 00:04:00,265
that is until we reach our desired text length.

77
00:04:00,265 --> 00:04:04,455
Finally, we join all these characters together to return a sample text,

78
00:04:04,455 --> 00:04:06,770
and here I've generated a couple samples.

79
00:04:06,770 --> 00:04:10,740
You can see that I've passed in my model that was trained for 20 epochs, and I said,

80
00:04:10,740 --> 00:04:15,145
generate a text that's 1,000 characters long starting with the prime word Anna.

81
00:04:15,145 --> 00:04:18,340
I've also passed in a value for top k equal to five.

82
00:04:18,340 --> 00:04:21,220
You can see that this starts with the prime word and generates

83
00:04:21,220 --> 00:04:24,195
what might be thought of as a paragraph of text in a book.

84
00:04:24,195 --> 00:04:26,515
Even with just a few prime characters,

85
00:04:26,515 --> 00:04:30,395
our model is definitely making complete and real words that make sense.

86
00:04:30,395 --> 00:04:32,875
The structure and spelling looks pretty good

87
00:04:32,875 --> 00:04:35,765
even if the content itself is a little confusing,

88
00:04:35,765 --> 00:04:38,800
and here's another example where I've loaded in a model by

89
00:04:38,800 --> 00:04:42,820
name and I'm using this loaded model to generate a longer piece of text,

90
00:04:42,820 --> 00:04:46,150
starting with the prime words, "And Levin said."

91
00:04:46,150 --> 00:04:47,740
So, this is pretty cool.

92
00:04:47,740 --> 00:04:51,235
A well-trained model can actually generate some text that makes some sense.

93
00:04:51,235 --> 00:04:53,750
It learned just from looking at long sequences of

94
00:04:53,750 --> 00:04:56,655
characters what characters were likely to come next.

95
00:04:56,655 --> 00:04:58,880
Then in our sampling and prediction code,

96
00:04:58,880 --> 00:05:04,115
we used top-k sampling and some randomness in selecting the best likely next character.

97
00:05:04,115 --> 00:05:07,630
You can train a model like this on any other text data.

98
00:05:07,630 --> 00:05:10,010
For example, you could try it on generating

99
00:05:10,010 --> 00:05:12,755
Shakespeare sonnets or another text of your choice.

100
00:05:12,755 --> 00:05:14,350
Great job on getting this far.

101
00:05:14,350 --> 00:05:18,330
You've really learned a lot about implementing RNNs in PyTorch.

