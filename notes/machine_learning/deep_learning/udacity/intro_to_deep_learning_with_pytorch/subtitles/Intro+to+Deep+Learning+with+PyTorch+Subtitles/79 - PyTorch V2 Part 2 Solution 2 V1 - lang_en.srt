1
00:00:00,000 --> 00:00:04,920
Welcome back. Here is my solution for the softmax function.

2
00:00:04,920 --> 00:00:07,215
Here in the numerator,

3
00:00:07,215 --> 00:00:08,850
we know we want to take the exponential,

4
00:00:08,850 --> 00:00:12,180
so it's pretty straight forward with torch.exp.

5
00:00:12,180 --> 00:00:14,520
So we're going to use the exponential of x,

6
00:00:14,520 --> 00:00:16,470
which is our input tensor.

7
00:00:16,470 --> 00:00:20,100
In the denominator, we know we want to do something like,

8
00:00:20,100 --> 00:00:23,150
again take exponentials so torch.exp,

9
00:00:23,150 --> 00:00:25,740
and then take the sum across all those values.

10
00:00:25,740 --> 00:00:29,940
So, one thing we need to remember is that we want the sum across one single row.

11
00:00:29,940 --> 00:00:33,915
So, each of the columns in one single row for each example.

12
00:00:33,915 --> 00:00:36,375
So, for one example, we want to sum up those values.

13
00:00:36,375 --> 00:00:38,160
So, for here in torch.sum,

14
00:00:38,160 --> 00:00:40,170
we're going to use dimension equals one.

15
00:00:40,170 --> 00:00:44,195
So, this is basically going to take the sum across the columns.

16
00:00:44,195 --> 00:00:46,985
What this does, torch.sum here,

17
00:00:46,985 --> 00:00:50,345
is going to actually going to give us a tensor,

18
00:00:50,345 --> 00:00:53,530
that is just a vector of 64 elements.

19
00:00:53,530 --> 00:00:54,870
So, the problem with this is that,

20
00:00:54,870 --> 00:00:56,880
if this is 64 by 10,

21
00:00:56,880 --> 00:01:00,115
and this is just a 64-long vector,

22
00:01:00,115 --> 00:01:06,575
it's going to try to divide every element in this tensor by all 64 of these values.

23
00:01:06,575 --> 00:01:11,450
So, it's going give us a 64 by 64 tensor, and that's not what we want.

24
00:01:11,450 --> 00:01:13,355
We want our output to be 64 by 10.

25
00:01:13,355 --> 00:01:20,090
So, what you actually need to do is reshape this tensor here to have 64 rows,

26
00:01:20,090 --> 00:01:22,345
but only one value for each of those rows.

27
00:01:22,345 --> 00:01:23,520
So, what that's going do,

28
00:01:23,520 --> 00:01:27,670
it's going look at for each row in this tensor,

29
00:01:27,670 --> 00:01:31,505
is going to look at the equivalent row in this tensor.

30
00:01:31,505 --> 00:01:35,180
So, since each row in this tensor only has one value,

31
00:01:35,180 --> 00:01:40,550
it's going to divide this exponential by the one value in this denominator tensor.

32
00:01:40,550 --> 00:01:42,700
This can be really tricky,

33
00:01:42,700 --> 00:01:49,279
but it's also super important to understand how broadcasting works in PyTorch,

34
00:01:49,279 --> 00:01:52,100
and how to actually fit all these tensors together with

35
00:01:52,100 --> 00:01:55,510
the correct shape and the correct operations to get everything out right.

36
00:01:55,510 --> 00:01:57,525
So, if we do this, it look what we have,

37
00:01:57,525 --> 00:02:01,340
we pass our output through the softmax function,

38
00:02:01,340 --> 00:02:02,390
and then we get our probabilities,

39
00:02:02,390 --> 00:02:05,285
and we can look the shape and it is 64 by 10,

40
00:02:05,285 --> 00:02:07,700
and if you take the sum across each of the rows,

41
00:02:07,700 --> 00:02:08,765
then it adds up to one,

42
00:02:08,765 --> 00:02:11,340
like it should with a proper probability distribution.

43
00:02:11,340 --> 00:02:17,870
So, now, we're going to look at how you use this nn module to build neural networks.

44
00:02:17,870 --> 00:02:22,940
So, you'll find that it's actually in a lot of ways simpler and more powerful.

45
00:02:22,940 --> 00:02:28,565
You'll be able to build larger and larger neural networks using the same framework.

46
00:02:28,565 --> 00:02:30,215
The way this works in general,

47
00:02:30,215 --> 00:02:32,375
is that we're going to create a new class,

48
00:02:32,375 --> 00:02:34,190
and you can call it networking,

49
00:02:34,190 --> 00:02:35,300
you can call it whatever you want,

50
00:02:35,300 --> 00:02:36,800
you can call it classifier,

51
00:02:36,800 --> 00:02:38,430
you can call it MNIST.

52
00:02:38,430 --> 00:02:40,920
It doesn't really matter so much what you call it,

53
00:02:40,920 --> 00:02:46,125
but you need to subclass it from nn.module.

54
00:02:46,125 --> 00:02:51,360
Then, in the init method, it's __init method.

55
00:02:51,360 --> 00:02:54,925
You need to call it super and run the init method of nn.module.

56
00:02:54,925 --> 00:02:57,120
So, you need to do this because then,

57
00:02:57,120 --> 00:02:59,285
PyTorch will know to register

58
00:02:59,285 --> 00:03:00,770
all the different layers and

59
00:03:00,770 --> 00:03:03,290
operations that you're going to be putting into this network.

60
00:03:03,290 --> 00:03:04,970
If you don't do this part then,

61
00:03:04,970 --> 00:03:06,740
it won't be able to track the things that

62
00:03:06,740 --> 00:03:08,680
you're adding to your network, and it just won't work.

63
00:03:08,680 --> 00:03:14,025
So, here, we can create our hidden layers using nn.Linear.

64
00:03:14,025 --> 00:03:15,230
So, what this does,

65
00:03:15,230 --> 00:03:19,250
is it creates a operation for the linear transformation.

66
00:03:19,250 --> 00:03:24,370
So, when we take our inputs x and then multiply it by weights and add your bias terms,

67
00:03:24,370 --> 00:03:26,205
that's a linear transformation.

68
00:03:26,205 --> 00:03:29,110
So, what this does is calling NN.Linear,

69
00:03:29,110 --> 00:03:32,330
it creates an object that itself has created

70
00:03:32,330 --> 00:03:35,620
parameters for the weights and parameters for the bias and then,

71
00:03:35,620 --> 00:03:39,270
when you pass a tensor through this hidden layer,

72
00:03:39,270 --> 00:03:43,540
this object, it's going to automatically calculate the linear transformation for you.

73
00:03:43,540 --> 00:03:47,540
So, all you really need to do is tell it what's the size of the inputs,

74
00:03:47,540 --> 00:03:48,770
and then what are the size of the output.

75
00:03:48,770 --> 00:03:52,220
So, 784 by 256,

76
00:03:52,220 --> 00:03:54,590
we're going to use 256 outputs for this.

77
00:03:54,590 --> 00:03:58,105
So, it's kind of rebuilding the network that we saw before.

78
00:03:58,105 --> 00:04:04,115
Similarly, we want another linear transformation between our hidden units and our output.

79
00:04:04,115 --> 00:04:06,695
So, again, we have 256 hidden units,

80
00:04:06,695 --> 00:04:09,690
and we have 10 outputs, 10 output units,

81
00:04:09,690 --> 00:04:14,120
so we're going to create a output layer called self.output,

82
00:04:14,120 --> 00:04:17,530
and create this linear transformation operation.

83
00:04:17,530 --> 00:04:21,470
We also want to create a sigmoid operation for the activation and then,

84
00:04:21,470 --> 00:04:23,150
softmax for the output,

85
00:04:23,150 --> 00:04:25,135
so we get this probability distribution.

86
00:04:25,135 --> 00:04:28,815
Now, we're going to create a forward method and so,

87
00:04:28,815 --> 00:04:31,110
forward is basically going to be,

88
00:04:31,110 --> 00:04:34,890
as we pass a tensor in to the network.

89
00:04:34,890 --> 00:04:36,520
It's gonna go through all these operations,

90
00:04:36,520 --> 00:04:37,945
and eventually give us our output.

91
00:04:37,945 --> 00:04:41,120
So, here, x, the argument is going to be the input tensor and then,

92
00:04:41,120 --> 00:04:43,560
we're going to pass it through our hidden layer.

93
00:04:43,560 --> 00:04:47,095
So, this is again, like this linear transformation that we defined up here,

94
00:04:47,095 --> 00:04:50,510
and it's going to go through a sigmoid activation,

95
00:04:50,510 --> 00:04:55,665
and then through our output layer or output linear transformation, we have here,

96
00:04:55,665 --> 00:04:57,924
and then through the sigmoid function,

97
00:04:57,924 --> 00:05:02,060
and then finally return the output of our softmax.

98
00:05:02,060 --> 00:05:03,725
so we can create this.

99
00:05:03,725 --> 00:05:07,140
Then, if we kind of look at it, so it'll print it out,

100
00:05:07,140 --> 00:05:10,400
and it'll tell us the operations, and not necessarily the order,

101
00:05:10,400 --> 00:05:14,845
but at least it tells us the operations that we have defined for this network.

102
00:05:14,845 --> 00:05:20,375
You can also use some functional definitions for things like sigmoid and softmax,

103
00:05:20,375 --> 00:05:24,950
and it kind of makes the class the way you write the code a little bit cleaner.

104
00:05:24,950 --> 00:05:27,785
We can get that from torch.nn.functional.

105
00:05:27,785 --> 00:05:32,690
Most of the time, you'll see is like import torch.nn.functional as capital F. So,

106
00:05:32,690 --> 00:05:35,545
there's kind of that convention in PyTorch code.

107
00:05:35,545 --> 00:05:38,970
So, again, we define our linear transformations,

108
00:05:38,970 --> 00:05:43,335
self.hidden, self.output but now in our forward method.

109
00:05:43,335 --> 00:05:47,910
So, we can call self.hidden to get like our values for hidden layer, but then,

110
00:05:47,910 --> 00:05:49,785
we pass it through the sigmoid function,

111
00:05:49,785 --> 00:05:53,310
f.sigmoid, and the same thing with the output layers.

112
00:05:53,310 --> 00:05:56,990
So, we have our output linear transformations of the output,

113
00:05:56,990 --> 00:05:59,510
and we pass it through this softmax operation.

114
00:05:59,510 --> 00:06:01,865
So, the reason we can do this because,

115
00:06:01,865 --> 00:06:04,445
when we create these linear transformations,

116
00:06:04,445 --> 00:06:09,665
it's creating the weights and bias matrices on its own.

117
00:06:09,665 --> 00:06:11,870
But for sigmoid and softmax,

118
00:06:11,870 --> 00:06:14,330
it's just an element wise operation,

119
00:06:14,330 --> 00:06:17,120
so it doesn't have to create any extra parameters

120
00:06:17,120 --> 00:06:19,550
or extra matrices to do these operations,

121
00:06:19,550 --> 00:06:22,354
and so we can have these be purely functional

122
00:06:22,354 --> 00:06:25,700
without having to create any sort of object or classes.

123
00:06:25,700 --> 00:06:28,385
However, they are equivalent.

124
00:06:28,385 --> 00:06:33,440
So this way to build the network is equivalent to this way up here,

125
00:06:33,440 --> 00:06:36,260
but it's a little bit more succinct when you're

126
00:06:36,260 --> 00:06:39,490
doing it with these kind of functional pattern.

127
00:06:39,490 --> 00:06:44,150
So far, we've only been using the sigmoid function as an activation function,

128
00:06:44,150 --> 00:06:45,365
but there are, of course,

129
00:06:45,365 --> 00:06:47,660
a lot of different ones you want to use.

130
00:06:47,660 --> 00:06:49,700
Really the only requirement is that,

131
00:06:49,700 --> 00:06:53,300
these activation functions should typically be non-linear.

132
00:06:53,300 --> 00:07:00,260
So, if you want your network to be able to learn non-linear correlations and patterns,

133
00:07:00,260 --> 00:07:02,745
and we want the output to be non-linear,

134
00:07:02,745 --> 00:07:07,010
then you need to use non-linear activation functions in your hidden layers.

135
00:07:07,010 --> 00:07:09,800
So, a sigmoid is one example.

136
00:07:09,800 --> 00:07:12,065
The hyperbolic tangent is another.

137
00:07:12,065 --> 00:07:14,510
One that is pretty much used all the time,

138
00:07:14,510 --> 00:07:17,880
like almost exclusively as activation function and hidden layers,

139
00:07:17,880 --> 00:07:20,670
is the ReLU, so the rectified linear unit.

140
00:07:20,670 --> 00:07:24,845
This is basically the simplest non-linear function that you can use,

141
00:07:24,845 --> 00:07:28,330
and it turns out that networks tend to train a lot faster

142
00:07:28,330 --> 00:07:32,810
when using ReLU as compared to sigmoid and hyperbolic tangent,

143
00:07:32,810 --> 00:07:35,465
so ReLU was what we typically use.

144
00:07:35,465 --> 00:07:40,160
Okay. So, here, you're going to build your own neural network, that's larger.

145
00:07:40,160 --> 00:07:43,170
So, this time, it's going to have two hidden layers,

146
00:07:43,170 --> 00:07:49,010
and you'll be using the ReLU activation function for this on your hidden layers.

147
00:07:49,010 --> 00:07:55,520
So using this object-oriented class method within a.module,

148
00:07:55,520 --> 00:07:59,135
go ahead and build a network that looks like this,

149
00:07:59,135 --> 00:08:00,935
with 784 input units,

150
00:08:00,935 --> 00:08:03,560
a 128 units in the first hidden layer,

151
00:08:03,560 --> 00:08:05,930
64 units and the second hidden layer,

152
00:08:05,930 --> 00:08:09,000
and then 10 output units. All right. Cheers.

