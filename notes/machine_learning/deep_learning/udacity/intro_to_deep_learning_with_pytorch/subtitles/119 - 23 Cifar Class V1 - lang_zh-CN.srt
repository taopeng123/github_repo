1
00:00:00,000 --> 00:00:04,635
CIFAR-10 是一个很热门的数据集 其中包含 60,000 张小图

2
00:00:04,635 --> 00:00:07,825
共分成 10 个类别 每张图都描绘了一个对象

3
00:00:07,825 --> 00:00:10,275
可以看出每张图都很小

4
00:00:10,275 --> 00:00:12,705
宽和高只有 32 个像素

5
00:00:12,705 --> 00:00:15,690
它们是彩色图像 所以计算机将它们解析为

6
00:00:15,690 --> 00:00:18,905
深度为 3 的数组 3 表示 RGB 颜色通道

7
00:00:18,905 --> 00:00:21,820
因为它们是彩色图像并且有很多

8
00:00:21,820 --> 00:00:25,470
所以首先我将演示如何使用 GPU 训练模型

9
00:00:25,470 --> 00:00:28,470
GPU 使你能够并行地处理数据

10
00:00:28,470 --> 00:00:33,150
所以也许能够缩短训练时间

11
00:00:33,150 --> 00:00:36,350
我在这里照常加载库

12
00:00:36,350 --> 00:00:40,115
PyTorch 提供了一种检测是否有可用 GPU 设备的方式

13
00:00:40,115 --> 00:00:43,280
torch.cuda.is_available 将返回布尔值true 或 false

14
00:00:43,280 --> 00:00:46,985
表示 GPU 设备是否可用

15
00:00:46,985 --> 00:00:50,630
我们将存储该变量 之后当 GPU 可用时

16
00:00:50,630 --> 00:00:54,290
我们能够将数据和模型移到该设备上并加快训练

17
00:00:54,290 --> 00:00:58,190
暂时我将可视化数据并演示该问题 所以我将使用笔记本电脑

18
00:00:58,190 --> 00:01:02,700
当我运行此单元格时 可以看出我是在 CPU 上训练

19
00:01:02,700 --> 00:01:06,140
稍后 当我尝试开发解决方案并训练模型时

20
00:01:06,140 --> 00:01:10,895
我将切换到 GPU 这是通常的做法

21
00:01:10,895 --> 00:01:13,670
接下来我将照常加载数据

22
00:01:13,670 --> 00:01:18,390
和 MNIST 很像   CIFAR 也位于 torchvision datasets 库中

23
00:01:18,390 --> 00:01:22,625
我将通过调用 datasets.CIFAR10 定义训练和测试数据

24
00:01:22,625 --> 00:01:27,040
此外 将此数据转换为张量数据类型并标准化 RGB 值

25
00:01:27,040 --> 00:01:31,565
以便像素值的范围是在 0 到 1 之间

26
00:01:31,565 --> 00:01:37,060
最后 使用 PyTorch 的 DataLoader 类按批次加载转换的数据

27
00:01:37,060 --> 00:01:39,340
这几行代码应该看起来和 MNIST 代码很像

28
00:01:39,340 --> 00:01:41,705
我们加载了数据

29
00:01:41,705 --> 00:01:43,655
转换为张量数据类型

30
00:01:43,655 --> 00:01:45,600
将数据划分为训练集

31
00:01:45,600 --> 00:01:47,680
验证集和测试集

32
00:01:47,680 --> 00:01:51,975
然后在下面指定要读取的 10 个图像类别

33
00:01:51,975 --> 00:01:55,490
所有 CIFAR-10 图像都将属于其中一个类别

34
00:01:55,490 --> 00:01:57,850
数据加载可能需要一段时间

35
00:01:57,850 --> 00:02:00,395
加载之后 你可以可视化一批数据

36
00:02:00,395 --> 00:02:04,790
我在这里定义了一个辅助函数 它会取消标准化图像

37
00:02:04,790 --> 00:02:09,505
并将它们从张量图像类型转换为 NumPy 图像类型以进行可视化

38
00:02:09,505 --> 00:02:12,110
然后加载并显示一批数据

39
00:02:12,110 --> 00:02:16,715
从这里可以看出图像看起来和我们预期的一样

40
00:02:16,715 --> 00:02:18,170
这些图像包括猫

41
00:02:18,170 --> 00:02:21,290
青蛙 小鹿和汽车

42
00:02:21,290 --> 00:02:24,985
甚至可以更详细地查看图像

43
00:02:24,985 --> 00:02:27,170
我将一个图像的红绿蓝颜色通道

44
00:02:27,170 --> 00:02:32,030
显示为灰阶值

45
00:02:32,030 --> 00:02:34,160
应该会看到最亮的像素值接近 1

46
00:02:34,160 --> 00:02:37,305
更暗的像素值接近 0

47
00:02:37,305 --> 00:02:41,745
接下来 你需要定义并训练 CNN 分类这些图像

48
00:02:41,745 --> 00:02:44,250
你也可以尝试 MLP 方法

49
00:02:44,250 --> 00:02:46,435
看看二者的效果并加以比较

50
00:02:46,435 --> 00:02:50,450
假定你想定义 CNN 架构

51
00:02:50,450 --> 00:02:54,800
我们提供了 PyTorch 卷积层和最大池化层的文档链接

52
00:02:54,800 --> 00:02:56,630
这个示例图表显示了

53
00:02:56,630 --> 00:02:59,415
输入图像如何经过这几个层级

54
00:02:59,415 --> 00:03:00,850
向下滚动

55
00:03:00,850 --> 00:03:04,340
你会发现我为你定义了一个示例卷积层

56
00:03:04,340 --> 00:03:06,995
我在网络的 init 函数中定义了该卷积层

57
00:03:06,995 --> 00:03:09,020
要定义卷积层

58
00:03:09,020 --> 00:03:12,895
我调用了 nn.Conv2d 并传入一些参数

59
00:03:12,895 --> 00:03:14,525
对于第一个卷积层

60
00:03:14,525 --> 00:03:18,290
这个参数将是输入图像的深度

61
00:03:18,290 --> 00:03:24,415
输入是 32x32 图像 深度是 3 表示 RGB 颜色通道

62
00:03:24,415 --> 00:03:27,500
所以将该输入设为 3

63
00:03:27,500 --> 00:03:30,800
并指定输出卷积层的深度应该为 16

64
00:03:30,800 --> 00:03:35,930
表示该层应该接受输入图像并生成 16 个过滤图像

65
00:03:35,930 --> 00:03:39,310
同时指定过滤器大小是 3x2

66
00:03:39,310 --> 00:03:44,050
为了让输出层的 x,y 大小和输入图像的一样

67
00:03:44,050 --> 00:03:46,650
我将边界填充设为一个像素

68
00:03:46,650 --> 00:03:49,980
我还定义了一个最大池化层 称为 pool

69
00:03:49,980 --> 00:03:53,990
核大小和步长为 2

70
00:03:53,990 --> 00:03:58,510
表示会将任何输入的 x,y 维度下采样到一半大小

71
00:03:58,510 --> 00:03:59,640
然后在 forward 函数中

72
00:03:59,640 --> 00:04:02,000
将所有这一切组合到一起

73
00:04:02,000 --> 00:04:03,885
对于输入图像 x

74
00:04:03,885 --> 00:04:05,080
首先将其传入第一个卷积层

75
00:04:05,080 --> 00:04:09,300
并应用 ReLu 激活函数

76
00:04:09,300 --> 00:04:12,710
将这个输出传入最后的池化层

77
00:04:12,710 --> 00:04:15,900
生成并返回被下采样的转换后 x

78
00:04:15,900 --> 00:04:16,930
要完成此模型

79
00:04:16,930 --> 00:04:21,020
你需要添加多个卷积层和池化层

80
00:04:21,020 --> 00:04:23,390
最后扁平化并应用一个全连接层

81
00:04:23,390 --> 00:04:26,285
从而生成所需数量的类别分数

82
00:04:26,285 --> 00:04:28,265
定义完整的 CNN 后

83
00:04:28,265 --> 00:04:31,840
你可以实例化该模型 如果有 GPU 的话 甚至可以移到 GPU 上

84
00:04:31,840 --> 00:04:35,090
接下来你需要为此训练任务

85
00:04:35,090 --> 00:04:38,440
定义合适的损失和优化函数

86
00:04:38,440 --> 00:04:40,350
最后我为你提供了训练循环

87
00:04:40,350 --> 00:04:44,135
你可能需要增加最终模型的训练周期数

88
00:04:44,135 --> 00:04:47,950
但是这个循环会跟踪训练和验证损失

89
00:04:47,950 --> 00:04:50,660
如果验证损失在某个周期后降低了

90
00:04:50,660 --> 00:04:52,100
模型将保存

91
00:04:52,100 --> 00:04:54,650
然后你将能够测试训练过的模型

92
00:04:54,650 --> 00:04:57,470
看看它在每种测试图像上表现如何

93
00:04:57,470 --> 00:05:02,030
最大的挑战是定义完整的 CNN

94
00:05:02,030 --> 00:05:03,650
像往常一样 我建议你在定义模型之前先查阅资料

95
00:05:03,650 --> 00:05:06,695
再做出正确的决策

96
00:05:06,695 --> 00:05:09,710
请尝试自己完成这道练习

97
00:05:09,710 --> 00:05:12,800
接下来我将介绍一种可能的解决方案供你参考

