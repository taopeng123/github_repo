1
00:00:00,000 --> 00:00:02,190
I'm going to quickly show you how I went about

2
00:00:02,190 --> 00:00:05,545
defining and training an MLP for image classification.

3
00:00:05,545 --> 00:00:09,490
So first, I just loaded in and looked at my normalized image data.

4
00:00:09,490 --> 00:00:12,750
Then, I started to define the classification model.

5
00:00:12,750 --> 00:00:15,930
I started with the fully connected layer FC1 that sees

6
00:00:15,930 --> 00:00:20,290
the 784 entry long vector that represents a flattened image as input.

7
00:00:20,290 --> 00:00:23,280
Then, I proceed it based on the resource that I found,

8
00:00:23,280 --> 00:00:27,170
which had two hidden layers with 512 inputs and outputs.

9
00:00:27,170 --> 00:00:31,775
I've actually stored those values here as two variables, hidden_1 and hidden_2.

10
00:00:31,775 --> 00:00:34,800
This is an extra step, but it makes it easy should I want to change

11
00:00:34,800 --> 00:00:37,630
these values later on for testing or something else.

12
00:00:37,630 --> 00:00:39,979
So, to complete my first fully connected layer,

13
00:00:39,979 --> 00:00:43,245
I put hidden_1 as the number of outputs I want this to produce.

14
00:00:43,245 --> 00:00:46,425
Then, I create a second fully connected layer, FC2,

15
00:00:46,425 --> 00:00:51,350
which takes in that number of outputs and produces 512 outputs again.

16
00:00:51,350 --> 00:00:56,215
I also want to be clear that the outputs of one layer feed into the inputs of the next.

17
00:00:56,215 --> 00:00:59,480
Then, I've defined the last fully connected layer FC3,

18
00:00:59,480 --> 00:01:03,670
such that it sees 512 inputs and produces 10 outputs.

19
00:01:03,670 --> 00:01:08,330
The 10 outputs correspond to the number of digit classes zero through nine.

20
00:01:08,330 --> 00:01:11,255
So, this layer will produce our class scores.

21
00:01:11,255 --> 00:01:13,400
Finally, still in the init function,

22
00:01:13,400 --> 00:01:18,520
I've also defined a dropout layer with a dropout probability of 0.2 or 205.

23
00:01:18,520 --> 00:01:23,330
Now that I've defined all the layers that I need to make up this classification MLP,

24
00:01:23,330 --> 00:01:26,695
I have to explicitly define the feedforward behavior of the network.

25
00:01:26,695 --> 00:01:28,170
With the forward function,

26
00:01:28,170 --> 00:01:32,415
I basically want to answer how will an input vector proceed through these layers?

27
00:01:32,415 --> 00:01:36,485
So, the input x starts out as a 28 by 28 image tensor,

28
00:01:36,485 --> 00:01:40,335
and my first step is to flatten it into a 784-length vector.

29
00:01:40,335 --> 00:01:41,810
Once the input is flattened,

30
00:01:41,810 --> 00:01:44,280
I'm passing it through the first fully connected layer,

31
00:01:44,280 --> 00:01:47,425
FC1, and applying an activation function.

32
00:01:47,425 --> 00:01:52,175
Next, I'm going to do the exact same thing with our second fully connected layer, FC2.

33
00:01:52,175 --> 00:01:53,855
Only in-between these two layers,

34
00:01:53,855 --> 00:01:57,725
I'm actually adding a dropout layer which would help prevent over-fitting.

35
00:01:57,725 --> 00:02:00,380
After x passes through our second hidden layer,

36
00:02:00,380 --> 00:02:04,370
we're going to add one more dropout layer and reach our final fully connected layer.

37
00:02:04,370 --> 00:02:08,450
You'll notice that I'm not applying an activation function to this final layer.

38
00:02:08,450 --> 00:02:12,595
That's because later it will have a softmax activation function applied to it.

39
00:02:12,595 --> 00:02:17,365
So right now, I am leaving it as is and returning the final transformed x.

40
00:02:17,365 --> 00:02:20,390
Because FC3 produces 10 outputs,

41
00:02:20,390 --> 00:02:23,245
this x should represent our 10 class scores.

42
00:02:23,245 --> 00:02:26,015
Okay. Then, I'm instantiating and printing

43
00:02:26,015 --> 00:02:28,850
out the net to make sure it has the layers that I want.

44
00:02:28,850 --> 00:02:31,655
I should see our three linear layers and our dropout.

45
00:02:31,655 --> 00:02:35,115
The next step is defining the loss and optimization functions.

46
00:02:35,115 --> 00:02:38,780
Here, I'm defining the last criterion as cross-entropy loss.

47
00:02:38,780 --> 00:02:41,880
This is just a pretty standard loss for any classification task.

48
00:02:41,880 --> 00:02:44,675
Then, I'm going to use stochastic gradient descent,

49
00:02:44,675 --> 00:02:47,465
SGD, from the torch optimization library.

50
00:02:47,465 --> 00:02:50,600
This takes in our model parameters and a learning rate.

51
00:02:50,600 --> 00:02:53,260
I've set a learning rate of 0.01.

52
00:02:53,260 --> 00:02:56,745
If you find that your loss is decreasing too slowly or sporadically,

53
00:02:56,745 --> 00:02:58,165
you can change this value.

54
00:02:58,165 --> 00:03:02,050
Next, I spent some time training this model for 50 epochs.

55
00:03:02,050 --> 00:03:05,240
This took some time because I'm just using my CPU for now,

56
00:03:05,240 --> 00:03:08,180
and later I'll show you how to use a GPU for faster training.

57
00:03:08,180 --> 00:03:10,430
At the end of each of these epochs I printed out

58
00:03:10,430 --> 00:03:13,415
the training loss and watched how it decreased over time.

59
00:03:13,415 --> 00:03:16,970
You can see that it decreased fairly quickly at first and then later

60
00:03:16,970 --> 00:03:21,240
on slows down especially around the 40 epoch mark.

61
00:03:21,240 --> 00:03:24,520
But it is still decreasing up to epoch 50.

62
00:03:24,520 --> 00:03:29,060
Then, to actually see how well my trained model generalizes to new data,

63
00:03:29,060 --> 00:03:32,165
I tested on our test data that we loaded in at the start.

64
00:03:32,165 --> 00:03:35,225
Here, I've iterated through all the data in our test litter.

65
00:03:35,225 --> 00:03:38,165
I applied the model and recorded the test loss.

66
00:03:38,165 --> 00:03:42,140
Recall that our model is actually returning a list of class scores.

67
00:03:42,140 --> 00:03:43,835
To isolate our predicted class,

68
00:03:43,835 --> 00:03:48,330
I'm going to take the maximum value of the scores and return it as our prediction.

69
00:03:48,330 --> 00:03:51,795
Then, I compare this prediction to our target label.

70
00:03:51,795 --> 00:03:55,195
This creates a list whether a certain prediction was correct or not,

71
00:03:55,195 --> 00:03:58,070
then I actually separate these into the 10 classes

72
00:03:58,070 --> 00:04:00,740
and I print out the accuracy for each class.

73
00:04:00,740 --> 00:04:06,030
I have our overall test loss and our overall accuracy which is 97%.

74
00:04:06,030 --> 00:04:08,060
This is pretty good, and you can see among

75
00:04:08,060 --> 00:04:10,695
all the digit classes this value is pretty consistent.

76
00:04:10,695 --> 00:04:14,500
The model seems to do the worst on images of the number seven or eight.

77
00:04:14,500 --> 00:04:16,910
But an even distribution indicates that your model

78
00:04:16,910 --> 00:04:19,755
is trained pretty evenly on each type of data.

79
00:04:19,755 --> 00:04:21,130
I've also included a cell,

80
00:04:21,130 --> 00:04:25,534
where you can display test images and the predicted and true labels side-by-side.

81
00:04:25,534 --> 00:04:29,340
This makes it easy to see if you've gotten any specific image wrong.

82
00:04:29,340 --> 00:04:32,270
But going back up to the test accuracy overall,

83
00:04:32,270 --> 00:04:34,450
I actually wonder if I could do even better.

84
00:04:34,450 --> 00:04:36,500
If I could improve this model by adding, say,

85
00:04:36,500 --> 00:04:38,590
another layer to find more patterns in the data.

86
00:04:38,590 --> 00:04:42,740
I even wonder if I chose the right point to stop training this model.

87
00:04:42,740 --> 00:04:45,230
In fact, one thing to note is that I stopped training at

88
00:04:45,230 --> 00:04:49,220
50 epochs based on only how I expected the loss to decrease over time.

89
00:04:49,220 --> 00:04:51,835
But it's more of an art than a science at this point.

90
00:04:51,835 --> 00:04:56,360
So next, I'll talk about one more concrete method of knowing when to stop training.

91
00:04:56,360 --> 00:04:58,730
A technique called model validation.

