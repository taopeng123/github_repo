1
00:00:00,000 --> 00:00:04,510
We wanted to define a character RNN with a two layer LSTM.

2
00:00:04,510 --> 00:00:07,200
Here in my solution, I am running this code on

3
00:00:07,200 --> 00:00:11,275
GPU and here's my code for defining our character level RNN.

4
00:00:11,275 --> 00:00:15,255
First, I defined an LSTM layer, self.lstm.

5
00:00:15,255 --> 00:00:17,020
This takes in an input size,

6
00:00:17,020 --> 00:00:18,555
which is going to be the length of

7
00:00:18,555 --> 00:00:20,700
a one-hot encoded input character and

8
00:00:20,700 --> 00:00:23,365
that's just the length of all of my unique characters.

9
00:00:23,365 --> 00:00:25,940
Then, it takes a hidden dimension a number of

10
00:00:25,940 --> 00:00:29,030
layers and a dropout probability that we've specified.

11
00:00:29,030 --> 00:00:33,685
Remember that this will create a dropout layer in between multiple LSTM layers,

12
00:00:33,685 --> 00:00:36,335
and all of these are parameters that are going to be passed

13
00:00:36,335 --> 00:00:39,190
in as input to our RNN when it's constructed.

14
00:00:39,190 --> 00:00:43,340
Then, I've set batch first to true because when we created our batch data,

15
00:00:43,340 --> 00:00:45,270
the first dimension is the batch size,

16
00:00:45,270 --> 00:00:47,215
rather than the sequence length.

17
00:00:47,215 --> 00:00:49,835
Okay. Next, I've defined a dropout layer to go

18
00:00:49,835 --> 00:00:52,940
in-between my LSTM and a final linear layer.

19
00:00:52,940 --> 00:00:56,510
Then, I have FC, my final fully connected linear layer.

20
00:00:56,510 --> 00:00:59,120
This takes in our LSTM outputs,

21
00:00:59,120 --> 00:01:01,045
which are going to be dimension and hidden.

22
00:01:01,045 --> 00:01:05,555
It's going to output our character class scores for the most likely next character.

23
00:01:05,555 --> 00:01:08,970
So, these are the class scores for each possible next character.

24
00:01:08,970 --> 00:01:11,480
This output size is the same size as our input,

25
00:01:11,480 --> 00:01:13,605
the length of our character vocabulary.

26
00:01:13,605 --> 00:01:15,935
Then, I move to the forward function.

27
00:01:15,935 --> 00:01:20,275
I'm passing my input X and a hidden state to my LSTM layer here.

28
00:01:20,275 --> 00:01:23,810
This produces my LSTM output and a new hidden state.

29
00:01:23,810 --> 00:01:26,270
I'm going to pass the LSTM output through

30
00:01:26,270 --> 00:01:29,120
the dropout layer that I defined here to get a new output.

31
00:01:29,120 --> 00:01:31,345
Then, I'm making sure to reshape this output,

32
00:01:31,345 --> 00:01:33,845
so that the last dimension is our hidden dim.

33
00:01:33,845 --> 00:01:38,610
This negative one basically means I'm going to be stacking up the outputs of the LSTM.

34
00:01:38,610 --> 00:01:42,735
Finally, I'm passing this V-shaped output to the final fully connected layer.

35
00:01:42,735 --> 00:01:45,080
Then, I'm returning this final output and

36
00:01:45,080 --> 00:01:47,595
the hidden state that was generated by our LSTM.

37
00:01:47,595 --> 00:01:51,915
These two functions in addition to the init hidden function complete my model.

38
00:01:51,915 --> 00:01:57,475
Next, it's time to train and let's take a look at the training loop that was provided.

39
00:01:57,475 --> 00:02:00,740
This function takes in a model to train some data,

40
00:02:00,740 --> 00:02:02,320
and the number of epics to train for,

41
00:02:02,320 --> 00:02:03,650
and a batch size,

42
00:02:03,650 --> 00:02:06,330
and sequence length that define our mini batch size.

43
00:02:06,330 --> 00:02:09,140
It also takes in a few more training parameters.

44
00:02:09,140 --> 00:02:13,140
First in here, I've defined my optimizer and my loss function.

45
00:02:13,140 --> 00:02:16,055
The optimizer is a standard Adam optimizer with

46
00:02:16,055 --> 00:02:19,070
a learning rate set to the past and learning rate up here.

47
00:02:19,070 --> 00:02:21,360
The last function is cross entropy loss,

48
00:02:21,360 --> 00:02:24,775
which is useful for when we're outputting character class scores.

49
00:02:24,775 --> 00:02:26,960
Here, you'll see some details about creating

50
00:02:26,960 --> 00:02:31,105
some validation data and moving our model to GPU if it's available.

51
00:02:31,105 --> 00:02:33,775
Here, you can see the start of our epic loop.

52
00:02:33,775 --> 00:02:35,460
At the start of each epic,

53
00:02:35,460 --> 00:02:38,255
I'm initializing the hidden state of our LSTM.

54
00:02:38,255 --> 00:02:41,750
Recall that this takes in the batch size of our data to define the size of

55
00:02:41,750 --> 00:02:45,970
the hidden state and it returns a hidden in cell state that are all zeros.

56
00:02:45,970 --> 00:02:47,695
Then, inside this epic loop,

57
00:02:47,695 --> 00:02:49,105
I have my batch loop.

58
00:02:49,105 --> 00:02:53,335
This is getting our X and Y mini batches from our get batches generator.

59
00:02:53,335 --> 00:02:57,090
Remember that this function basically iterates through are encoded data,

60
00:02:57,090 --> 00:02:59,810
and returns batches of inputs X and targets

61
00:02:59,810 --> 00:03:04,390
Y. I'm then converting the input into a one-hot encoded representation,

62
00:03:04,390 --> 00:03:06,925
and I'm converting both X and Y are

63
00:03:06,925 --> 00:03:10,335
inputs and targets into Tensors that can be seen by our model.

64
00:03:10,335 --> 00:03:14,920
If GPU's available, I'm moving those inputs and targets to our GPU device.

65
00:03:14,920 --> 00:03:17,875
The next thing that you see is making sure that we detach

66
00:03:17,875 --> 00:03:20,590
any past in hidden state from its history.

67
00:03:20,590 --> 00:03:23,530
Recall that the hidden state of an LSTM layer is a Tuple,

68
00:03:23,530 --> 00:03:25,630
and so here, we are getting the data as a tuple.

69
00:03:25,630 --> 00:03:28,815
Then, we proceed with back propagation as usual.

70
00:03:28,815 --> 00:03:34,615
We zero out any accumulated gradients and pass in our input Tensors to our model.

71
00:03:34,615 --> 00:03:37,090
We also pass in the latest hidden state here.

72
00:03:37,090 --> 00:03:40,820
In this returns of final output and a new hidden state,

73
00:03:40,820 --> 00:03:44,885
then we calculate the loss by looking at the predicted output and the targets.

74
00:03:44,885 --> 00:03:47,420
Recall that in the forward function of our model,

75
00:03:47,420 --> 00:03:52,460
I smashed the batch size and sequence length of our LSTM outputs into one dimension,

76
00:03:52,460 --> 00:03:55,150
and so I'm doing the same thing for our targets here.

77
00:03:55,150 --> 00:03:57,575
Then, we're performing back propagation and moving

78
00:03:57,575 --> 00:04:00,690
one step in the right direction updating the weights of our network.

79
00:04:00,690 --> 00:04:02,420
Now before the optimization step,

80
00:04:02,420 --> 00:04:05,045
I've added one line of code that may look unfamiliar.

81
00:04:05,045 --> 00:04:07,105
I'm calling clip grad norm.

82
00:04:07,105 --> 00:04:10,970
Now, this kind of LSTM model has one main problem with gradients.

83
00:04:10,970 --> 00:04:13,220
They can explode and get really, really big.

84
00:04:13,220 --> 00:04:15,490
So, what we do is we can clip the gradients,

85
00:04:15,490 --> 00:04:17,570
we just set some clip threshold,

86
00:04:17,570 --> 00:04:20,305
and then if the gradient is larger than that threshold,

87
00:04:20,305 --> 00:04:22,530
we set it to that clip threshold,

88
00:04:22,530 --> 00:04:24,530
and encode we do this by just passing in

89
00:04:24,530 --> 00:04:27,840
the parameters and the value that we want to clip the gradients at.

90
00:04:27,840 --> 00:04:29,990
In this case, this value is passed in,

91
00:04:29,990 --> 00:04:32,485
in our train function as a value five.

92
00:04:32,485 --> 00:04:34,205
Okay. So, we take a backwards step,

93
00:04:34,205 --> 00:04:35,510
then we clip our gradients,

94
00:04:35,510 --> 00:04:37,520
and we perform an optimization step.

95
00:04:37,520 --> 00:04:40,445
At the end here, I'm doing something very similar for processing

96
00:04:40,445 --> 00:04:43,860
our validation data except not performing the back propagation step.

97
00:04:43,860 --> 00:04:46,900
Then, I'm printing out some statistics about our loss.

98
00:04:46,900 --> 00:04:48,955
Now with this train function defined,

99
00:04:48,955 --> 00:04:51,740
I can go about instantiating and training a model.

100
00:04:51,740 --> 00:04:53,275
In the exercise notebook,

101
00:04:53,275 --> 00:04:55,930
I've left these hyper parameters for you to define.

102
00:04:55,930 --> 00:05:00,660
I've set our hidden dimension to the value of 512 and a number of layers up two,

103
00:05:00,660 --> 00:05:02,040
which we talked about before.

104
00:05:02,040 --> 00:05:04,520
Then, I have instantiated our model, and printed it out,

105
00:05:04,520 --> 00:05:07,580
and we can see that we have 83 unique characters as input,

106
00:05:07,580 --> 00:05:09,290
512 as a hidden dimension,

107
00:05:09,290 --> 00:05:11,420
and two layers in our LSTM.

108
00:05:11,420 --> 00:05:12,800
For a dropout layer,

109
00:05:12,800 --> 00:05:17,855
we have the default dropout value of 0.5 and for our last fully connected layer,

110
00:05:17,855 --> 00:05:19,055
we have our Input features,

111
00:05:19,055 --> 00:05:22,195
which is the same as this hidden dimension and our output features,

112
00:05:22,195 --> 00:05:23,610
the number of characters.

113
00:05:23,610 --> 00:05:26,210
Then, there are more hyper parameters that define

114
00:05:26,210 --> 00:05:29,565
our batch size sequence length and number of epics to train for.

115
00:05:29,565 --> 00:05:31,845
Here, I've set the sequence length to 100,

116
00:05:31,845 --> 00:05:33,365
which is a lot of characters,

117
00:05:33,365 --> 00:05:36,740
but it gives our model a great deal of context to learn from.

118
00:05:36,740 --> 00:05:39,020
I also want to note that the hidden dimension is

119
00:05:39,020 --> 00:05:41,840
basically the number of features that your model can detect.

120
00:05:41,840 --> 00:05:46,190
Larger values basically allow a network to learn more text features.

121
00:05:46,190 --> 00:05:50,830
There's some more information below in this notebook about defining hyper parameters.

122
00:05:50,830 --> 00:05:55,050
In general, I'll try to start out with a pretty big model like this,

123
00:05:55,050 --> 00:05:58,105
multiple LSTM layers and a large hidden dimension.

124
00:05:58,105 --> 00:06:01,115
Then, I'll basically take a look at the loss as

125
00:06:01,115 --> 00:06:04,475
this model trains and if it's decreasing, I'll keep going.

126
00:06:04,475 --> 00:06:06,285
But if it's not decreasing as I expect,

127
00:06:06,285 --> 00:06:08,780
then I'll probably change some hyper parameters.

128
00:06:08,780 --> 00:06:11,330
Our text data is pretty large and here,

129
00:06:11,330 --> 00:06:14,930
I've trained our entire model for 20 epics on GPU.

130
00:06:14,930 --> 00:06:19,300
I can see the training and validation loss over time decreasing.

131
00:06:19,300 --> 00:06:23,430
Around epic 15, I'm seeing the lost slow down a bit.

132
00:06:23,430 --> 00:06:25,520
But it actually looks like the validation and

133
00:06:25,520 --> 00:06:28,470
training loss are still decreasing even after epic 20.

134
00:06:28,470 --> 00:06:31,810
I could have stood to train for an even longer amount of time.

135
00:06:31,810 --> 00:06:34,670
I encourage you to read this information about setting

136
00:06:34,670 --> 00:06:38,135
the hyper parameters of a model and really getting the best model.

137
00:06:38,135 --> 00:06:40,760
Then, after you've trained a model like I've just done,

138
00:06:40,760 --> 00:06:43,280
you can save it by name and then there's one last step,

139
00:06:43,280 --> 00:06:47,055
which is using that model to make predictions and generate some new text,

140
00:06:47,055 --> 00:06:48,600
which I'll go over next.

