1
00:00:00,333 --> 00:00:03,859
Imagine uma rede neural
que reconhece imagens,

2
00:00:03,896 --> 00:00:05,548
na qual alimentamos
esta imagem.

3
00:00:05,585 --> 00:00:09,124
A rede adivinha que a imagem
deve ser um cachorro,

4
00:00:09,161 --> 00:00:13,076
com mais chance de ser um lobo
e menos chance de ser um peixe.

5
00:00:13,113 --> 00:00:17,260
Mas e se a imagem for um lobo?
Como a rede neural saberá?

6
00:00:17,297 --> 00:00:19,539
Imagine um programa de TV
sobre natureza

7
00:00:19,576 --> 00:00:22,459
em que a imagem anterior à do lobo
é a de um urso

8
00:00:22,496 --> 00:00:24,558
e, antes dela,
a de uma raposa.

9
00:00:24,595 --> 00:00:27,574
Neste caso, utilizamos
essas informações como dica

10
00:00:27,611 --> 00:00:30,351
de que a imagem
é um lobo e não um cachorro.

11
00:00:30,388 --> 00:00:34,933
Nós analisamos cada imagem
com a mesma cópia da rede neural,

12
00:00:34,970 --> 00:00:37,277
mas utilizamos a saída
da rede neural

13
00:00:37,314 --> 00:00:39,974
como parte da entrada
da próxima.

14
00:00:40,011 --> 00:00:42,517
Isso melhora os resultados.

15
00:00:42,554 --> 00:00:44,205
Matematicamente,
isso é simples.

16
00:00:44,242 --> 00:00:46,684
Combinamos os vetores
em uma função linear

17
00:00:46,721 --> 00:00:49,860
que serão "espremidos"
com uma função de ativação,

18
00:00:49,897 --> 00:00:52,573
que pode ser sigmoide
ou tangente hiperbólica.

19
00:00:52,610 --> 00:00:54,707
Utilizamos
as informações anteriores

20
00:00:54,744 --> 00:00:56,515
para que a rede neural saiba

21
00:00:56,552 --> 00:00:58,786
que o programa
é sobre animais selvagens

22
00:00:58,823 --> 00:01:01,379
e utilize as informações
para prever corretamente

23
00:01:01,416 --> 00:01:04,091
que a imagem é a de um lobo
e não a de um cachorro.

24
00:01:04,128 --> 00:01:07,046
É assim que as redes neurais
recorrentes funcionam.

25
00:01:07,083 --> 00:01:08,653
Mas existem
alguns problemas.

26
00:01:08,690 --> 00:01:10,181
Digamos que o urso apareceu

27
00:01:10,218 --> 00:01:13,788
e, logo depois,
uma árvore e um esquilo.

28
00:01:13,825 --> 00:01:17,261
A partir disso, não saberemos
se é lobo ou cachorro,

29
00:01:17,298 --> 00:01:22,332
pois árvores e esquilos
são comuns aos dois.

30
00:01:22,369 --> 00:01:26,429
Então a informação
sobre a floresta vem do urso.

31
00:01:26,466 --> 00:01:28,020
Mas, como já vimos,

32
00:01:28,057 --> 00:01:31,797
as informações são "espremidas"
por funções sigmoides

33
00:01:31,834 --> 00:01:33,277
e, pior do que isso,

34
00:01:33,314 --> 00:01:36,333
treinar uma rede
com retropropagação tão longa

35
00:01:36,370 --> 00:01:40,181
pode causar problemas
como o da dissipação do gradiente.

36
00:01:40,218 --> 00:01:43,621
Nesse ponto, as informações
sobre o urso se perderam,

37
00:01:43,658 --> 00:01:46,261
e esse é um problema
das redes neurais recorrentes,

38
00:01:46,298 --> 00:01:49,299
pois a memória armazenada
é de curta duração.

39
00:01:49,336 --> 00:01:52,437
As RNNs não armazenam
memórias de longa duração,

40
00:01:52,474 --> 00:01:54,132
e é aqui que as LSTMs,

41
00:01:54,169 --> 00:01:57,700
redes de memória de longa
e de curta duração, entram em ação.

42
00:01:57,737 --> 00:02:00,428
Resumindo,
a RNN funciona assim:

43
00:02:00,465 --> 00:02:03,243
a memória entra,
se mistura com o evento atual,

44
00:02:03,280 --> 00:02:06,291
e a saída será uma previsão
do que é a entrada,

45
00:02:06,328 --> 00:02:10,100
bem como a entrada da próxima
iteração da rede neural.

46
00:02:10,137 --> 00:02:12,700
De forma parecida,
uma LSTM funciona assim:

47
00:02:12,737 --> 00:02:16,196
ela rastreia
a memória de longa duração,

48
00:02:16,233 --> 00:02:18,203
que entra e sai,

49
00:02:18,240 --> 00:02:21,788
e a memória de curta duração,
que também entra e sai.

50
00:02:21,825 --> 00:02:25,764
Em cada estágio, a memória de longa
e de curta duração se misturam.

51
00:02:25,801 --> 00:02:28,604
A partir daí, teremos
a nova memória de longa duração,

52
00:02:28,641 --> 00:02:30,995
a de curta duração
e uma previsão.

53
00:02:31,032 --> 00:02:33,685
Aqui protegemos mais
as informações antigas.

54
00:02:33,722 --> 00:02:37,733
Assim a rede poderá se lembrar
de coisas muito antigas.

55
00:02:37,770 --> 00:02:38,998
Nos próximos vídeos,

56
00:02:39,035 --> 00:02:42,762
mostrarei a arquitetura
das LSTMs e como funcionam.

