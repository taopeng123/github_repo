1
00:00:00,000 --> 00:00:04,745
好的创建迷你批次数据后 现在该定义模型了

2
00:00:04,745 --> 00:00:08,115
此图表显示了最终模型的结构

3
00:00:08,115 --> 00:00:12,915
将这些字符传入输入层 然后堆叠 LSTM 单元

4
00:00:12,915 --> 00:00:17,970
这些 LSTM 单元构成了隐藏递归层 将迷你批次的数据传入这些递归层时

5
00:00:17,970 --> 00:00:20,130
它们将一次查看一个字符

6
00:00:20,130 --> 00:00:22,920
生成输出和隐藏状态

7
00:00:22,920 --> 00:00:25,500
我们将输入字符传入第一个 LSTM 单元

8
00:00:25,500 --> 00:00:28,575
该单元生成一个隐藏状态

9
00:00:28,575 --> 00:00:30,250
在下个时间步

10
00:00:30,250 --> 00:00:33,450
查看序列中的下个字符并将该字符传入此 LSTM 单元中

11
00:00:33,450 --> 00:00:37,495
同时将上个隐藏状态当做输入

12
00:00:37,495 --> 00:00:42,170
你已经在单层 RNN 中见过此行为

13
00:00:42,170 --> 00:00:46,520
但是在此例中 我们打算使用双层模型 将 LSTM 层级堆叠起来

14
00:00:46,520 --> 00:00:50,270
所以此 LSTM 层的输出将作为输入进入下个层级

15
00:00:50,270 --> 00:00:52,790
每个单元都会

16
00:00:52,790 --> 00:00:56,330
与展开层级中的下个单元分享隐藏状态

17
00:00:56,330 --> 00:00:59,930
最后一个 LSTM 层级的输出

18
00:00:59,930 --> 00:01:03,635
将包含一些字符类别分数 长度等于词汇表的长度

19
00:01:03,635 --> 00:01:07,730
对此输出应用 Softmax 激活函数

20
00:01:07,730 --> 00:01:11,660
并获得概率分布以预测下个最有可能的字符

21
00:01:11,660 --> 00:01:14,320
为了帮助你完成此任务

22
00:01:14,320 --> 00:01:17,015
我们提供了创建模型的框架代码

23
00:01:17,015 --> 00:01:19,640
首先 我们将检查是否有 GPU 可用于训练

24
00:01:19,640 --> 00:01:23,370
然后是这个 CharRNN 类

25
00:01:23,370 --> 00:01:26,270
这个类包含

26
00:01:26,270 --> 00:01:29,190
常见的 init 和 forward 函数

27
00:01:29,190 --> 00:01:31,910
我们提供了初始化 LSTM 层级隐藏状态的代码

28
00:01:31,910 --> 00:01:35,070
稍后我会讲解这些代码的

29
00:01:35,070 --> 00:01:38,240
你可以自己查看下这些代码

30
00:01:38,240 --> 00:01:41,770
看看我们是如何创建初始字符字典的 但是不需要更改代码

31
00:01:41,770 --> 00:01:45,170
在实例化 CharRNN 时我们会传入几个参数

32
00:01:45,170 --> 00:01:49,555
我将其中几个参数的值保存为类变量

33
00:01:49,555 --> 00:01:52,130
你需要使用这些输入参数和变量

34
00:01:52,130 --> 00:01:55,990
创建模型层级并完成 forward 函数

35
00:01:55,990 --> 00:02:00,420
首先需要创建一个 LSTM 层级 你可以点击此链接并阅读 LSTM 文档

36
00:02:00,420 --> 00:02:04,510
我们可以使用常见的参数创建 LSTM 层级

37
00:02:04,510 --> 00:02:06,330
这些参数包括 input_size、hidden_size

38
00:02:06,330 --> 00:02:08,930
num_layers 和 batch_first

39
00:02:08,930 --> 00:02:11,140
还需要添加 dropout 值

40
00:02:11,140 --> 00:02:14,210
如果你决定堆叠多个层级

41
00:02:14,210 --> 00:02:17,855
此值将在 LSTM 层级的输出之间引入丢弃层

42
00:02:17,855 --> 00:02:20,210
定义 LSTM 层级之后

43
00:02:20,210 --> 00:02:22,410
还需要定义两个层级

44
00:02:22,410 --> 00:02:27,320
一个是丢弃层 一个是最终全连接层 后者用于获取期望的输出大小

45
00:02:27,320 --> 00:02:28,740
定义好这些层级之后

46
00:02:28,740 --> 00:02:31,000
需要定义 forward 函数

47
00:02:31,000 --> 00:02:33,770
参数包括输入 x 和隐藏状态

48
00:02:33,770 --> 00:02:35,900
你需要使此输入经过模型层级

49
00:02:35,900 --> 00:02:38,975
并返回最终输出和隐藏状态

50
00:02:38,975 --> 00:02:41,540
确保变形 LSTM 输出

51
00:02:41,540 --> 00:02:44,350
使其能够传入最终全连接层

52
00:02:44,350 --> 00:02:45,940
Ok在底部

53
00:02:45,940 --> 00:02:49,720
是一个初始化 LSTM 隐藏状态的函数

54
00:02:49,720 --> 00:02:54,775
LSTM 有一个隐藏状态和单元状态 它们存储为元组 hidden

55
00:02:54,775 --> 00:02:57,290
隐藏状态和单元状态的形状由以下参数决定：

56
00:02:57,290 --> 00:02:59,840
模型的层级数量

57
00:02:59,840 --> 00:03:01,420
输入的批次大小

58
00:03:01,420 --> 00:03:04,730
以及在创建模型时指定的隐藏维度

59
00:03:04,730 --> 00:03:07,610
在此函数中 我们将隐藏状态全初始化为 0

60
00:03:07,610 --> 00:03:10,250
如果有 GPU 则移到 GPU 上

61
00:03:10,250 --> 00:03:13,290
你不需要更改看到的所有这些代码

62
00:03:13,290 --> 00:03:16,690
只需定义模型层级和前馈行为

63
00:03:16,690 --> 00:03:18,555
如果代码实现正确

64
00:03:18,555 --> 00:03:21,065
你应该能够设置模型超参数

65
00:03:21,065 --> 00:03:24,230
继续训练模型并生成一些示例文本

66
00:03:24,230 --> 00:03:26,360
自己尝试一下

67
00:03:26,360 --> 00:03:27,800
然后看看我的答案

