1
00:00:00,000 --> 00:00:02,730
So far, we've investigated how to train

2
00:00:02,730 --> 00:00:06,415
an MLP to classify handwritten digits in the MNIST dataset,

3
00:00:06,415 --> 00:00:08,765
and we got a really high test accuracy.

4
00:00:08,765 --> 00:00:11,405
MLPs are a nice solution in this case.

5
00:00:11,405 --> 00:00:14,970
Some other solutions for the MNIST classification task and

6
00:00:14,970 --> 00:00:19,000
their errors on the test dataset can be explored at the link provided below.

7
00:00:19,000 --> 00:00:21,650
You'll find that the best algorithms or the ones with

8
00:00:21,650 --> 00:00:26,795
the least test error are the approaches that use convolutional neural networks or CNNs.

9
00:00:26,795 --> 00:00:29,710
In fact, for most image classification tasks,

10
00:00:29,710 --> 00:00:32,615
CNNs and MLPs do not even compare,

11
00:00:32,615 --> 00:00:34,420
CNNs do much better.

12
00:00:34,420 --> 00:00:36,795
The MNIST database is an exception,

13
00:00:36,795 --> 00:00:39,190
and that it's very clean and pre-processed.

14
00:00:39,190 --> 00:00:45,245
All images of digits are roughly the same size and are centered in a 28 by 28 pixel grid.

15
00:00:45,245 --> 00:00:47,660
You can imagine that if instead of having to

16
00:00:47,660 --> 00:00:50,480
classify the digit within these very clean images,

17
00:00:50,480 --> 00:00:53,785
you had to work with images where the digit could appear anywhere within the grid,

18
00:00:53,785 --> 00:00:56,730
and sometimes appear quite small or quite large.

19
00:00:56,730 --> 00:00:59,830
It would be a more challenging task for an MLP.

20
00:00:59,830 --> 00:01:02,760
In the case of real-world messy image data,

21
00:01:02,760 --> 00:01:05,670
CNNs will truly shine over MLPs.

22
00:01:05,670 --> 00:01:08,270
For some intuition for why this might be the case,

23
00:01:08,270 --> 00:01:11,080
we saw that in order to feed an image to an MLP,

24
00:01:11,080 --> 00:01:13,665
you must first convert the image to a vector.

25
00:01:13,665 --> 00:01:16,340
The MLP then treats this converted image as

26
00:01:16,340 --> 00:01:19,045
a simple vector of numbers with no special structure.

27
00:01:19,045 --> 00:01:20,930
It has no knowledge of the fact that

28
00:01:20,930 --> 00:01:23,925
these numbers were originally spatially arranged in a grid.

29
00:01:23,925 --> 00:01:27,320
CNNs, in contrast, are built for the exact purpose

30
00:01:27,320 --> 00:01:31,175
of working with or elucidating the patterns in multidimensional data.

31
00:01:31,175 --> 00:01:35,750
Unlike MLPs, CNNs understand the fact that image pixels that are closer

32
00:01:35,750 --> 00:01:40,475
in proximity to each other are more heavily related than pixels that are far apart.

33
00:01:40,475 --> 00:01:44,970
In this section, Alexis and I will discuss the math behind this kind of understanding.

34
00:01:44,970 --> 00:01:47,850
We'll present different types of layers that make up a CNN,

35
00:01:47,850 --> 00:01:50,300
and provide some intuition about each of their roles in

36
00:01:50,300 --> 00:01:53,640
processing an input and forming a complete neural network.

