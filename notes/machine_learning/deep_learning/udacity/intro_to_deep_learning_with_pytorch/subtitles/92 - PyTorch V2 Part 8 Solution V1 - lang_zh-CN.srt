1
00:00:00,000 --> 00:00:04,530
现在说说上道练习的答案

2
00:00:04,530 --> 00:00:05,910
这次我将采用不同的讲解方式

3
00:00:05,910 --> 00:00:09,900
我将一边输入代码一边讲解

4
00:00:09,900 --> 00:00:14,565
实际上是将在这节课所学的所有知识结合到了一起

5
00:00:14,565 --> 00:00:16,785
首先

6
00:00:16,785 --> 00:00:18,900
如果我有 GPU

7
00:00:18,900 --> 00:00:23,895
我将先询问 GPU 设备是否可用

8
00:00:23,895 --> 00:00:30,255
输入 device = torch.device 在这里添加 cuda

9
00:00:30,255 --> 00:00:40,290
If torch.cuda.is_available() else “cpu”

10
00:00:40,290 --> 00:00:42,240
这行代码的作用是

11
00:00:42,240 --> 00:00:44,565
如果 GPU 可用

12
00:00:44,565 --> 00:00:50,330
这个将为 True 在这里返回 cuda 否则返回 CPU

13
00:00:50,330 --> 00:00:53,990
现在我们可以将 device 传递给所有张量和模型

14
00:00:53,990 --> 00:00:57,700
如果 GPU 可用 则自动使用 GPU

15
00:00:57,700 --> 00:01:02,490
接下来获取预训练的模型

16
00:01:02,490 --> 00:01:04,440
我将使用 ResNet

17
00:01:04,440 --> 00:01:07,290
输入 model = models

18
00:01:07,290 --> 00:01:09,140
我们已经从 torchvision 导入 models

19
00:01:09,140 --> 00:01:12,850
菜单中列出了所有模型

20
00:01:12,850 --> 00:01:14,565
ResNet 在这里

21
00:01:14,565 --> 00:01:16,545
我将使用很小的模型 选择 resnet50

22
00:01:16,545 --> 00:01:22,905
输入 pretrained=True 这样应该就能获得该模型

23
00:01:22,905 --> 00:01:24,630
好了

24
00:01:24,630 --> 00:01:27,650
像这样输出模型

25
00:01:27,650 --> 00:01:31,490
就会显示出所有不同的操作和层级等

26
00:01:31,490 --> 00:01:36,260
向下滚动到末尾 可以看到这个 fc

27
00:01:36,260 --> 00:01:38,300
表示最后一层

28
00:01:38,300 --> 00:01:40,840
是一个充当分类器的全连接层

29
00:01:40,840 --> 00:01:42,480
它要求这层输入为 2,048 个单元

30
00:01:42,480 --> 00:01:47,660
输出为 1,000 个单元

31
00:01:47,660 --> 00:01:51,830
这个模型是用 ImageNet 训练的

32
00:01:51,830 --> 00:01:56,610
ImageNet 用有 1,000 个不同类别的图像训练而成

33
00:01:56,610 --> 00:01:58,720
但是我们只使用猫和狗图像

34
00:01:58,720 --> 00:02:02,035
因此这个分类器只需 2 个输出单元

35
00:02:02,035 --> 00:02:06,410
我们可以加载这样的模型

36
00:02:06,410 --> 00:02:08,630
确保在训练模型时冻结模型参数

37
00:02:08,630 --> 00:02:11,615
这样就不会更新这些参数

38
00:02:11,615 --> 00:02:14,275
运行这部分代码 看看是否没有任何问题

39
00:02:14,275 --> 00:02:19,325
现在加载模型并关闭梯度

40
00:02:19,325 --> 00:02:21,980
关闭模型的梯度

41
00:02:21,980 --> 00:02:24,965
下一步是

42
00:02:24,965 --> 00:02:29,975
定义新分类器

43
00:02:29,975 --> 00:02:31,715
这一步可以很简单

44
00:02:31,715 --> 00:02:34,850
输入 models= nn.sequential

45
00:02:34,850 --> 00:02:36,440
用很多不同的定义方式可以用

46
00:02:36,440 --> 00:02:39,220
我使用 nn.sequential

47
00:02:39,220 --> 00:02:42,145
第一个层级是线性层级

48
00:02:42,145 --> 00:02:47,585
我们需要 248 个输入

49
00:02:47,585 --> 00:02:53,080
缩减到 512 个 添加 ReLu 层、丢弃层

50
00:02:53,080 --> 00:02:54,840
然后是输出层

51
00:02:54,840 --> 00:03:01,950
512 到 2 调用 LogSoftmax

52
00:03:01,950 --> 00:03:05,850
应该将这个改为 classifier.

53
00:03:05,850 --> 00:03:09,590
Ok.定义好 classifier 将其附加到模型上

54
00:03:09,590 --> 00:03:13,580
输入 model.fc= classifier

55
00:03:13,580 --> 00:03:16,639
再次查看模型

56
00:03:16,639 --> 00:03:19,270
向下滚动到底部

57
00:03:19,270 --> 00:03:23,960
可以看出现在这个全连接层是一个序列分类器 

58
00:03:23,960 --> 00:03:28,550
这里是线性操作 然后是 ReLu

59
00:03:28,550 --> 00:03:32,740
丢弃层 另一个线性转换 然后是 log softmax

60
00:03:32,740 --> 00:03:36,740
接下来定义损失/条件

61
00:03:36,740 --> 00:03:40,700
设为负对数似然损失

62
00:03:40,700 --> 00:03:47,935
然后定义优化器 等于 optim.Adam

63
00:03:47,935 --> 00:03:52,130
我们想使用分类器中的参数

64
00:03:52,130 --> 00:03:57,160
分类器是这个 fc 然后设置学习速率

65
00:03:57,160 --> 00:04:03,640
最后一步是将模型移到可用的设备上

66
00:04:03,640 --> 00:04:07,825
设置好模型后 现在开始训练模型

67
00:04:07,825 --> 00:04:10,459
首先我将定义一些

68
00:04:10,459 --> 00:04:13,430
将在训练过程中使用的变量

69
00:04:13,430 --> 00:04:16,580
例如我将周期设为 1

70
00:04:16,580 --> 00:04:19,130
跟踪训练步数

71
00:04:19,130 --> 00:04:20,555
设为 0

72
00:04:20,555 --> 00:04:22,645
跟踪损失

73
00:04:22,645 --> 00:04:26,215
也设为 0

74
00:04:26,215 --> 00:04:29,270
最后设置一个循环

75
00:04:29,270 --> 00:04:33,820
表示在输出验证损失之前训练多少步

76
00:04:33,820 --> 00:04:36,960
遍历周期

77
00:04:36,960 --> 00:04:41,730
for epoch in range(epochs)

78
00:04:41,730 --> 00:04:46,245
遍历图像数据

79
00:04:46,245 --> 00:04:52,005
for images, labels in trainloader

80
00:04:52,005 --> 00:04:54,500
递增步数 每次经过一个批次

81
00:04:54,500 --> 00:04:56,590
步数都递增

82
00:04:56,590 --> 00:04:58,850
现在已经获得图像和标签

83
00:04:58,850 --> 00:05:04,160
如果有 GPU 则将它们移到 GPU 上

84
00:05:04,160 --> 00:05:13,685
= images.to(device), labels.to(device)

85
00:05:13,685 --> 00:05:17,820
现在编写训练循环

86
00:05:17,820 --> 00:05:22,680
首先清零梯度

87
00:05:22,680 --> 00:05:25,170
这是很重要的步骤 一定不要忘记了

88
00:05:25,170 --> 00:05:28,730
然后从模型中获取对数概率

89
00:05:28,730 --> 00:05:33,170
传入 images 获得对数概率后

90
00:05:33,170 --> 00:05:37,520
我们可以从 criterion 获取损失 传入 labels

91
00:05:37,520 --> 00:05:42,665
然后执行反向传播 最后执行优化器步骤

92
00:05:42,665 --> 00:05:47,630
递增 running_loss

93
00:05:47,630 --> 00:05:50,030
这样的话 当我们用越来越多的数据训练模型后

94
00:05:50,030 --> 00:05:52,440
可以跟踪训练损失好了

95
00:05:52,440 --> 00:05:54,075
这就是训练循环

96
00:05:54,075 --> 00:05:58,730
我们希望每隔一段时间输出结果 即这个 print_every 变量

97
00:05:58,730 --> 00:06:02,685
现在 我们要退出训练循环

98
00:06:02,685 --> 00:06:07,400
使用测试数据集测试网络的准确率和损失

99
00:06:07,400 --> 00:06:12,395
输入 for step %% print_every == 0:

100
00:06:12,395 --> 00:06:15,860
如果等于 0 则进入验证循环

101
00:06:15,860 --> 00:06:18,480
首先调用 model.eval

102
00:06:18,480 --> 00:06:24,965
将模型变成评估推理模式 这样会关闭丢弃

103
00:06:24,965 --> 00:06:28,670
从而准确地使用网络做出预测

104
00:06:28,670 --> 00:06:34,260
设置测试损失和准确率

105
00:06:34,260 --> 00:06:40,310
从测试数据中获取图像和标签

106
00:06:40,310 --> 00:06:42,410
执行验证循环

107
00:06:42,410 --> 00:06:47,030
向模型中传入图像

108
00:06:47,030 --> 00:06:49,480
这些图像来自测试集

109
00:06:49,480 --> 00:06:53,835
从测试集获取 logps

110
00:06:53,835 --> 00:07:01,280
通过 criterion 获取损失 输入 test_loss += loss.item() 来跟踪损失

111
00:07:01,280 --> 00:07:04,280
这样 在经过这些验证循环时

112
00:07:04,280 --> 00:07:08,200
可以跟踪测试损失

113
00:07:08,200 --> 00:07:12,020
接下来计算准确率

114
00:07:12,020 --> 00:07:17,390
Ps = torch.exp(logps)

115
00:07:17,390 --> 00:07:20,790
模型返回的是 log softmax

116
00:07:20,790 --> 00:07:25,460
表示类别的对数概率

117
00:07:25,460 --> 00:07:27,380
要获取实际概率 我们将使用 torch.exp

118
00:07:27,380 --> 00:07:34,230
top_ps, top_class = ps.topk(1)

119
00:07:34,230 --> 00:07:39,950
获得最大概率

120
00:07:39,950 --> 00:07:43,490
将维度设为 1

121
00:07:43,490 --> 00:07:47,215
这样可以确保沿着列查找最高概率

122
00:07:47,215 --> 00:07:48,760
获得最高概率后

123
00:07:48,760 --> 00:07:56,300
检查是否和标签匹配

124
00:07:56,300 --> 00:07:58,280
根据 equality 张量更新 accuracy

125
00:07:58,280 --> 00:08:03,155
我们可以从 equality 计算 accuracy

126
00:08:03,155 --> 00:08:05,660
更改为浮点数张量后

127
00:08:05,660 --> 00:08:08,210
就可以运行torch.mean 并获得 accuracy

128
00:08:08,210 --> 00:08:13,320
递增这个 accuracy 变量好了

129
00:08:13,320 --> 00:08:18,305
现在我们位于这个 for 循环里 for step %% print_every

130
00:08:18,305 --> 00:08:23,375
获得训练损失 running_loss 和测试损失 test_loss

131
00:08:23,375 --> 00:08:26,240
将测试数据传入模型中

132
00:08:26,240 --> 00:08:29,140
并衡量损失和准确率

133
00:08:29,140 --> 00:08:30,855
现在输出所有这些结果

134
00:08:30,855 --> 00:08:33,580
直接复制粘贴这段代码 手动输入的话 代码太多了

135
00:08:33,580 --> 00:08:37,070
在这里输出 epochs

136
00:08:37,070 --> 00:08:40,475
不断跟踪进度

137
00:08:40,475 --> 00:08:43,610
用 running_loss 除以 print_every

138
00:08:43,610 --> 00:08:46,640
对训练损失取平均值

139
00:08:46,640 --> 00:08:47,855
每次输出时

140
00:08:47,855 --> 00:08:49,610
都会取平均值

141
00:08:49,610 --> 00:08:52,910
用 test_loss 除以 testloader 的长度

142
00:08:52,910 --> 00:08:56,630
len(testloader) 表示

143
00:08:56,630 --> 00:09:00,540
我们从 testloader 获得了多少批数据

144
00:09:00,540 --> 00:09:03,950
因为我们对每批的损失求和

145
00:09:03,950 --> 00:09:05,750
所以 如果用总损失除以批次数量

146
00:09:05,750 --> 00:09:07,910
就能得出平均损失

147
00:09:07,910 --> 00:09:09,815
准确率也一样

148
00:09:09,815 --> 00:09:13,820
对每个批次的准确率求和

149
00:09:13,820 --> 00:09:18,540
然后除以批次数量 得出测试集的平均准确率

150
00:09:18,540 --> 00:09:19,880
最后

151
00:09:19,880 --> 00:09:23,600
将 running_loss 设为 0

152
00:09:23,600 --> 00:09:29,095
并将模型设为训练模式

153
00:09:29,095 --> 00:09:34,475
好了这些就是训练代码 看看能否运行

154
00:09:34,475 --> 00:09:38,410
这里应该是 if 而不是 for

155
00:09:38,410 --> 00:09:41,945
这里我忘记将张量转移到 GPU 上了

156
00:09:41,945 --> 00:09:48,515
这种情况很常见

157
00:09:48,515 --> 00:09:51,095
希望能运行好的

158
00:09:51,095 --> 00:09:53,805
测试准确率超过了 95%

159
00:09:53,805 --> 00:09:57,820
甚至速度很快

160
00:09:57,820 --> 00:10:00,710
我们每隔五个周期都输出结果

161
00:10:00,710 --> 00:10:04,400
因此总共 15 个批次

162
00:10:04,400 --> 00:10:06,950
我们用 15 批数据训练了模型

163
00:10:06,950 --> 00:10:10,850
太好了 我们学习了如何轻松地调整这些分类器

164
00:10:10,850 --> 00:10:15,530
并在数据集上获得超过 95% 的准确率

