1
00:00:00,000 --> 00:00:03,540
So now, this is my solution for this exercise

2
00:00:03,540 --> 00:00:08,190
on calculating the output of this small simple neural network.

3
00:00:08,190 --> 00:00:12,330
So, remember that what we want to do is multiply our features by our weights,

4
00:00:12,330 --> 00:00:14,070
so features times weights.

5
00:00:14,070 --> 00:00:18,225
So these tensors, they work basically the same as NumPy arrays,

6
00:00:18,225 --> 00:00:19,845
if you've used NumPy before.

7
00:00:19,845 --> 00:00:22,770
So, when you multiply features times weights,

8
00:00:22,770 --> 00:00:25,320
it'll just take the first element from each one, multiply them together,

9
00:00:25,320 --> 00:00:27,420
take the second element and multiply them together and so

10
00:00:27,420 --> 00:00:29,895
on and give you back a new tensor,

11
00:00:29,895 --> 00:00:33,060
where there's element by element multiplication.

12
00:00:33,060 --> 00:00:37,020
So, from that we can do torch.sum to sum it all up into one value,

13
00:00:37,020 --> 00:00:41,100
add our bias term and then pass it through the activation function and then we get Y.

14
00:00:41,100 --> 00:00:44,720
So, we can also do this where we do features times weights again,

15
00:00:44,720 --> 00:00:46,295
and this creates another tensor,

16
00:00:46,295 --> 00:00:48,710
but tensors have a method.sum,

17
00:00:48,710 --> 00:00:53,645
where you just take a tensor do.sum and then it sums up all the values in that tensor.

18
00:00:53,645 --> 00:00:56,550
So, we can either do it this way or we do torch.sum,

19
00:00:56,550 --> 00:00:58,605
or we can just take this method,

20
00:00:58,605 --> 00:01:01,430
this sum method of a tensor and some upper values that way.

21
00:01:01,430 --> 00:01:04,645
Again, pass it through our our activation function.

22
00:01:04,645 --> 00:01:06,440
So, here what we're doing, we're doing

23
00:01:06,440 --> 00:01:11,080
this element wise multiplication and taking the sum in two separate operations.

24
00:01:11,080 --> 00:01:13,180
We're doing this multiplication and then we're doing the sum.

25
00:01:13,180 --> 00:01:17,420
But you can actually do this in the same operation using matrix multiplication.

26
00:01:17,420 --> 00:01:19,520
So, in general, you're going to be wanting to

27
00:01:19,520 --> 00:01:22,070
use matrix multiplications most of the time,

28
00:01:22,070 --> 00:01:24,365
since they're the more efficient and

29
00:01:24,365 --> 00:01:30,145
these linear algebra operations have been accelerated using modern libraries,

30
00:01:30,145 --> 00:01:32,985
such as CUDA that run on GPUs.

31
00:01:32,985 --> 00:01:37,550
To do matrix multiplication in PyTorch with our two tensors features and weights,

32
00:01:37,550 --> 00:01:40,310
we can use one of two methods.

33
00:01:40,310 --> 00:01:44,875
So, either torch.mm or torch.matmul.

34
00:01:44,875 --> 00:01:48,590
So, torch.mm, so matrix multiplication is more

35
00:01:48,590 --> 00:01:52,760
simple and more strict about the tensors that you pass in.

36
00:01:52,760 --> 00:01:56,420
So, torch.matmul, it actually supports broadcasting.

37
00:01:56,420 --> 00:02:02,029
So, if you put in tensors that have weird sizes,

38
00:02:02,029 --> 00:02:05,665
weird shapes, then you could get an output that you're not expecting.

39
00:02:05,665 --> 00:02:09,495
So, what I tend to use torch.mm more often,

40
00:02:09,495 --> 00:02:12,520
so that it does what I expect basically,

41
00:02:12,520 --> 00:02:15,710
and then if I get something wrong it's going throw an error instead of just

42
00:02:15,710 --> 00:02:19,085
doing it and continuing the calculations.

43
00:02:19,085 --> 00:02:21,665
So, however, if we actually try to use

44
00:02:21,665 --> 00:02:25,175
torch.mm with features and weights, we'll get an error.

45
00:02:25,175 --> 00:02:28,975
So, here we see RuntimeError, size mismatch.

46
00:02:28,975 --> 00:02:33,485
So, what this means is that we passed in our two tensors to torch.mm,

47
00:02:33,485 --> 00:02:35,810
but there's a mismatch in the sizes and it can't actually do

48
00:02:35,810 --> 00:02:39,815
the matrix multiplication and it lists out the sizes here.

49
00:02:39,815 --> 00:02:42,155
So, the first tensor,

50
00:02:42,155 --> 00:02:45,905
M1 is one by five and the second tensor is one by five also.

51
00:02:45,905 --> 00:02:51,360
So, if you remember from your linear algebra classes or if you studied it recently,

52
00:02:51,360 --> 00:02:53,425
when you're doing matrix multiplication,

53
00:02:53,425 --> 00:02:56,690
the first matrix has to have a number of

54
00:02:56,690 --> 00:03:00,520
columns that's equal to the number of rows in the second matrix.

55
00:03:00,520 --> 00:03:04,245
So, really what we need is we need our weights tensor,

56
00:03:04,245 --> 00:03:07,765
our weights matrix to be five by one instead of one by five.

57
00:03:07,765 --> 00:03:11,339
To checkout the shape of tensors,

58
00:03:11,339 --> 00:03:13,365
as you're building your networks,

59
00:03:13,365 --> 00:03:15,210
you want to use tensor.shape.

60
00:03:15,210 --> 00:03:18,500
So, this is something you're going to be using all the time in PyTorch,

61
00:03:18,500 --> 00:03:22,890
but also in TensorFlow and in other deep learning frameworks So,

62
00:03:22,890 --> 00:03:25,760
most of the errors you're going to see when you're building networks and

63
00:03:25,760 --> 00:03:28,910
just a lot of the difficulty when it comes to

64
00:03:28,910 --> 00:03:32,240
designing the architecture of neural networks is getting

65
00:03:32,240 --> 00:03:35,780
the shapes of your tensors to work right together.

66
00:03:35,780 --> 00:03:38,014
So, what that means is that a large part of debugging,

67
00:03:38,014 --> 00:03:39,980
you're actually going to be trying to look at

68
00:03:39,980 --> 00:03:42,635
the shape of your tensors as they're going through your network.

69
00:03:42,635 --> 00:03:45,170
So, remember this, tensor.shape.

70
00:03:45,170 --> 00:03:47,690
So, for reshaping tensors,

71
00:03:47,690 --> 00:03:49,790
there are three, in general,

72
00:03:49,790 --> 00:03:51,860
three different options to choose from.

73
00:03:51,860 --> 00:03:53,045
So, we have these methods;

74
00:03:53,045 --> 00:03:56,075
reshape, resize, and view.

75
00:03:56,075 --> 00:03:58,205
The way these all work, in general,

76
00:03:58,205 --> 00:04:01,105
is that you take your tensor

77
00:04:01,105 --> 00:04:04,805
weights.reshape and then pass in the new shape that you want.

78
00:04:04,805 --> 00:04:05,930
So, in this case,

79
00:04:05,930 --> 00:04:08,900
you want to change our weights to be a five by one matrix,

80
00:04:08,900 --> 00:04:11,630
so we'd say.reshape and then five comma one.

81
00:04:11,630 --> 00:04:15,320
So, reshape here, what it will do is it's going to

82
00:04:15,320 --> 00:04:18,750
return a new tensor with the same data as weights.

83
00:04:18,750 --> 00:04:22,595
So, the same data that's sitting in memory at those addresses in memory.

84
00:04:22,595 --> 00:04:24,275
So, it's going to basically just create

85
00:04:24,275 --> 00:04:27,775
a new tensor that has the shape that you requested,

86
00:04:27,775 --> 00:04:31,775
but the actual data in memory isn't being changed.

87
00:04:31,775 --> 00:04:33,740
But that's only sometimes.

88
00:04:33,740 --> 00:04:38,570
Sometimes it does return a clone and what that means is that it actually

89
00:04:38,570 --> 00:04:41,090
copies the data to another part of memory and then returns

90
00:04:41,090 --> 00:04:44,215
you a tensor on top of that part of the memory.

91
00:04:44,215 --> 00:04:47,050
As you can imagine when it actually does that,

92
00:04:47,050 --> 00:04:50,080
when it's copying the data that's less efficient than if you had

93
00:04:50,080 --> 00:04:54,080
just changed the shape of your tensor without cloning the data.

94
00:04:54,080 --> 00:04:55,260
To do something like that,

95
00:04:55,260 --> 00:04:58,050
we can use resize, where there's underscore at the end.

96
00:04:58,050 --> 00:05:02,884
The underscore means that this method is an in-place operation.

97
00:05:02,884 --> 00:05:04,235
So, when it's in-place,

98
00:05:04,235 --> 00:05:06,220
that basically means that you're just not touching the data

99
00:05:06,220 --> 00:05:08,140
at all and all you're doing is changing

100
00:05:08,140 --> 00:05:12,575
the tensor that's sitting on top of that addressed data in memory.

101
00:05:12,575 --> 00:05:16,435
The problem with the resize method is that if you request

102
00:05:16,435 --> 00:05:20,560
a shape that has more or less elements than the original tensor,

103
00:05:20,560 --> 00:05:23,470
then you can actually cut off,

104
00:05:23,470 --> 00:05:26,305
you can actually lose some of the data that you had or you can

105
00:05:26,305 --> 00:05:30,005
create this spurious data from uninitialized memory.

106
00:05:30,005 --> 00:05:32,510
So instead, what you typically want is

107
00:05:32,510 --> 00:05:35,780
that you want a method that's going to return an error

108
00:05:35,780 --> 00:05:38,030
if you changed the shape from

109
00:05:38,030 --> 00:05:41,555
the original number of elements to a different number of elements.

110
00:05:41,555 --> 00:05:43,645
So, we can actually do that with.view.

111
00:05:43,645 --> 00:05:46,870
So.view is the one that I use the most,

112
00:05:46,870 --> 00:05:49,535
and basically what it does it just returns a new tensor

113
00:05:49,535 --> 00:05:52,985
with the same data in memory as weights.

114
00:05:52,985 --> 00:05:55,345
This is just all the time, 100 percent of the time,

115
00:05:55,345 --> 00:05:57,620
all it's going to do is return a new tensor without

116
00:05:57,620 --> 00:06:00,155
messing with any of the data in memory.

117
00:06:00,155 --> 00:06:03,755
If you tried to get a new size,

118
00:06:03,755 --> 00:06:08,250
a new shape for your tensor with a different number of elements,

119
00:06:08,250 --> 00:06:09,885
it'll return an error.

120
00:06:09,885 --> 00:06:12,910
So, you are basically using.view,

121
00:06:12,910 --> 00:06:15,035
you're ensuring that you will always get

122
00:06:15,035 --> 00:06:17,905
the same number of elements when you change the shape of your weights.

123
00:06:17,905 --> 00:06:21,740
So, this is why I typically use when I'm reshaping tensors.

124
00:06:21,740 --> 00:06:23,555
So, with all that out of the way,

125
00:06:23,555 --> 00:06:27,110
if you want to reshape weights to have five rows and one column,

126
00:06:27,110 --> 00:06:30,420
then you can use something like weights.view (5, 1), right.

127
00:06:30,420 --> 00:06:33,155
So, now, that you have seen how you can change

128
00:06:33,155 --> 00:06:37,280
the shape of a tensor and also do matrix multiplication,

129
00:06:37,280 --> 00:06:39,830
so this time I want you to calculate the output of

130
00:06:39,830 --> 00:06:43,330
this little neural network using matrix multiplication.

