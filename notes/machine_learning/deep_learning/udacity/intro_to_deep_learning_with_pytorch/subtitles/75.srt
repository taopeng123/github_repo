1
00:00:00,000 --> 00:00:03,375
Welcome to my solution for this exercise.

2
00:00:03,375 --> 00:00:08,700
So, for here, I had you calculate the output of our network using matrix multiplication.

3
00:00:08,700 --> 00:00:12,060
So remember, we wanted to use matrix multiplication because it's more

4
00:00:12,060 --> 00:00:16,605
efficient than doing these two separate operations of the multiplication and the sum.

5
00:00:16,605 --> 00:00:18,300
But to do the matrix multiplication,

6
00:00:18,300 --> 00:00:21,405
we actually needed to change the size of our weights tensor.

7
00:00:21,405 --> 00:00:24,690
So, to do that, just do weights.view 5, 1,

8
00:00:24,690 --> 00:00:31,305
and so this will change the shape of our weights tensor to be five rows and one column.

9
00:00:31,305 --> 00:00:34,970
If you remember, our features has the shape of one row and five columns,

10
00:00:34,970 --> 00:00:37,145
so we can do this matrix multiplication.

11
00:00:37,145 --> 00:00:42,985
So, there's just one operation that does the multiplication and the sum and just one go,

12
00:00:42,985 --> 00:00:45,105
and then we again add our bias term,

13
00:00:45,105 --> 00:00:46,300
pass it through the activation,

14
00:00:46,300 --> 00:00:49,130
and we get our output.

15
00:00:49,130 --> 00:00:51,560
So, as I mentioned before, you could actually stack up

16
00:00:51,560 --> 00:00:55,310
these simple neural networks into a multi-layer neural network,

17
00:00:55,310 --> 00:00:57,680
and this basically gives

18
00:00:57,680 --> 00:01:02,255
your network greater power to capture patterns and correlations in your data.

19
00:01:02,255 --> 00:01:06,410
Now, instead of a simple vector for our weights,

20
00:01:06,410 --> 00:01:08,270
we actually need to use a matrix.

21
00:01:08,270 --> 00:01:13,745
So, in this case, we have our input vector and our input data x_1, x_2, x_3.

22
00:01:13,745 --> 00:01:16,790
You think of this as a vector of just x, which our features.

23
00:01:16,790 --> 00:01:23,695
Then we have weights that connect our input to one hidden unit in this middle layers,

24
00:01:23,695 --> 00:01:25,865
usually called the hidden layer, hidden units,

25
00:01:25,865 --> 00:01:29,135
and we have two units in this hidden layer.

26
00:01:29,135 --> 00:01:32,370
So then, if we have our features,

27
00:01:32,370 --> 00:01:34,370
our inputs as a row vector,

28
00:01:34,370 --> 00:01:37,040
if we multiply it by this first column,

29
00:01:37,040 --> 00:01:39,275
then we're going to get the output,

30
00:01:39,275 --> 00:01:41,485
we're going to get this value of h_1.

31
00:01:41,485 --> 00:01:44,840
Then if we take our features and multiply it by the second column,

32
00:01:44,840 --> 00:01:48,625
then we're going to get the value for h_2.

33
00:01:48,625 --> 00:01:53,240
So again, mathematically looking at this with matrices and vectors and linear algebra,

34
00:01:53,240 --> 00:01:56,930
we see that to get the values for this hidden layer that

35
00:01:56,930 --> 00:02:00,855
we do a matrix multiplication between our feature vector,

36
00:02:00,855 --> 00:02:02,505
this x_1 to x_n,

37
00:02:02,505 --> 00:02:04,605
and our weight matrix.

38
00:02:04,605 --> 00:02:06,780
Then as before with these values,

39
00:02:06,780 --> 00:02:08,150
we're going to pass them through

40
00:02:08,150 --> 00:02:11,690
some activation function or maybe not an activation function,

41
00:02:11,690 --> 00:02:14,955
maybe we just want the row output of our network.

42
00:02:14,955 --> 00:02:18,560
So here, I'm generating some random data, some features,

43
00:02:18,560 --> 00:02:22,430
and some random weight matrices and bias terms that you'll

44
00:02:22,430 --> 00:02:26,500
be using to calculate the output of a multi-layer network.

45
00:02:26,500 --> 00:02:30,820
So, what I've built is basically we have three input features,

46
00:02:30,820 --> 00:02:32,990
two hidden units and one output unit.

47
00:02:32,990 --> 00:02:35,000
So, you can see that I've listed it here.

48
00:02:35,000 --> 00:02:41,025
So, our features we're going to create three features and this features vector here,

49
00:02:41,025 --> 00:02:44,085
and then we have an input equals three,

50
00:02:44,085 --> 00:02:48,195
so the shape is this, and two hidden units, one output unit.

51
00:02:48,195 --> 00:02:53,260
So, these weight matrices are created using these values. All right.

52
00:02:53,260 --> 00:02:58,235
I'll leave it up to you to calculate the output for this multi-layer network.

53
00:02:58,235 --> 00:03:01,895
Again, feel free to use the activation function defined

54
00:03:01,895 --> 00:03:06,670
earlier for the output of your network and the hidden layer. Cheers.

