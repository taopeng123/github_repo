1
00:00:00,000 --> 00:00:02,490
In the code example that I'll go through,

2
00:00:02,490 --> 00:00:05,940
we'll recreate a style transfer method that's outlined in the paper,

3
00:00:05,940 --> 00:00:09,270
image style transfer using convolutional neural networks.

4
00:00:09,270 --> 00:00:14,325
In this paper, style transfer uses the features found in the 19 layer VGG network,

5
00:00:14,325 --> 00:00:16,200
which I'll call VGG 19.

6
00:00:16,200 --> 00:00:19,020
This network accepts a color image as input and

7
00:00:19,020 --> 00:00:22,135
passes it through a series of convolutional and pooling layers.

8
00:00:22,135 --> 00:00:26,750
Followed finally by three fully connected layers but classify the past in image.

9
00:00:26,750 --> 00:00:28,815
In-between the five pooling layers,

10
00:00:28,815 --> 00:00:31,735
there are stacks of two or four convolutional layers.

11
00:00:31,735 --> 00:00:34,909
The depth of these layers is standard within each stack,

12
00:00:34,909 --> 00:00:37,275
but increases after each pooling layer.

13
00:00:37,275 --> 00:00:40,405
Here they're named by stack and their order in the stack.

14
00:00:40,405 --> 00:00:43,100
Conv one, one, is the first convolutional layer

15
00:00:43,100 --> 00:00:45,780
that an image is passed through in the first stack.

16
00:00:45,780 --> 00:00:49,070
Conv two, one, is the first convolutional layer in the second stack.

17
00:00:49,070 --> 00:00:53,295
The deepest convolutional layer in the network is conv five, four.

18
00:00:53,295 --> 00:00:55,850
Now, we know that style transfer wants to create

19
00:00:55,850 --> 00:00:59,155
an image that has the content of one image and the style of another.

20
00:00:59,155 --> 00:01:00,560
To create this image,

21
00:01:00,560 --> 00:01:02,210
which I'll call our target image,

22
00:01:02,210 --> 00:01:07,480
it will first pass both the content and style images through this VGG 19 network.

23
00:01:07,480 --> 00:01:10,110
First, when the network sees the content image,

24
00:01:10,110 --> 00:01:12,740
it will go through the feed-forward process until it

25
00:01:12,740 --> 00:01:16,135
gets to a convolutional layer that is deep in the network.

26
00:01:16,135 --> 00:01:20,455
The output of this layer will be the content representation of the input image.

27
00:01:20,455 --> 00:01:22,520
Next, when it sees the style image,

28
00:01:22,520 --> 00:01:24,590
it will extract different features from

29
00:01:24,590 --> 00:01:27,485
multiple layers that represent the style of that image.

30
00:01:27,485 --> 00:01:30,050
Finally, it will use both the content and style

31
00:01:30,050 --> 00:01:33,585
representations to inform the creation of the target image.

32
00:01:33,585 --> 00:01:36,400
The challenge is how to create the target image.

33
00:01:36,400 --> 00:01:39,680
How can we take a target image which often starts as

34
00:01:39,680 --> 00:01:43,280
either a blank canvas or as a copy of our content image,

35
00:01:43,280 --> 00:01:47,090
and manipulate it so that it's content is close to that of our content image,

36
00:01:47,090 --> 00:01:49,775
and it's style is close to that of our style image?

37
00:01:49,775 --> 00:01:53,430
Let's start by discussing in the content.

38
00:01:53,430 --> 00:01:56,960
In the paper, the content representation for an image is taken as

39
00:01:56,960 --> 00:02:01,040
the output from the fourth convolutional stack, conv four, two.

40
00:02:01,040 --> 00:02:02,805
As we form our new target image,

41
00:02:02,805 --> 00:02:06,375
we'll compare it's content representation with that of our content image.

42
00:02:06,375 --> 00:02:08,780
These two representations should be close to the

43
00:02:08,780 --> 00:02:12,040
same even as our target image changes it's style.

44
00:02:12,040 --> 00:02:13,890
To formalize this comparison,

45
00:02:13,890 --> 00:02:15,665
we'll define a content loss,

46
00:02:15,665 --> 00:02:17,690
a loss that calculates the difference between

47
00:02:17,690 --> 00:02:20,200
the content and target image representations,

48
00:02:20,200 --> 00:02:23,550
which I'll call Cc and Tc respectively.

49
00:02:23,550 --> 00:02:28,130
In this case, we calculate the mean squared difference between the two representations.

50
00:02:28,130 --> 00:02:29,620
This is our content loss,

51
00:02:29,620 --> 00:02:33,660
and it measures how far away these two representations are from one another.

52
00:02:33,660 --> 00:02:36,185
As we try to create the best target image,

53
00:02:36,185 --> 00:02:38,590
our aim will be to minimize this loss.

54
00:02:38,590 --> 00:02:40,700
This is similar to how we used loss and

55
00:02:40,700 --> 00:02:43,915
optimization to determine the weights of a CNN during training.

56
00:02:43,915 --> 00:02:47,410
But this time, our aim is not to minimize classification error.

57
00:02:47,410 --> 00:02:50,200
In fact, we're not training the CNN at all.

58
00:02:50,200 --> 00:02:53,110
Rather, our goal is to change only the target image,

59
00:02:53,110 --> 00:02:55,190
updating it's appearance until it's content

60
00:02:55,190 --> 00:02:57,830
representation matches that of our content image.

61
00:02:57,830 --> 00:03:01,970
So, we're not using the VGG 19 network in a traditional sense,

62
00:03:01,970 --> 00:03:04,380
we're not training it to produce a specific output.

63
00:03:04,380 --> 00:03:06,850
But we are using it as a feature extractor,

64
00:03:06,850 --> 00:03:08,945
and using back propagation to minimize

65
00:03:08,945 --> 00:03:12,785
a defined loss function between our target and content images.

66
00:03:12,785 --> 00:03:17,055
In fact, we'll have to define a loss function between our target and style images,

67
00:03:17,055 --> 00:03:19,830
in order to produce an image with our desired style.

68
00:03:19,830 --> 00:03:23,200
Next, let's learn more about how to represent the style of an image.

