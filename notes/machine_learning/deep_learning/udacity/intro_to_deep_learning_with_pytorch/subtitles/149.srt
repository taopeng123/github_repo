1
00:00:00,000 --> 00:00:03,630
This is a notebook where you'll be building a characterwise RNN.

2
00:00:03,630 --> 00:00:06,420
You're going to train this on the text of Anna Karenina,

3
00:00:06,420 --> 00:00:09,345
which is a really great but also quite sad a book.

4
00:00:09,345 --> 00:00:11,220
The general idea behind this,

5
00:00:11,220 --> 00:00:13,380
is that we're going to be passing one character at

6
00:00:13,380 --> 00:00:15,870
a time into a recurrent neural network.

7
00:00:15,870 --> 00:00:18,090
We're going to do this for a whole bunch of text,

8
00:00:18,090 --> 00:00:19,595
and at the end what's going to happen,

9
00:00:19,595 --> 00:00:22,170
is that our network is going to be able to generate new text,

10
00:00:22,170 --> 00:00:23,750
one character at a time.

11
00:00:23,750 --> 00:00:25,475
This is the general structure.

12
00:00:25,475 --> 00:00:28,980
We have our input characters and we want to one-hot encode them.

13
00:00:28,980 --> 00:00:32,819
This one-hot vector, will be fed into a hidden recurrent layer,

14
00:00:32,819 --> 00:00:35,215
and then the hidden layer has two outputs.

15
00:00:35,215 --> 00:00:37,680
First, it produces some RNN output,

16
00:00:37,680 --> 00:00:39,015
and it produces a hidden state,

17
00:00:39,015 --> 00:00:41,000
which will continue to change and be fed to

18
00:00:41,000 --> 00:00:43,850
this hidden layer at the next time step in the sequence.

19
00:00:43,850 --> 00:00:46,785
We saw something similar in the last code example.

20
00:00:46,785 --> 00:00:49,880
So, our recurrent layer keeps track of our hidden state,

21
00:00:49,880 --> 00:00:53,625
and its output goes to a final fully connected output layer.

22
00:00:53,625 --> 00:00:55,070
Our linear output layer,

23
00:00:55,070 --> 00:00:57,320
will produce a series of character class scores.

24
00:00:57,320 --> 00:01:00,035
So, this output will be as long as our input vector,

25
00:01:00,035 --> 00:01:02,150
and we can apply a Softmax function to get

26
00:01:02,150 --> 00:01:05,855
a probability distribution for the most likely next character.

27
00:01:05,855 --> 00:01:11,190
So, this network is based off of Andrej Karpathy's post on RNNs, which you can find here.

28
00:01:11,190 --> 00:01:14,920
It's a really good post and so you can check out these links to read more about RNNs.

29
00:01:14,920 --> 00:01:19,945
[inaudible] notebook is broken into a small series of exercises that you can implement yourself.

30
00:01:19,945 --> 00:01:23,615
For each exercise, I'm also going to provide a solution to consult.

31
00:01:23,615 --> 00:01:25,910
I recommend that you open the exercise notebook in

32
00:01:25,910 --> 00:01:28,280
one window and watch videos in another.

33
00:01:28,280 --> 00:01:30,180
That way you can work alongside me.

34
00:01:30,180 --> 00:01:31,845
Okay, so first things first,

35
00:01:31,845 --> 00:01:34,420
I'm loading in and taking a look at our text data.

36
00:01:34,420 --> 00:01:35,920
Here, I'm loading in

37
00:01:35,920 --> 00:01:40,285
the Anna Karenina text file and I'm printing out the first 100 characters.

38
00:01:40,285 --> 00:01:43,230
The characters are everything from letters, to spaces,

39
00:01:43,230 --> 00:01:46,815
to newline characters, and we can see the classic first-line,

40
00:01:46,815 --> 00:01:48,465
"Happy families are all alike.

41
00:01:48,465 --> 00:01:51,345
Every unhappy family is unhappy in its own way."

42
00:01:51,345 --> 00:01:55,220
Then, I'll actually want to turn our text into numerical tokens.

43
00:01:55,220 --> 00:01:58,280
This is because our network can only learn from numerical data,

44
00:01:58,280 --> 00:02:02,060
and so we want to map every character in the text to a unique index.

45
00:02:02,060 --> 00:02:04,395
So, first off, with the text,

46
00:02:04,395 --> 00:02:07,410
we can just create a unique vocabulary as a set.

47
00:02:07,410 --> 00:02:09,965
Sets, are a built in python data structure,

48
00:02:09,965 --> 00:02:13,385
and what this will do, is look at every character in the past in the text.

49
00:02:13,385 --> 00:02:16,739
Separate it out as a string and get rid of any duplicates.

50
00:02:16,739 --> 00:02:20,325
So, chars, is going to be a set of all our unique characters.

51
00:02:20,325 --> 00:02:23,655
This is also sometimes referred to as a vocabulary.

52
00:02:23,655 --> 00:02:27,980
Then, I'm creating a dictionary from a vocabulary of all our characters,

53
00:02:27,980 --> 00:02:31,430
that maps the actual character to a unique integer.

54
00:02:31,430 --> 00:02:35,660
So, it's just giving a numerical value to each of our unique characters,

55
00:02:35,660 --> 00:02:38,210
and putting it in a dictionary int2char.

56
00:02:38,210 --> 00:02:39,920
Then I'm doing this the other way,

57
00:02:39,920 --> 00:02:43,429
where we have a dictionary that goes from integers to characters.

58
00:02:43,429 --> 00:02:47,270
Recall that any dictionary is made of a set of key and value pairs.

59
00:02:47,270 --> 00:02:48,950
In the int2char case,

60
00:02:48,950 --> 00:02:53,070
the keys are going to be integers and the values are going to be string characters.

61
00:02:53,070 --> 00:02:54,740
In the char2int case,

62
00:02:54,740 --> 00:02:56,780
our keys are going to be the characters and

63
00:02:56,780 --> 00:02:59,430
our values are going to be their unique integers.

64
00:02:59,430 --> 00:03:03,265
So, these basically give us a way to encode text as numbers.

65
00:03:03,265 --> 00:03:05,155
Here, I am doing just that.

66
00:03:05,155 --> 00:03:08,855
I'm encoding each character in the text as an integer.

67
00:03:08,855 --> 00:03:11,045
This creates an encoded text,

68
00:03:11,045 --> 00:03:14,000
and just like I printed the first 100 characters before,

69
00:03:14,000 --> 00:03:16,670
I can print the first 100 encoded values.

70
00:03:16,670 --> 00:03:19,099
If you look at the length of our unique characters,

71
00:03:19,099 --> 00:03:21,850
you'll see that we have 83 unique characters in the text.

72
00:03:21,850 --> 00:03:24,500
So, our encoded values will fall in this range.

73
00:03:24,500 --> 00:03:27,765
You can also see some repeating values here like 82,

74
00:03:27,765 --> 00:03:30,800
82, 82 and 19,19.

75
00:03:30,800 --> 00:03:32,675
If we scroll back up to our actual text,

76
00:03:32,675 --> 00:03:36,590
we can surmise that the repeated 82s are probably this new line character,

77
00:03:36,590 --> 00:03:39,110
and 19 is maybe a p. Okay,

78
00:03:39,110 --> 00:03:40,405
so are encodings are working,

79
00:03:40,405 --> 00:03:41,710
and now what we want to do,

80
00:03:41,710 --> 00:03:44,105
is turn these encodings into one-hot vectors,

81
00:03:44,105 --> 00:03:46,060
that our RNN can take in as input,

82
00:03:46,060 --> 00:03:47,950
just like in our initial diagram.

83
00:03:47,950 --> 00:03:51,770
Here, I've actually written a function that takes in an encoded array,

84
00:03:51,770 --> 00:03:54,710
and turns it into a one-hot vector of some specified length.

85
00:03:54,710 --> 00:03:58,095
I can show you what this does with an example below.

86
00:03:58,095 --> 00:04:00,930
I've made a short test sequence three, five,

87
00:04:00,930 --> 00:04:03,975
one and a vector length that I specify, eight.

88
00:04:03,975 --> 00:04:06,530
So, I'm passing this test sequence and the number of

89
00:04:06,530 --> 00:04:09,765
labels that I expect into our one-hot function.

90
00:04:09,765 --> 00:04:14,085
I can see that the result is an array of three one-hot vectors.

91
00:04:14,085 --> 00:04:17,440
All of these vectors are of length eight and the index three,

92
00:04:17,440 --> 00:04:20,760
five, and one are on for their respective encodings.

93
00:04:20,760 --> 00:04:23,480
Now, for our vocabulary of 83 characters,

94
00:04:23,480 --> 00:04:25,745
these are just going to be much longer vectors.

95
00:04:25,745 --> 00:04:30,260
Cool. So, we have our preprocessing functions and data in place,

96
00:04:30,260 --> 00:04:33,380
and now your first task will be to take our encoded characters,

97
00:04:33,380 --> 00:04:36,975
and actually turn them into mini batches that we can feed into our network.

98
00:04:36,975 --> 00:04:38,910
So, as Matt mentioned before,

99
00:04:38,910 --> 00:04:40,880
the idea is that we actually want to run

100
00:04:40,880 --> 00:04:43,430
multiple sequences through our network at a time.

101
00:04:43,430 --> 00:04:47,110
Where one mini batch of data contains multiple sequences.

102
00:04:47,110 --> 00:04:49,695
So, here's an example starting sequence.

103
00:04:49,695 --> 00:04:51,620
If we say we want a batch size of two,

104
00:04:51,620 --> 00:04:54,405
we're going to split this data into two batches.

105
00:04:54,405 --> 00:04:57,110
Then, we'll have these sequence length windows that

106
00:04:57,110 --> 00:04:59,555
specify how big we want our sequences to be.

107
00:04:59,555 --> 00:05:01,880
In this case, we have a sequence length of three,

108
00:05:01,880 --> 00:05:03,900
and so our window will be three in width.

109
00:05:03,900 --> 00:05:06,320
For a batch size of two and sequence length of

110
00:05:06,320 --> 00:05:09,370
three these values will make up our first mini-batch.

111
00:05:09,370 --> 00:05:12,950
We'll just slide this window over by three to get the next mini-batch.

112
00:05:12,950 --> 00:05:17,370
So, each mini-batch is going to have the dimensions batch size by sequence length.

113
00:05:17,370 --> 00:05:20,120
In this case, we have a two by three window

114
00:05:20,120 --> 00:05:22,840
on are encoded array that we pass into our network.

115
00:05:22,840 --> 00:05:26,345
If you scroll down, I have more specific instructions.

116
00:05:26,345 --> 00:05:30,355
The first thing you're going to be doing is taking in an encoded array,

117
00:05:30,355 --> 00:05:35,320
and you'll want to discard any values that don't fit into completely full mini-batches.

118
00:05:35,320 --> 00:05:39,605
Then, you want to reshape this array into batch size number of rows.

119
00:05:39,605 --> 00:05:41,630
Finally, once you have that batch data,

120
00:05:41,630 --> 00:05:43,655
you're going to want to create a window that iterates

121
00:05:43,655 --> 00:05:45,920
over the batches a sequence length at a time,

122
00:05:45,920 --> 00:05:47,410
to get your mini batches.

123
00:05:47,410 --> 00:05:49,080
So, here's the skeleton code.

124
00:05:49,080 --> 00:05:51,590
Your array is going to be some encoded data,

125
00:05:51,590 --> 00:05:54,385
then you have a batch size and sequence length.

126
00:05:54,385 --> 00:05:57,590
Basically, you want to create an input x that should be

127
00:05:57,590 --> 00:06:01,805
a sequence length or number of timesteps wide and a batch size tall.

128
00:06:01,805 --> 00:06:05,870
This will make up our input data and you'll also want to provide targets.

129
00:06:05,870 --> 00:06:10,695
The targets y, for this network are going to be just like the input characters x,

130
00:06:10,695 --> 00:06:12,550
only shifted over by one.

131
00:06:12,550 --> 00:06:14,750
That's because we want our network to predict

132
00:06:14,750 --> 00:06:17,965
the most likely next character more some input sequence.

133
00:06:17,965 --> 00:06:22,550
So, you'll have your input sequence x and our targets y shifted over by one.

134
00:06:22,550 --> 00:06:24,955
Then finally, when we [inaudible] batches,

135
00:06:24,955 --> 00:06:27,110
we're going to create a generator that iterates through

136
00:06:27,110 --> 00:06:30,635
our array and returns x and y with this yield command.

137
00:06:30,635 --> 00:06:33,825
Okay, I'll leave implementing this batching function up to you.

138
00:06:33,825 --> 00:06:37,050
You can find more information about how you could do this in the notebook.

139
00:06:37,050 --> 00:06:39,925
There's some code for testing out your implementation below.

140
00:06:39,925 --> 00:06:43,570
In fact, this is what your batches should look like when you run this code.

141
00:06:43,570 --> 00:06:46,460
If you need any help or you just want to see my solution,

142
00:06:46,460 --> 00:06:49,470
go ahead and check out the solution video next.

