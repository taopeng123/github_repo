1
00:00:00,000 --> 00:00:01,995
Hello, everyone, and welcome back.

2
00:00:01,995 --> 00:00:05,235
So, in this video and in this notebook,

3
00:00:05,235 --> 00:00:09,195
I'll be showing you how to actually train neural networks in PyTorch.

4
00:00:09,195 --> 00:00:15,915
So, previously, we saw how to define neural networks in PyTorch using the nn module,

5
00:00:15,915 --> 00:00:18,270
but now we're going to see how we actually take

6
00:00:18,270 --> 00:00:20,610
one of these networks that we defined and train it.

7
00:00:20,610 --> 00:00:22,800
So, what I mean by training is that we're going to use

8
00:00:22,800 --> 00:00:26,415
our neural networks as a universal function approximator.

9
00:00:26,415 --> 00:00:28,275
What that means is that,

10
00:00:28,275 --> 00:00:30,375
for basically any function,

11
00:00:30,375 --> 00:00:33,440
we have some desired input for example,

12
00:00:33,440 --> 00:00:35,840
an image of the number four,

13
00:00:35,840 --> 00:00:38,450
and then we have some desired output of this function.

14
00:00:38,450 --> 00:00:40,970
In this case a probability distribution that

15
00:00:40,970 --> 00:00:43,865
is telling us the probabilities of the various digits.

16
00:00:43,865 --> 00:00:45,185
So, in this case,

17
00:00:45,185 --> 00:00:46,610
if we passed it in image four,

18
00:00:46,610 --> 00:00:49,250
we want to get out a probability distribution where there's

19
00:00:49,250 --> 00:00:52,240
a lot of probability in the digit four.

20
00:00:52,240 --> 00:00:54,590
So, the cool thing about neural networks is that if

21
00:00:54,590 --> 00:00:56,885
you use non-linear activations and then

22
00:00:56,885 --> 00:01:02,180
you have the correct dataset of these images that are labeled with the correct ones,

23
00:01:02,180 --> 00:01:06,555
then basically you pass in an image and the correct output,

24
00:01:06,555 --> 00:01:08,665
the correct label or class,

25
00:01:08,665 --> 00:01:13,220
and eventually your neural network will build to approximate this function that is

26
00:01:13,220 --> 00:01:17,970
converting these images into this probability distribution, and that's our goal here.

27
00:01:17,970 --> 00:01:21,505
So, basically we want to see how in PyTorch,

28
00:01:21,505 --> 00:01:26,240
we can build a neural network and then we're going to give it the inputs and outputs,

29
00:01:26,240 --> 00:01:31,315
and then adjust the weights of that network so that it approximates this function.

30
00:01:31,315 --> 00:01:34,940
So, the first thing that we need for that is what is called a loss function.

31
00:01:34,940 --> 00:01:37,070
So, it's sometimes also called the cost,

32
00:01:37,070 --> 00:01:40,130
and what this is it's a measure of our prediction error.

33
00:01:40,130 --> 00:01:42,995
So, we pass in the image of a four and then

34
00:01:42,995 --> 00:01:46,510
our network predicts something else that's an error.

35
00:01:46,510 --> 00:01:52,135
So, we want to measure how far away our networks prediction is from the correct label,

36
00:01:52,135 --> 00:01:54,195
and we do that using loss function.

37
00:01:54,195 --> 00:01:56,955
So, in this case, it's the mean squared error.

38
00:01:56,955 --> 00:01:59,960
So, a lot of times you'll use this in regression problems,

39
00:01:59,960 --> 00:02:04,430
but use other loss functions and classification problems like this one here.

40
00:02:04,430 --> 00:02:07,130
So, the loss depends on

41
00:02:07,130 --> 00:02:11,060
the output of our network or the predictions our network is making.

42
00:02:11,060 --> 00:02:14,080
The output of a network depends on the weight.

43
00:02:14,080 --> 00:02:15,785
So, like the network parameters.

44
00:02:15,785 --> 00:02:20,784
So, we can actually adjust our weights such that this loss is minimized,

45
00:02:20,784 --> 00:02:22,755
and once the loss is minimized,

46
00:02:22,755 --> 00:02:26,730
then we know that our network is making as good predictions as it can.

47
00:02:26,730 --> 00:02:33,140
So, this is the whole goal to adjust our network parameters to minimize our loss,

48
00:02:33,140 --> 00:02:36,350
and we do this by using a process called gradient descent.

49
00:02:36,350 --> 00:02:42,040
So, the gradient is the slope of the loss function with respect to our perimeters.

50
00:02:42,040 --> 00:02:45,380
The gradient always points in the direction of fastest change.

51
00:02:45,380 --> 00:02:47,180
So, for example if you have a mountain,

52
00:02:47,180 --> 00:02:49,610
the gradient is going to always point up the mountain.

53
00:02:49,610 --> 00:02:52,820
So, you can imagine our loss function being like

54
00:02:52,820 --> 00:02:56,360
this mountain where we have a high loss up here and we have a low loss down here.

55
00:02:56,360 --> 00:03:01,415
So, we know that we want to get to the minimum of our loss when we minimize our loss,

56
00:03:01,415 --> 00:03:02,825
and so, we want to go downwards.

57
00:03:02,825 --> 00:03:05,480
So, basically, the gradient points upwards and so,

58
00:03:05,480 --> 00:03:07,100
we just go the opposite direction.

59
00:03:07,100 --> 00:03:09,335
So, we go in the direction of the negative gradient,

60
00:03:09,335 --> 00:03:11,540
and then if we keep following this down,

61
00:03:11,540 --> 00:03:17,085
then eventually we get to the bottom of this mountain, the lowest loss.

62
00:03:17,085 --> 00:03:18,680
With multilayered neural networks,

63
00:03:18,680 --> 00:03:21,395
we use an algorithm called backpropagation to do this.

64
00:03:21,395 --> 00:03:26,015
Backpropagation is really just an application of the chain rule from calculus.

65
00:03:26,015 --> 00:03:29,835
So, if you think about it when we pass in some data,

66
00:03:29,835 --> 00:03:31,465
some input into our network,

67
00:03:31,465 --> 00:03:35,540
it goes through this forward pass through the network to calculate our loss.

68
00:03:35,540 --> 00:03:37,325
So, we pass in some data,

69
00:03:37,325 --> 00:03:39,710
some feature input x and then it goes through

70
00:03:39,710 --> 00:03:43,475
this linear transformation which depends on our weights and biases,

71
00:03:43,475 --> 00:03:46,490
and then through some activation function like a sigmoid,

72
00:03:46,490 --> 00:03:49,820
through another linear transformation with some more weights and biases,

73
00:03:49,820 --> 00:03:50,930
and then that goes in,

74
00:03:50,930 --> 00:03:52,430
from that we can calculate our loss.

75
00:03:52,430 --> 00:03:55,985
So, if we make a small change in our weights here, W1,

76
00:03:55,985 --> 00:03:58,910
it's going to propagate through the network and

77
00:03:58,910 --> 00:04:02,645
end up like results in a small change in our loss.

78
00:04:02,645 --> 00:04:05,330
So, you can think of this as a chain of changes.

79
00:04:05,330 --> 00:04:07,850
So, if we change here, this is going to change.

80
00:04:07,850 --> 00:04:09,740
Even that's going to propagate through here,

81
00:04:09,740 --> 00:04:12,130
it's going to propagate through here, it's going to propagate through here.

82
00:04:12,130 --> 00:04:15,860
So, with backpropagation, we actually use these same changes,

83
00:04:15,860 --> 00:04:17,830
but we go in the opposite direction.

84
00:04:17,830 --> 00:04:20,660
So, for each of these operations like the loss and

85
00:04:20,660 --> 00:04:23,600
the linear transformation into the sigmoid activation function,

86
00:04:23,600 --> 00:04:25,940
there's always going to be some derivative,

87
00:04:25,940 --> 00:04:29,450
some gradient between the outputs and inputs, and so,

88
00:04:29,450 --> 00:04:32,150
what we do is we take each of the gradients for

89
00:04:32,150 --> 00:04:35,930
these operations and we pass them backwards through the network.

90
00:04:35,930 --> 00:04:42,040
Each step we multiply the incoming gradient with the gradient of the operation itself.

91
00:04:42,040 --> 00:04:45,650
So, for example just starting at the end with the loss.

92
00:04:45,650 --> 00:04:48,740
So, we pass this gradient or the loss dldL2.

93
00:04:48,740 --> 00:04:53,510
So, this is the gradient of the loss with respect to the second linear transformation,

94
00:04:53,510 --> 00:05:03,335
and then we pass that backwards again and if we multiply it by the loss of this L2.

95
00:05:03,335 --> 00:05:06,170
So, this is the linear transformation with respect to

96
00:05:06,170 --> 00:05:08,840
the outputs of our activation function,

97
00:05:08,840 --> 00:05:10,975
that gives us the gradient for this operation.

98
00:05:10,975 --> 00:05:16,625
If you multiply this gradient by the gradient coming from the loss,

99
00:05:16,625 --> 00:05:18,600
then we get the total gradient for both of these parts,

100
00:05:18,600 --> 00:05:21,865
and this gradient can be passed back to this softmax function.

101
00:05:21,865 --> 00:05:25,845
So, as the general process for backpropagation, we take our gradients,

102
00:05:25,845 --> 00:05:28,745
we pass it backwards to the previous operation,

103
00:05:28,745 --> 00:05:30,110
multiply it by the gradient there,

104
00:05:30,110 --> 00:05:31,880
and then pass that total gradient backwards.

105
00:05:31,880 --> 00:05:35,630
So, we just keep doing that through each of the operations in our network,

106
00:05:35,630 --> 00:05:38,165
and eventually we'll get back to our weights.

107
00:05:38,165 --> 00:05:40,985
What this does is it allows us to calculate

108
00:05:40,985 --> 00:05:44,480
the gradient of the loss with respect to these weights.

109
00:05:44,480 --> 00:05:46,160
Like I was saying before,

110
00:05:46,160 --> 00:05:52,590
the gradient points in the direction of fastest change in our loss,

111
00:05:52,590 --> 00:05:53,895
so, to maximize it.

112
00:05:53,895 --> 00:05:55,485
So, if we want to minimize our loss,

113
00:05:55,485 --> 00:05:59,820
we can subtract the gradient off from our weights,

114
00:05:59,820 --> 00:06:03,110
and so, what this will do is it'll give us a new set of weights

115
00:06:03,110 --> 00:06:06,965
that will in general result in a smaller loss.

116
00:06:06,965 --> 00:06:09,770
So, the way that backpropagation algorithm works is that it will

117
00:06:09,770 --> 00:06:12,925
make a forward pass through a network, calculate the loss,

118
00:06:12,925 --> 00:06:14,675
and then once we have the loss, we can go

119
00:06:14,675 --> 00:06:17,030
backwards through our network and calculate the gradient,

120
00:06:17,030 --> 00:06:19,175
and get the gradient for a weights.

121
00:06:19,175 --> 00:06:20,485
Then we'll update the weights.

122
00:06:20,485 --> 00:06:21,660
Do another forward pass,

123
00:06:21,660 --> 00:06:24,495
calculate the loss, do another backward pass, update the weights,

124
00:06:24,495 --> 00:06:25,935
and so on and so on and so on,

125
00:06:25,935 --> 00:06:29,520
until we get sufficiently minimized loss.

126
00:06:29,520 --> 00:06:32,150
So, once we have the gradient and like I was saying before,

127
00:06:32,150 --> 00:06:34,220
we can subtract it off from our weights,

128
00:06:34,220 --> 00:06:38,770
but we also use this term Alpha which is called the learning rate.

129
00:06:38,770 --> 00:06:42,110
This is basically just a way to scale our gradients so that we're not

130
00:06:42,110 --> 00:06:45,864
taking too large steps in this iterative process.

131
00:06:45,864 --> 00:06:48,840
So, what can happen if you're update steps are too large,

132
00:06:48,840 --> 00:06:51,530
you can bounce around in this trough around

133
00:06:51,530 --> 00:06:55,250
the minimum and never actually settle in the minimum of the loss.

134
00:06:55,250 --> 00:07:00,010
So, let's see how we can actually calculate losses in PyTorch.

135
00:07:00,010 --> 00:07:02,280
Again using the nn module,

136
00:07:02,280 --> 00:07:06,905
PyTorch provides us a lot of different losses including the cross-entropy loss.

137
00:07:06,905 --> 00:07:12,265
So, this loss is what we'll typically use when we're doing classification problems.

138
00:07:12,265 --> 00:07:17,480
In PyTorch, the convention is to assign our loss to its variable criterion.

139
00:07:17,480 --> 00:07:19,805
So, if we wanted to use cross-entropy,

140
00:07:19,805 --> 00:07:25,060
we just say criterion equals nn.crossEntropyLoss and create that class.

141
00:07:25,060 --> 00:07:26,690
So, one thing to note is that,

142
00:07:26,690 --> 00:07:30,485
if you look at the documentation for cross-entropy loss,

143
00:07:30,485 --> 00:07:34,730
you'll see that it actually wants the scores

144
00:07:34,730 --> 00:07:39,060
like the logits of our network as the input to the cross-entropy loss.

145
00:07:39,060 --> 00:07:43,310
So, you'll be using this with an output such as softmax,

146
00:07:43,310 --> 00:07:46,405
which gives us this nice probability distribution.

147
00:07:46,405 --> 00:07:48,975
But for computational reasons,

148
00:07:48,975 --> 00:07:51,680
then it's generally better to use the logits which are

149
00:07:51,680 --> 00:07:55,705
the input to the softmax function as the input to this loss.

150
00:07:55,705 --> 00:07:58,835
So, the input is expected to be the scores

151
00:07:58,835 --> 00:08:02,240
for each class and not the probabilities themselves.

152
00:08:02,240 --> 00:08:06,500
So, first I'm going to import the necessary modules

153
00:08:06,500 --> 00:08:11,370
here and also download our data and create it in,

154
00:08:11,370 --> 00:08:13,410
like you've seen before, as a trainloader,

155
00:08:13,410 --> 00:08:15,110
and so, we can get our data out of here.

156
00:08:15,110 --> 00:08:17,165
So, here I'm defining a model.

157
00:08:17,165 --> 00:08:19,910
So, I'm using nn.Sequential, and if you haven't seen this,

158
00:08:19,910 --> 00:08:21,860
checkout the end of the previous notebook.

159
00:08:21,860 --> 00:08:23,140
So, the end of part two,

160
00:08:23,140 --> 00:08:25,845
will show you how to use nn.Sequential.

161
00:08:25,845 --> 00:08:31,300
It's just a somewhat more concise way to define simple feed-forward networks, and so,

162
00:08:31,300 --> 00:08:34,730
you'll notice here that I'm actually only returning the logits,

163
00:08:34,730 --> 00:08:41,250
the scores of our output function and not the softmax output itself.

164
00:08:41,250 --> 00:08:42,780
Then here we can define our loss.

165
00:08:42,780 --> 00:08:45,900
So, criterions equal to nn.crossEntropyLoss.

166
00:08:45,900 --> 00:08:48,140
We get our data with images and labels,

167
00:08:48,140 --> 00:08:51,710
flatten it, pass it through our model to get the logits,

168
00:08:51,710 --> 00:08:57,995
and then we can get the actual loss by bypassing in our logits and the true labels,

169
00:08:57,995 --> 00:09:00,710
and so, again we get the labels from our trainloader.

170
00:09:00,710 --> 00:09:03,920
So, if we do this, we see we have calculated the loss.

171
00:09:03,920 --> 00:09:06,995
So, my experience, it's more convenient to build your model

172
00:09:06,995 --> 00:09:10,975
using a log-softmax output instead of just normal softmax.

173
00:09:10,975 --> 00:09:13,860
So, with a log-softmax output to get the actual probabilities,

174
00:09:13,860 --> 00:09:17,250
you just pass it through torch.exp. So, the exponential.

175
00:09:17,250 --> 00:09:19,280
With a log-softmax output,

176
00:09:19,280 --> 00:09:24,340
you'll want to use the negative log-likelihood loss or nn.NLLLoss.

177
00:09:24,340 --> 00:09:26,780
So, what I want you to do here is build

178
00:09:26,780 --> 00:09:29,900
a model that returns the log-softmax as the output,

179
00:09:29,900 --> 00:09:33,980
and calculate the loss using the negative log-likelihood loss.

180
00:09:33,980 --> 00:09:36,095
When you're using log-softmax,

181
00:09:36,095 --> 00:09:40,040
make sure you pay attention to the dim keyword argument.

182
00:09:40,040 --> 00:09:45,200
You want to make sure you set it right so that the output is what you want.

183
00:09:45,200 --> 00:09:49,550
So, go and try this and feel free to check out my solution.

184
00:09:49,550 --> 00:09:51,455
It's in the notebook and also in the next video,

185
00:09:51,455 --> 00:09:53,730
if you're having problems. Cheers.

