1
00:00:00,000 --> 00:00:02,970
So far, we've checked how well our models are

2
00:00:02,970 --> 00:00:06,195
performing based on how the loss changes over each epoch.

3
00:00:06,195 --> 00:00:10,065
But the exact number of epochs to train for is often hard to determine.

4
00:00:10,065 --> 00:00:12,570
How many epochs should you train for so that your network

5
00:00:12,570 --> 00:00:15,680
is accurate but it's not overfitting the training data?

6
00:00:15,680 --> 00:00:19,310
One method that's used in practice involves breaking the dataset into

7
00:00:19,310 --> 00:00:23,310
three sets called training validation and test sets.

8
00:00:23,310 --> 00:00:25,670
Each is treated separately by the model.

9
00:00:25,670 --> 00:00:28,200
The model looks only at the training set when it's

10
00:00:28,200 --> 00:00:31,200
actively training and deciding how to modify its weights.

11
00:00:31,200 --> 00:00:32,740
After every training epoch,

12
00:00:32,740 --> 00:00:34,980
we check how the model is doing by looking at

13
00:00:34,980 --> 00:00:37,665
the training loss and the loss on the validation set.

14
00:00:37,665 --> 00:00:40,575
But it's important to note that the model does not use

15
00:00:40,575 --> 00:00:44,215
any part of the validation set for the back propagation step.

16
00:00:44,215 --> 00:00:47,150
We use this training set to find all the patterns we can,

17
00:00:47,150 --> 00:00:49,500
and to update and determine the weights of our model.

18
00:00:49,500 --> 00:00:54,330
The validation set only tells us if that model is doing well on the validation set.

19
00:00:54,330 --> 00:00:57,350
In this way gives us an idea of how well the model

20
00:00:57,350 --> 00:01:01,185
generalizes to a set of data that is separate from the training set.

21
00:01:01,185 --> 00:01:05,780
The idea is, since the model doesn't use the validation set for deciding its weights,

22
00:01:05,780 --> 00:01:09,035
that it can tell us if we're overfitting the training set of data.

23
00:01:09,035 --> 00:01:11,900
Finally, the test set of data is saved for checking

24
00:01:11,900 --> 00:01:14,870
the accuracy of the model after it's trained.

25
00:01:14,870 --> 00:01:17,870
This may be easiest to see in an example.

26
00:01:17,870 --> 00:01:20,810
Say it's at the model to train for 200 epochs,

27
00:01:20,810 --> 00:01:22,340
and around epoch 100,

28
00:01:22,340 --> 00:01:24,050
you notice that the training loss starts

29
00:01:24,050 --> 00:01:27,930
decreasing but the validation loss is actually starting to increase.

30
00:01:27,930 --> 00:01:32,240
This actually indicates that the model is overfitting the training data because it's

31
00:01:32,240 --> 00:01:36,585
not generalizing well enough to also perform well on the validation set.

32
00:01:36,585 --> 00:01:41,595
So, if you see this divide and how the training and validation losses decrease,

33
00:01:41,595 --> 00:01:45,845
you'll want to stop changing the weights of your network around epoch 100 and

34
00:01:45,845 --> 00:01:50,630
ignore or throw away the weights from later epochs where there's evidence of overfitting.

35
00:01:50,630 --> 00:01:53,390
This process can also prove useful if we

36
00:01:53,390 --> 00:01:56,115
have multiple potential architectures to choose from.

37
00:01:56,115 --> 00:01:59,825
For example, if you're deciding on the number of layers to put in the model,

38
00:01:59,825 --> 00:02:01,625
then you'll want to save the weights from

39
00:02:01,625 --> 00:02:04,265
each potential architecture for later comparison,

40
00:02:04,265 --> 00:02:08,105
and can choose to pick the model that gets the lowest validation loss.

41
00:02:08,105 --> 00:02:11,570
You might be wondering why must we create a third dataset.

42
00:02:11,570 --> 00:02:14,310
Couldn't we just use the test set for this purpose?

43
00:02:14,310 --> 00:02:17,305
Well, the idea is that when we go to test the model,

44
00:02:17,305 --> 00:02:20,405
it looks at data that it has truly never seen before.

45
00:02:20,405 --> 00:02:24,220
Even though the model doesn't use the validation set to update its weights,

46
00:02:24,220 --> 00:02:27,230
our model selection process is based on how the model

47
00:02:27,230 --> 00:02:30,565
performs on both the training and validation sets.

48
00:02:30,565 --> 00:02:34,930
So, in the end, the model is biased in favor of the validation set.

49
00:02:34,930 --> 00:02:38,720
Thus, we need a separate test set of data to truly see how

50
00:02:38,720 --> 00:02:41,120
our selected model generalizes and performs when

51
00:02:41,120 --> 00:02:43,925
given data it really has not seen before.

52
00:02:43,925 --> 00:02:46,010
In the next video, I'll show you how to use

53
00:02:46,010 --> 00:02:48,750
these ideas to fine tune your network architecture

