1
00:00:00,000 --> 00:00:03,465
That's really interesting that you can do it piece by piece as well.

2
00:00:03,465 --> 00:00:09,347
At Udacity, we use a lot of Jupyter Notebooks to sort of break up tasks

3
00:00:09,347 --> 00:00:11,310
as far as like defining a model and training it,

4
00:00:11,310 --> 00:00:13,860
and I think it's really valuable to look at things

5
00:00:13,860 --> 00:00:16,695
like one pea at a time as these building blocks.

6
00:00:16,695 --> 00:00:21,690
Yeah. Absolutely. The other part that I think is worth telling is,

7
00:00:21,690 --> 00:00:24,420
if you have a giant model,

8
00:00:24,420 --> 00:00:28,965
you might not want to change big parts of the model.

9
00:00:28,965 --> 00:00:33,570
So, lets say there is a ResNet block.

10
00:00:33,570 --> 00:00:36,225
You might want to change how the ResNet

11
00:00:36,225 --> 00:00:38,960
looks and everything but the blocks in the ResNet,

12
00:00:38,960 --> 00:00:41,745
you almost never generally change.

13
00:00:41,745 --> 00:00:45,120
Like, they're just processing input data in the same kind of way.

14
00:00:45,120 --> 00:00:47,745
Yeah. Exactly, there like an LSTM cell.

15
00:00:47,745 --> 00:00:50,750
LSTM cell is a standard definition cell.

16
00:00:50,750 --> 00:00:57,200
You might change how the LSTM is put together and how many layers there are and stuff,

17
00:00:57,200 --> 00:01:01,610
but the cell as the core of it you never usually change.

18
00:01:01,610 --> 00:01:05,030
So, you can just add functional annotations

19
00:01:05,030 --> 00:01:09,570
to these smaller components while training the other components a lot.

20
00:01:09,570 --> 00:01:15,030
Which is why we call the new programming model hybrid front end,

21
00:01:15,030 --> 00:01:20,730
because you can make parts of a model compiled,

22
00:01:20,730 --> 00:01:24,345
parts of model still experimenting,

23
00:01:24,345 --> 00:01:28,430
and then that just gives you the best of both worlds.

24
00:01:28,430 --> 00:01:31,505
Yeah. So, this hybrid front end is allowing you

25
00:01:31,505 --> 00:01:37,700
to just sort of switch in between Python and basically like a C++ representation?

26
00:01:37,700 --> 00:01:38,100
Yeah.

27
00:01:38,100 --> 00:01:40,095
That's really interesting.

28
00:01:40,095 --> 00:01:48,410
So, if I was building a model and I wanted to- is there a way that I could sort of use

29
00:01:48,410 --> 00:01:55,520
optimization to train it faster or is it strictly these annotations

30
00:01:55,520 --> 00:01:58,355
are used strictly for after a model is trained?

31
00:01:58,355 --> 00:02:03,020
So, all of this is powered by what we call the JIT compiler in PyTorch.

32
00:02:03,020 --> 00:02:06,070
So, the [inaudible] digit compiler,

33
00:02:06,070 --> 00:02:13,550
in the short-term is to make sure we can export everything to production ready.

34
00:02:13,550 --> 00:02:17,990
Because we asked our users what's the thing that's most missing

35
00:02:17,990 --> 00:02:22,350
in PyTorch that's holding you back.

36
00:02:22,350 --> 00:02:25,710
They said, we use it at all these companies,

37
00:02:25,710 --> 00:02:28,235
like I've worked at my startup or I work at

38
00:02:28,235 --> 00:02:33,995
my big tech company and constantly other people around me are like,

39
00:02:33,995 --> 00:02:38,390
you can use PyTorch but remember you can't ship it to production.

40
00:02:38,390 --> 00:02:42,709
So, we wanted to first focus and remove that big constraint,

41
00:02:42,709 --> 00:02:46,225
make our users happy, open up that market.

42
00:02:46,225 --> 00:02:50,370
But the long-term investment of the JIT compiler remained to,

43
00:02:50,370 --> 00:02:53,570
is the parts of the model they're compiled,

44
00:02:53,570 --> 00:02:59,870
we're going to make them non-trivially faster by fusing operations,

45
00:02:59,870 --> 00:03:04,415
making more of the memory bandwidth bound operations into compute bond operations,

46
00:03:04,415 --> 00:03:07,715
and as newer hardwares come through,

47
00:03:07,715 --> 00:03:10,370
we can apply special tricks that are particular to

48
00:03:10,370 --> 00:03:14,950
each hardware that can do various things when they see a larger graph.

49
00:03:14,950 --> 00:03:17,945
Like make it more memory efficient and things like that.

50
00:03:17,945 --> 00:03:18,395
Yeah.

51
00:03:18,395 --> 00:03:20,135
But that's the longer term plan,

52
00:03:20,135 --> 00:03:27,655
to not just leveraged this JIT compiler we're using for exporting a train model,

53
00:03:27,655 --> 00:03:31,820
but also while you're training to make sure things get faster.

54
00:03:31,820 --> 00:03:36,740
So, before PyTorch 1.0,

55
00:03:36,740 --> 00:03:41,085
what were some of the process for moving a model to deployment?

56
00:03:41,085 --> 00:03:43,755
So, before PyTorch 1.0,

57
00:03:43,755 --> 00:03:52,850
about six to nine months ago we introduced an open standard called ONNX, O-N-N-X,

58
00:03:52,850 --> 00:03:59,345
and it's a standard that is not even like just focusing on PyTorch,

59
00:03:59,345 --> 00:04:01,890
it was a standard we tried to develop

60
00:04:01,890 --> 00:04:05,930
where all deep learning frameworks can talk to each other.

61
00:04:05,930 --> 00:04:08,240
That's another formatting kind of stuff. Yeah.

62
00:04:08,240 --> 00:04:11,750
That is correct. We partnered with Microsoft,

63
00:04:11,750 --> 00:04:15,920
with various big players to make

64
00:04:15,920 --> 00:04:21,885
sure all the deep learning frameworks like Chainer,

65
00:04:21,885 --> 00:04:26,354
Caffe 2, PyTorch and TensorFlow,

66
00:04:26,354 --> 00:04:32,360
someone can take a model that's trained in one and then export it to another framework.

67
00:04:32,360 --> 00:04:37,650
So, with PyTorch, the model then before 1.0,

68
00:04:37,650 --> 00:04:43,210
was to only export it to ONNX and then run it as another framework like TensorFlow.

69
00:04:43,210 --> 00:04:43,990
I see.

70
00:04:43,990 --> 00:04:49,110
The shortcomings of that which we felt were that ONNX is a standard,

71
00:04:49,110 --> 00:04:57,050
which means that all PyTorch models

72
00:04:57,050 --> 00:05:01,940
couldn't be exported because the standard hasn't developed fast enough to cover that.

73
00:05:01,940 --> 00:05:08,525
So, which means it was turning at models that are more complicated,

74
00:05:08,525 --> 00:05:10,400
wouldn't be ONNX exportable.

75
00:05:10,400 --> 00:05:13,765
Which is why we talk like we want a robust solution,

76
00:05:13,765 --> 00:05:15,600
and that's the 1.0 story.

77
00:05:15,600 --> 00:05:18,300
That's great. So, it's so instead of these separate steps

78
00:05:18,300 --> 00:05:21,140
of exporting and then importing into a different framework,

79
00:05:21,140 --> 00:05:23,690
you have kind of like squished all these steps into

80
00:05:23,690 --> 00:05:26,900
one thing that you can do an a hybrid front end.

81
00:05:26,900 --> 00:05:27,785
Exactly.

82
00:05:27,785 --> 00:05:29,240
That does sound ideal.

