1
00:00:00,000 --> 00:00:01,080
Again.

2
00:00:01,080 --> 00:00:02,625
So, in the last video,

3
00:00:02,625 --> 00:00:05,610
I hand you try out building

4
00:00:05,610 --> 00:00:09,015
your own neural network to classify this fashion in this dataset.

5
00:00:09,015 --> 00:00:12,900
Here is my solution like how I decided to build this.

6
00:00:12,900 --> 00:00:14,685
So first, building our network.

7
00:00:14,685 --> 00:00:18,270
So, here, I'm going to import our normal modules from PyTorch.

8
00:00:18,270 --> 00:00:19,665
So, nn and optim, so,

9
00:00:19,665 --> 00:00:21,630
nn is going to allow us to build our network,

10
00:00:21,630 --> 00:00:23,565
and optim is going to give us our optimizers.

11
00:00:23,565 --> 00:00:26,100
I must going to import this functional modules, so,

12
00:00:26,100 --> 00:00:30,015
we can use functions like ReLU and log softmax.

13
00:00:30,015 --> 00:00:33,810
I decided to define my network architectures using the class.

14
00:00:33,810 --> 00:00:36,150
So, in nn.modules subclassing from this,

15
00:00:36,150 --> 00:00:37,650
and it's called a classifier.

16
00:00:37,650 --> 00:00:41,270
Then I created four different linear transformations.

17
00:00:41,270 --> 00:00:45,935
So, in this case, it's three hidden layers and then one output layer.

18
00:00:45,935 --> 00:00:49,190
Our first hidden layer has 256 units.

19
00:00:49,190 --> 00:00:50,990
The second hidden layer has a 128,

20
00:00:50,990 --> 00:00:52,560
one after that has 64.

21
00:00:52,560 --> 00:00:55,200
Then our output has 10 units.

22
00:00:55,200 --> 00:00:57,330
So, in the forward pass, I did something a little different.

23
00:00:57,330 --> 00:01:01,385
So, I made sure here that the input tensor is actually flattened.

24
00:01:01,385 --> 00:01:06,050
So now, you don't have to flatten your input tensors in the training loop,

25
00:01:06,050 --> 00:01:08,335
it'll just do it in the forward pass itself.

26
00:01:08,335 --> 00:01:10,305
So, to do this is do x.view,

27
00:01:10,305 --> 00:01:12,645
which is going to change our shape.

28
00:01:12,645 --> 00:01:16,185
So, x.shape zero is going to give us our batch size.

29
00:01:16,185 --> 00:01:19,460
Then the negative one here is going to basically fill out

30
00:01:19,460 --> 00:01:21,590
the the second dimension with as many elements as it

31
00:01:21,590 --> 00:01:24,320
needs to keep the same total number of elements.

32
00:01:24,320 --> 00:01:27,530
So, what this does is it basically gives us another tensor,

33
00:01:27,530 --> 00:01:30,560
that is the flattened version of our input tensor.

34
00:01:30,560 --> 00:01:33,455
It doing a pass these through our linear transformations,

35
00:01:33,455 --> 00:01:36,025
and then ReLU activation functions.

36
00:01:36,025 --> 00:01:40,550
Then finally, we use a log softmax with a dimension set to one,

37
00:01:40,550 --> 00:01:43,205
as our output, and return that from our forward function.

38
00:01:43,205 --> 00:01:44,585
With the model defined,

39
00:01:44,585 --> 00:01:46,970
I can do model equals classifiers.

40
00:01:46,970 --> 00:01:48,460
So, this actually creates our model.

41
00:01:48,460 --> 00:01:53,435
Then we define our criterion with the negative log likelihood loss.

42
00:01:53,435 --> 00:01:56,300
So, I'm using log softmax as the output my model.

43
00:01:56,300 --> 00:02:00,010
So, I want to use the NLLLoss as the criterion.

44
00:02:00,010 --> 00:02:03,150
Then, here I'm using the Adam optimizer.

45
00:02:03,150 --> 00:02:07,070
So, this is basically the same as stochastic gradient descent,

46
00:02:07,070 --> 00:02:10,265
but it has some nice properties where it uses

47
00:02:10,265 --> 00:02:14,570
momentum which speeds up the actual fitting process.

48
00:02:14,570 --> 00:02:21,610
It also adjust the learning rate for each of the individual parameters in your model.

49
00:02:21,610 --> 00:02:24,465
Here, I wrote my training loop.

50
00:02:24,465 --> 00:02:26,674
So, again I'm using five epochs.

51
00:02:26,674 --> 00:02:28,670
So, for e in range epoch, so,

52
00:02:28,670 --> 00:02:31,955
this is going to basically loop through our dataset five times,

53
00:02:31,955 --> 00:02:35,090
I'm tracking the loss with running loss,

54
00:02:35,090 --> 00:02:36,900
and just kind of instantiated it here.

55
00:02:36,900 --> 00:02:38,565
Then, getting our images.

56
00:02:38,565 --> 00:02:41,175
So, from images labels in train loader,

57
00:02:41,175 --> 00:02:45,635
so I get our log probabilities by passing in the images to a model.

58
00:02:45,635 --> 00:02:48,020
So, one thing to note, you can kind of do a little shortcut.

59
00:02:48,020 --> 00:02:51,320
If you just pass these in to model as if it was a function,

60
00:02:51,320 --> 00:02:53,810
then it will run the forward method.

61
00:02:53,810 --> 00:02:59,635
So, this is just a kind of a shorter way to run the forward pass through your model.

62
00:02:59,635 --> 00:03:02,060
Then with the log probabilities and the labels,

63
00:03:02,060 --> 00:03:03,560
I can calculate the loss.

64
00:03:03,560 --> 00:03:06,110
Then here, I am zeroing the gradients.

65
00:03:06,110 --> 00:03:09,485
Now I'm doing the lost up backwards to calculating our gradients,

66
00:03:09,485 --> 00:03:11,890
and then with the gradients, I can do our optimizer step.

67
00:03:11,890 --> 00:03:14,210
If we tried it, we can see at least for

68
00:03:14,210 --> 00:03:17,645
these first five epochs that are loss actually drops.

69
00:03:17,645 --> 00:03:18,860
Now the network is trained,

70
00:03:18,860 --> 00:03:20,555
we can actually test it out.

71
00:03:20,555 --> 00:03:24,965
So, we pass in data to our model, calculate the probabilities.

72
00:03:24,965 --> 00:03:28,130
So, here, doing the forward pass through the model to

73
00:03:28,130 --> 00:03:31,510
get our actual log probabilities and with the log probabilities,

74
00:03:31,510 --> 00:03:33,980
you can take the exponential to get the actual probabilities.

75
00:03:33,980 --> 00:03:38,875
Then with that, we can pass it into this nice little view classify function that I wrote,

76
00:03:38,875 --> 00:03:40,365
and it shows us,

77
00:03:40,365 --> 00:03:42,110
if we pass an image of a shirt,

78
00:03:42,110 --> 00:03:44,195
it tells us that it's a shirt.

79
00:03:44,195 --> 00:03:47,975
So, our network seems to have learned fairly well,

80
00:03:47,975 --> 00:03:50,800
what this dataset is showing us.

