1
00:00:00,000 --> 00:00:03,990
When we design an algorithm to classify objects in images,

2
00:00:03,990 --> 00:00:07,685
we have to deal with a lot of irrelevant information.

3
00:00:07,685 --> 00:00:10,320
We really only want our algorithm to

4
00:00:10,320 --> 00:00:13,565
determine if an object is present in the image or not.

5
00:00:13,565 --> 00:00:16,225
The size of the object doesn't matter,

6
00:00:16,225 --> 00:00:17,765
neither does the angle,

7
00:00:17,765 --> 00:00:20,905
or if I move it all the way to the right side of the image.

8
00:00:20,905 --> 00:00:23,669
It's still an image with an avocado.

9
00:00:23,669 --> 00:00:27,330
In other words, we can say that we want our algorithm to

10
00:00:27,330 --> 00:00:31,245
learn an invariant representation of the image.

11
00:00:31,245 --> 00:00:36,490
We don't want our model to change its prediction based on the size of the object.

12
00:00:36,490 --> 00:00:39,355
This is called scale invariance.

13
00:00:39,355 --> 00:00:43,335
Likewise, we don't want the angle of the object to matter.

14
00:00:43,335 --> 00:00:46,385
This is called rotation invariance.

15
00:00:46,385 --> 00:00:50,880
If I shift the image a little to the left or to the right, well,

16
00:00:50,880 --> 00:00:53,050
it's still an image with an avocado,

17
00:00:53,050 --> 00:00:56,350
and this is called translation invariance.

18
00:00:56,350 --> 00:01:02,115
CNN's do have some built-in translation invariance.

19
00:01:02,115 --> 00:01:07,170
To see this, you'll need to first recall how we calculate max-pooling layers.

20
00:01:07,170 --> 00:01:09,710
Remember that at each window location,

21
00:01:09,710 --> 00:01:13,380
we took the maximum of the pixels contained in the window.

22
00:01:13,380 --> 00:01:17,245
This maximum value can occur anywhere within the window.

23
00:01:17,245 --> 00:01:20,180
The value of the max-pooling node would be the

24
00:01:20,180 --> 00:01:23,765
same if we translated the image a little to the left,

25
00:01:23,765 --> 00:01:25,815
to the right, up,

26
00:01:25,815 --> 00:01:30,170
down, as long as the maximum value stays within the window.

27
00:01:30,170 --> 00:01:33,290
The effect of applying many max-pooling layers in

28
00:01:33,290 --> 00:01:37,640
a sequence each one following a convolutional layer,

29
00:01:37,640 --> 00:01:40,770
is that we could translate the object quite far to the left,

30
00:01:40,770 --> 00:01:42,110
to the top of the image,

31
00:01:42,110 --> 00:01:43,550
to the bottom of the image,

32
00:01:43,550 --> 00:01:46,870
and still our network will be able to make sense of it all.

33
00:01:46,870 --> 00:01:49,925
This is truly a non-trivial problem.

34
00:01:49,925 --> 00:01:53,790
Recall that the computer only sees a matrix of pixels.

35
00:01:53,790 --> 00:01:56,770
Transforming an object's scale, rotation,

36
00:01:56,770 --> 00:02:00,895
or position in the image has a huge effect on the pixel values.

37
00:02:00,895 --> 00:02:05,660
We as humans can see the difference in images quite clearly,

38
00:02:05,660 --> 00:02:10,090
but how do you think you'd do if you were just given the corresponding array of numbers?

39
00:02:10,090 --> 00:02:12,710
Thankfully, there's a technique that works well for

40
00:02:12,710 --> 00:02:16,339
making our algorithms more statistically invariant,

41
00:02:16,339 --> 00:02:19,700
but it will feel a little bit like cheating.

42
00:02:19,700 --> 00:02:21,935
The idea is this,

43
00:02:21,935 --> 00:02:25,745
if you want your CNN to be rotation invariant,

44
00:02:25,745 --> 00:02:28,730
well, then you can just add some images to your training

45
00:02:28,730 --> 00:02:33,180
set created by doing random rotations on your training images.

46
00:02:33,180 --> 00:02:35,985
If you want more translation invariance,

47
00:02:35,985 --> 00:02:39,350
you can also just add new images created

48
00:02:39,350 --> 00:02:42,925
by doing random translations of your training images.

49
00:02:42,925 --> 00:02:44,240
When we do this,

50
00:02:44,240 --> 00:02:48,640
we say that we have expanded the training set by augmenting the data.

51
00:02:48,640 --> 00:02:53,390
Data augmentation will also help us to avoid overfitting.

52
00:02:53,390 --> 00:02:57,595
This is because the model is seeing many new images.

53
00:02:57,595 --> 00:03:00,485
Thus, it should be better at generalizing

54
00:03:00,485 --> 00:03:04,150
and we should get better performance on the test dataset.

55
00:03:04,150 --> 00:03:09,380
Let's augment the training data and the CFR 10 dataset from the previous video,

56
00:03:09,380 --> 00:03:12,250
and see if we can improve our test accuracy.

57
00:03:12,250 --> 00:03:16,950
We'll be using a Jupiter notebook that you can download below.

