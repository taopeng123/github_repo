1
00:00:00,000 --> 00:00:03,360
When a CNN is trained to classify images,

2
00:00:03,360 --> 00:00:05,445
it's convolutional layers learn to extract

3
00:00:05,445 --> 00:00:08,350
more and more complex features from a given image.

4
00:00:08,350 --> 00:00:12,900
Intermittently, max pooling layers will discard detailed spatial information,

5
00:00:12,900 --> 00:00:16,880
information that's increasingly irrelevant to the task of classification.

6
00:00:16,880 --> 00:00:20,640
The effect of this is that as we go deeper into a CNN,

7
00:00:20,640 --> 00:00:24,660
the input image is transformed into feature maps that increasingly care about

8
00:00:24,660 --> 00:00:29,565
the content of the image rather than any detail about the texture and color of pixels.

9
00:00:29,565 --> 00:00:32,280
Later layers of a network are even sometimes referred

10
00:00:32,280 --> 00:00:35,435
to as a content representation of an image.

11
00:00:35,435 --> 00:00:38,040
In this way, a trained CNN has already learned to

12
00:00:38,040 --> 00:00:41,155
represent the content of an image, but what about style?

13
00:00:41,155 --> 00:00:45,515
Style can be thought of as traits that might be found in the brush strokes of a painting,

14
00:00:45,515 --> 00:00:48,305
its texture, colors, curvature, and so on.

15
00:00:48,305 --> 00:00:49,745
To perform style transfer,

16
00:00:49,745 --> 00:00:53,500
we need to combine the content of one image with the style of another.

17
00:00:53,500 --> 00:00:56,505
So, how can we isolate only the style of an image?

18
00:00:56,505 --> 00:00:59,030
To represent the style of an input image,

19
00:00:59,030 --> 00:01:03,105
a feature space designed to capture texture and color information is used.

20
00:01:03,105 --> 00:01:07,420
This space essentially looks at spatial correlations within a layer of a network.

21
00:01:07,420 --> 00:01:12,410
A correlation is a measure of the relationship between two or more variables.

22
00:01:12,410 --> 00:01:15,230
For example, you could look at the features extracted

23
00:01:15,230 --> 00:01:18,305
in the first convolutional layer which has some depth.

24
00:01:18,305 --> 00:01:21,825
The depth corresponds to the number of feature maps in that layer.

25
00:01:21,825 --> 00:01:23,255
For each feature map,

26
00:01:23,255 --> 00:01:24,560
we can measure how strongly

27
00:01:24,560 --> 00:01:28,565
its detected features relate to the other feature maps in that layer.

28
00:01:28,565 --> 00:01:32,885
Is a certain color detected in one map similar to a color in another map?

29
00:01:32,885 --> 00:01:36,705
What about the differences between detected edges and corners, and so on?

30
00:01:36,705 --> 00:01:41,565
See which colors and shapes in a set of feature maps are related and which are not.

31
00:01:41,565 --> 00:01:44,120
Say, we detect that mini-feature maps in

32
00:01:44,120 --> 00:01:47,775
the first convolutional layer have similar pink edge features.

33
00:01:47,775 --> 00:01:50,900
If there are common colors and shapes among the feature maps,

34
00:01:50,900 --> 00:01:53,680
then this can be thought of as part of that image's style.

35
00:01:53,680 --> 00:01:57,890
So, the similarities and differences between features in a layer should

36
00:01:57,890 --> 00:02:02,495
give us some information about the texture and color information found in an image.

37
00:02:02,495 --> 00:02:03,890
But at the same time,

38
00:02:03,890 --> 00:02:06,050
it should leave out information about

39
00:02:06,050 --> 00:02:09,970
the actual arrangement and identity of different objects in that image.

40
00:02:09,970 --> 00:02:14,155
Now, we've seen that content and style can be separate components of an image.

41
00:02:14,155 --> 00:02:17,310
Let's think about this in a complete style transfer example.

42
00:02:17,310 --> 00:02:20,385
Style transfer will look at two different images.

43
00:02:20,385 --> 00:02:23,780
We often call these the style image and the content image.

44
00:02:23,780 --> 00:02:25,345
Using a trained CNN,

45
00:02:25,345 --> 00:02:29,425
style transfer finds the style of one image and the content of the other.

46
00:02:29,425 --> 00:02:33,120
Finally, it tries to merge the two to create a new third image.

47
00:02:33,120 --> 00:02:34,740
In this newly created image,

48
00:02:34,740 --> 00:02:38,240
the objects and their arrangement are taken from the content image,

49
00:02:38,240 --> 00:02:41,135
and the colors and textures are taken from the style image.

50
00:02:41,135 --> 00:02:43,390
Here's our example of an image of a cat,

51
00:02:43,390 --> 00:02:48,040
the content image, being combined with a Hokusai-style image of waves.

52
00:02:48,040 --> 00:02:52,315
Effectively, style transfer creates a new image that keeps the cat content,

53
00:02:52,315 --> 00:02:53,760
but renders it with the colors,

54
00:02:53,760 --> 00:02:57,035
the print texture, the style of the wave artwork.

55
00:02:57,035 --> 00:03:00,105
This is the theory behind how style transfer works.

56
00:03:00,105 --> 00:03:04,670
Next, let's talk more about how we can actually extract features from different layers of

57
00:03:04,670 --> 00:03:09,920
a trained model and use them to combine the style and content of two different images.

