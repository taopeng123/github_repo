
The book "A Byte of Python" has been all recroded in this note.

==
findgeneral                        Python general
findcomment                        Comment in Python
finddatatypes                      Data types
findescape                         Escape Sequences
findoperators                      Operators
findmath                           Math functions
findand                            and
findor                             or
findnot                            not
findprint                          print
findreadfrominput                  Read from input in terminal
findreadfromfile                   Read from file 
findwritetofile                    Write to file
findreadfromwebpage                Read from webpage
findwhile                          while
findif                             if
findcast                           cast
findround                          Round to nearest integer
findprecision                      Display a float with two decimal places:
findbreak                          break
findcontinue                       continue
findfor                            for
findrange                          range
findfunction                       function
findparameter                      Function parameters
findreturn                         The return Statement
findpass                           Pass statement
finddocstring                      DocStrings
findlocal                          Local variables
findglobal                         Global statement
findnonlocal                       Nonlocal statement
finddefaultargument                Default Argument Values
findkeywordargument                Keyword Arguments
findvararg                         VarArgs parameters
findkeywordonly                    Keyword- only Parameters
findmodule                         Modules
findtuple                          tuple
findlist                           list
finddictionary                     dictionary
findset                            set
findsequence                       sequences 
findnull                           null
findnone                           None
findstring                         string
findsubstring                      Return a substring, Determine whether a string contains a substring
findsplit                          Split a string
findprefix                         Whether a string start with a prefix
findstrip                          Strip a string (remove charaters at beginning and end)
findtrim                           Strip a string (remove charaters at beginning and end)
findjoinlisttostring               Join list to string
findargs                           Parse command line arguments  
findargv                           Parse command line arguments 
findclass                          class
findinheritance                    inheritance
finddestructor                     destructor
findgetter                         Getters and setters
findsetter                         Getters and setters
findexception                      Exceptions
findsleep                          sleep
findtime                           Output current local time
findnumpy                          NumPy
findrandomnumber                   Random number
findpandas                         Pandas
findreadcsv                        Read csv file to a dataframe
finddataframe                      DataFrame
findcreatedataframe                Create DataFrame
findrank                           Rank
findcorrelation                    Correlation
findaddcolumn                      Add a column
findplot                           Plot in Python, using matplotlib
finddates                          Dates in pandas
findjoin                           Join
findmerge                          Join, merge, concat
findconcat                         Join, merge, concat
finddropnan                        Drop the rows with NaN
findrenamecolumn                   Rename colum
findhead                           head
findtail                           tail
findslice                          Slice dataframe             
findselect                         Select rows and columns
findiloc                           iloc, loc, ix
findloc                            iloc, loc, ix
findix                             iloc, loc, ix
findmax                            Max
findstatistics                     Statistics functions
findlinearregression               Linear Regression
findpolyfit                        Polyfit
findsort                           Sort
findgroupby                        group by
findlen                            Number of rows in dataframe df
findsize                           Number of rows in dataframe df
findisnull                         isnull or missing values
findmissingvalues                  isnull or missing values
findnormalize                      Normalize
findsumcolumns                     Sum of all columns 
findmeancolumns                    Mean of all columns 


(endfind)

==
# Read the book "A Byte of Python" in full again in May 2018, and recorded all my notes in this file.

Features of Python (from Tao):

1. There is no ; at the end of each statement.
2. We can not randomly add spaces in front of a statement.
3. In "i = 5", there can be spaces between =
4. No need to declare data type for a variable like: int i = 5.

==
(findgeneral)                        
Python general

Portable:

Due to its open-source nature, Python has been ported to (i.e. changed to make it work on) many platforms. All your Python programs can work on any of these platforms without requiring any changes at all if you are careful enough to avoid any system-dependent features.

--
Interpreted:

A program written in a compiled language like C or C++ is converted from the source language i.e. C or C++ into a language that is spoken by your computer (binary code i.e. 0s and 1s) using a compiler with various flags and options. When you run the program, the linker/loader software copies the program from hard disk to memory and starts running it.

Python, on the other hand, does not need compilation to binary. You just run the program directly from the source code. Internally, Python converts the source code into an intermediate form called bytecodes and then translates this into the native language of your computer and then runs it. All this, actually, makes using Python much easier since you don't have to worry about compiling the program, making sure that the proper libraries are linked and loaded, etc, etc. This also makes your Python programs much more portable, since you can just copy your Python program onto another computer and it just works!

--
Python is strongly object-oriented in the sense that everything is an object including numbers, strings and functions.

--
To test if you have Python already installed on your Linux box:

$ python -V

If you have Python 2.x already installed, then try python3 -V

--
There are two ways of using Python to run your program - using the interactive interpreter
prompt or using a source file.

Using The Interpreter Prompt:

$ python
>>> print('Hello World')

$ python3
>>> print('Hello World')

To exit the prompt, press ctrl-d 

--
Like C++, unlike Java:
A file name can be different from the class name. A file can even have no class in it.

--
Using A Source File:

I follow the convention of having all Python programs saved with the extension .py

-- File helloworld.py starts --

#!/usr/bin/python
#Filename: helloworld.py
print('Hello World')

-- File helloworld.py ends --

Run the above program:

$ python helloworld.py

See below for the benefit of specifying the interpreter at the beginning of the file

--
Executable Python Programs:

First, we have to give the program executable permission using the chmod command then run the source program.

$ chmod a+x helloworld.py
$ ./helloworld.py

The chmod command is used here to change the mode of the file by giving execute permission to all users of the system.

--
Tao: the benefit of specifying the interpreter at the beginning of the file:

You can rename the file to just helloworld and run it as ./helloworld and it will still work since the system knows that it has to run the program using the interpreter whose location is specified in the first line in the source file.

What if you don't know where Python is located? Then, you can use the special env program on Linux/Unix systems. Just change the first line of the program to the following:

#!/usr/bin/env python

The env program will in turn look for the Python interpreter which will run the program.

--
Python does not use comments except for the special case of the first line here (#!/usr/bin/python). It is called the shebang line - whenever the first two characters of the source file are #! followed by the location of a program, this tells your Linux/Unix system that this program should be run with this interpreter when you execute the program.

W.r.t. Python, a program or a script or software all mean the same thing.

--
If you want to specify more than one logical line on a single physical line, then you have to
explicitly specify this using a semicolon (;) which indicates the end of a logical
line/statement. For example:

i = 5; print(i);

An example of writing a logical line spanning many physical lines follows. This is referred to
as explicit line joining.
s = 'This is a string. \
This continues the string.'
print(s)

This gives the output:
This is a string. This continues the string.
Similarly,
print\
(i)
is the same as
print(i)

--
Indentation

Statements which go together must have the same indentation. Each such set of statements is called a block

I strongly recommend that you use a single tab or four spaces for each indentation level.

Python will always use indentation for blocks and will never use braces.

In the above file (helloworld.py), ensure there are no spaces or tabs before the first character in each line

--
What if we wanted to be able to run the program from anywhere? 

You can do this by storing the program in one of the directories listed in the PATH environment variable. Whenever you run any program, the system looks for that program in each of the directories listed in the PATH environment variable and then runs that program. We can make this program available everywhere by simply copying this source file to one of the directories listed in PATH.

$ echo $PATH
/usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin:/home/swaroop/bin

$ cp helloworld.py /home/swaroop/bin/helloworld

$ helloworld
Hello World

We see that /home/swaroop/bin is one of the directories in the PATH variable

Alternatively, you can add a directory of your choice to the PATH variable - this can be done by running PATH=$PATH:/home/swaroop/mydir

This method is very useful if you want to write useful scripts that you want to run the program anytime, anywhere. It is like creating your own commands just like cd or any other commands that you use in the Linux terminal or DOS prompt.

--
Python is case-sensitive

--
(findcomment)
Comment in Python:

Anything to the right of the # symbol is a comment

--
If you need quick information about any function or statement in Python, then you can use the built-in help functionality. This is very useful especially when using the interpreter prompt. For example, run help(print) - this displays the help for the print function which is used to print things to the screen. Use help() to learn
more about using help itself! In case you need to get help for operators like return, then you need to put those inside quotes such as help('return') so that Python doesn't get confused on what we're trying to do.

Press q to exit the help.

==
(finddatatypes)
Data types:

# Show data type:
i = 123
type(i)
# Displays: <type 'int'>

# Convert string to int:
x = '123456'
i = int(x)

# Convert int to string:
str(10) # Returns: '10'

Numbers:

Numbers in Python are of three types - integers, floating point (or floats for short) and complex numbers.

Examples:

Integer: 2
Float: 3.23, 52.3E-4.
Complex number: (-5+4j), (2.3 - 4.6j)

The default integer type can be any large value.

Boolean:

The True and False are called Boolean types and you can consider them to be equivalent to the value 1 and 0 respectively.

if True:
    print('Yes, it is true')

--
Print the type of a variable:

>>> i = 123
>>> type(i)
<type 'int'>
>>> type(i) is int
True

==
(findnull)
(findnone)
None

与C不同的是，在python中是没有NULL的，取而代之的是None，它的含义是为空，但要注意和空列表与空字符串的区别，None的类型是Nonetype

>>>a=None
>>>type(a)
<class 'Nonetype'>

另外，None是没有像len,size等属性的，要判断一个变量是否为None，直接使用
if a == None. 

Same as C++:
"if a" is equivalent to "if a != None"
"if not a" is equivalent to "if a == None"

再者，注意None与布尔类型的区别，布尔类型只包括两个：True和False（注意它的大小写）
但python是把0，空字符串‘ ’和None都看作False，把其他数值和非空字符串都看作True

==
(findstring)
String

name = 'john'

len(str) # Length of str

--
(findsubstring)

** Return substring:

To return a substring from a string, search for "find sequence",
this is also what people did online.

** Whether contains substring:

Determine whether a string contains a substring:
substr in str # Returns True if str contains substr, otherwise false.

** Position of substring:

str1 = "this is string example....wow!!!";
str2 = "exam";

print str1.find(str2) # Returns: 15 <- the index of 'e' in str1

** Starts with **

'helo'.startswith('he') # Returns: true

** Ends with **

'helo'.endswith('lo') # Returns: true

** Contains substring **

'el' in 'helo' # Returns: true


--
(findsplit)
Split a string

split() method returns a list of strings after breaking the given string by the specified separator.

# Splits at space
text = 'geeks for geeks'
text.split() # Returns: ['geeks', 'for', 'geeks']
 
# Splits at ',' 
word = 'geeks, for, geeks'
word.split(', ') # Returns: ['geeks', 'for', 'geeks']

--
(findstrip)
(findtrim)
Strip a string (remove charaters at beginning and end)

str.strip() removes all whitespace at the start and end, including spaces, tabs, newlines and carriage returns. Leaving it in doesn't do any harm, and allows your program to deal with unexpected extra whitespace inserted into the file.

# str.strip([chars]): 相當於Java中的trim(), returns a copy of the string in which all chars have been stripped(即removed) from the beginning and the end of the string (default whitespace characters).

str = "0000000this is string example....wow!!!0000000";
str.strip('0') # Returns: "this is string example....wow!!!"

--
(findprefix)
Whether a string start with a prefix

The method startswith() checks whether string starts with str

str = "this is string example"
str.startswith('this') # Returns True

--
(findjoinlisttostring)
Join list to string

delimiter = '_*_'
mylist = ['Brazil', 'Russia', 'India', 'China']
delimiter.join(mylist) # Returns 'Brazil_*_Russia_*_India_*_China'

--
(findargs)   
(findargv)    
Parse command line arguments   

** Simple way **

import sys

for i in sys.argv:
    print(i)

$ python using_sys.py we are arguments

using_sys.py
we
are
arguments

** Complicated way **

File prog.py:

import optparse
parser = optparse.OptionParser()
parser.add_option('-q', '--query', action="store", dest="query") # Tao: dest="query" means the input argument is stored in options.query.
options, args = parser.parse_args()
print 'Query string=', options.query

Using the file prog.py:

python prog.py -q helo
# Output: Query string= helo

python prog.py --query helo 
# Output: Query string= helo

--
Single Quotes:
You can specify strings using single quotes such as 'Quote me on this'.

Double Quotes:
Strings in double quotes work exactly the same way as strings in single quotes. An example
is "What's your name?"

Triple Quotes:
You can specify multi-line strings using triple quotes - (""" or '''). You can use single quotes
and double quotes freely within the triple quotes. An example is:
'''This is a multi-line string. This is the first line.
This is the second line.
"What's your name?," I asked.
He said "Bond, James Bond."
'''

--
Reverse a string: nameStr[::-1]

--
# The format function does not only applies to print, but applies to all strings:

name = "Tao"
s = "My name is {}".format(name) 

--
(findescape)
Escape Sequences:

You specify the single quote as \'
Now, you can specify the string as 'What\'s your name?'

What if you wanted to specify a two-line string? One way is to use a triple-quoted string as shown previously or you can use an escape sequence for the newline character - \n to indicate the start of a new line.

One thing to note is that in a string, a single backslash at the end of the line indicates that the string is continued in the next line, but no newline is added. For example:
"This is the first sentence.\
This is the second sentence."
is equivalent to "This is the first sentence. This is the second sentence.".

--
Raw Strings
If you need to specify some strings where no special processing such as escape sequences are handled, then what you need is to specify a raw string by prefixing r or R to the string. An example is r"Newlines are indicated by \n".

Always use raw strings when dealing with regular expressions. Otherwise, a lot of backwhacking may be required. For example, backreferences can be referred to as '\\1' or r'\1'.

--
Strings Are Immutable
This means that once you have created a string, you cannot change it.

--
String Literal Concatenation
If you place two string literals side by side, they are automatically concatenated by Python.
For example, 'What\'s ' 'your name?' is automatically converted in to "What's your name?".

--
There is no separate char data type in Python. There is no real need for it and I am sure you won't miss it.

--
The format Method

Sometimes we may want to construct strings from other information. This is where the format() method is useful. The format method can be called to substitute those specifications with corresponding arguments to the format method.

age = 25
name = 'Swaroop'
print('{0} is {1} years old'.format(name, age)) # Output: Swaroop is 25 years old
print('Why is {0} playing with that python?'.format(name)) # Output: Why is Swaroop playing with that python?

Notice that we could achieved the same using string concatenation: name + ' is ' + str(age) + ' years old' but notice how much uglier and error-prone this is.

More examples of format method:

>>> '{0:.3}'.format(1/3) # decimal (.) precision of 3 for float
'0.333'

>>> '{0:_^11}'.format('hello') # fill with underscores (_) with the text centered (^) to 11 width
'___hello___'

>>> '{name} wrote {book}'.format(name='Swaroop', book='A Byte of Python') # keyword-based
'Swaroop wrote A Byte of Python'

==
(findoperators)
Operators

Evaluation order:
Remember (same as Java and C++):
->||

There are no ++ and -- operators in Python.
x++ can be written as x += 1 and x-- can be written as x -= 1 <- tao: so there are += and -= operators in Python.

Shortcut:
a *= 3
is equivalent as
a = a * 3

Power:
3 ** 4 gives 3 * 3 * 3 * 3

Divide:
4 / 3 gives 1.3333333333333333, not 1!

Floor Division:
4 // 3 gives 1

Modulus:
5 % 2 gives 1

(findand)
(findor)
(findnot)
Bitwise AND: &
Bit-wise OR: |

Boolean AND: and
Boolean OR:  or 
Boolean NOT: not

Less Than: <
Less Than or Equal To: <=

Equal To: ==
x = 'str'; 
y = 'str'; 
x == y returns True.

Not Equal To: !=

Right Shift: >>
11 >> 1 gives 5. 11 is represented in bits by 1011 which when right shifted by 1 bit gives 101 which is the decimal 5.

Left Shift: <<

==
(findmath)
Math functions

import math 
math.exp(-45.17) # The method exp() returns returns exponential of x: e^x.
math.sqrt(100)

# No need to import anything for abs
abs(-23.56)
abs(-46)


==
(findprint)
print

A better way to print without using format (from online and tao's experiment):

a = 1
b = 2
print('a =', a) # Output: a = 1
print('a = ', a, ', b = ', b, sep="") # Output: a = 1, b = 2

--
age = 25
age *= 2
name = 'John'
print(age)
print('{0} is speaking'.format(name))
print('{0} is {1} years old'.format(name, age))
print('{name} is {age} years old'.format(name = 'Kevin', age = 40))
print('Age is', age) # Output: Age is 25, note a space is added between "Age is" and "25".

By deafult, the print() function prints the text as well as an automatic newline to the screen. Tao: to override it, use this:
print(line, end = 'a')
this makes the newline character replaced by 'a'.

--
From online:

print in Python 3 vs Python 2:

Old: print "The answer is", 2*2
New: print("The answer is", 2*2)

Old: print x,           # Trailing comma suppresses newline
New: print(x, end=" ")  # Appends a space instead of a newline

Old: print              # Prints a newline
New: print()            # You must call the function!

==
(findreadfrominput)
Read from input in terminal

# something = input('Enter text: ') //Prints "Enter text: " to the screen and waits for input from the user.
# Enter text: sir // sir是用户從鍵盤輸入的, 然後something就等於sir了

==
(findreadfromfile)
Read from file 

file = open('poem.txt') # if no mode is specified, 'r'ead mode is assumed by default

while True:
    line = file.readline()
    if len(line) == 0: # Zero length indicates EOF
        break
    print(line, end = '') # Suppress the newline at end of each output line, see more below.
file.close() # close the file

By deafult, the print() function prints the text as well as an automatic newline to the screen. We are suppressing the newline by specifying end='' because the line that is read from the file already ends with a newline character.

==
(findwritetofile)
Write to file

Write to file:

f = open('poem.txt', 'w') # w: write mode. r: read mode. a: append mode.
f = open('poem.txt', 'w+') # w+: the + sign that means it will create a file if it does not exist
f.write('helo') # write text to file
f.close() # close the file

Determine whether a file exists:

import os
os.path.exists('/this/is/a/dir') # Returns true for directories, not just files.

==
(findreadfromwebpage)
Read from webpage:

link = "https://stackoverflow.com/questions/15138614/how-can-i-read-the-contents-of-an-url-with-python"
f = urllib.urlopen(link)
myfile = f.read()
print myfile

==
(findwhile)
(findif)
(findbreak)
(findcontinue)
(findtrue)

while, if, cast, break, continue, True

number = 1
running = True

while running:
    #guess = int(input("Input a number: ")) //由此可知, int(str)可以將 string類型的str 轉化為int
    guess = 1

    if guess == number:
        print("That's right.")
        running = False
    elif guess > number:
        print('Too big.')
        break
    else:
        print('Too small.')
        continue
else:
    print('The while loop is over.') // A while statement can have an optional else clause. The else block is executed when the while loop condition becomes False

There is no switch statement in Python.

The continue statement is used to tell Python to skip the rest of the statements in the current loop block and to continue to the next iteration of the loop.

The break statement is used to break out of a loop statement i.e. stop the execution of a looping statement, even if the loop condition has not become False or the sequence of items has been completely iterated over.

An important note is that if you break out of a for or while loop, any corresponding loop else block is not executed.

==
(findcast)
int(x)
float(x)

--
(findround)
Round to nearest integer:

int(round(x))

--
(findprecision)
Display a float with two decimal places:

>>> '%.2f' % 1.234
'1.23'

>>> '%.2f' % 5.0
'5.00'

==
(findfor)
(findrange)
for, range

for i in range(1, 5): # range(1, 5)是一個sequence: [1, 2, 3, 4]. 它包括1, 但不包括5! range(1,5,2) = [1,3].
    print(i)

Remember that the for..in loop works for any sequence. Here, we have a list of numbers generated by the built-in range function, but in general we can use any kind of sequence of any kind of objects!

--
fruits = ['banana', 'apple',  'mango']

for fruit in fruits:        
   print fruit

--
Question:

Is there a way to step between 0 and 1 by 0.1?

I thought I could do it like the following, but it failed:

for i in range(0, 1, 0.1):
    print i
Instead, it says that the step argument cannot be zero, which I did not expect.

Answer:

Rather than using a decimal step directly, it's much safer to express this in terms of how many points you want. Otherwise, floating-point rounding error is likely to give you a wrong result.

You can use the linspace function from the NumPy library (which isn't part of the standard library but is relatively easy to obtain). linspace takes a number of points to return, and also lets you specify whether or not to include the right endpoint:

>>> np.linspace(0,1,11)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])
>>> np.linspace(0,1,10,endpoint=False)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])
If you really want to use a floating-point step value, you can, with numpy.arange.

>>> import numpy as np
>>> np.arange(0.0, 1.0, 0.1)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])
Floating-point rounding error will cause problems, though. Here's a simple case where rounding error causes arange to produce a length-4 array when it should only produce 3 numbers:

>>> numpy.arange(1, 1.3, 0.1)
array([1. , 1.1, 1.2, 1.3])

==
(findfunction)
function

def sayHello():
    print('Hello World!')

sayHello() # Call the function

def fahrenheit(value):
    return (value * 9 / 5) + 32

Tao: from above and below, Python also uses naming conventions like sayHello (Java style), rather than say_hello (C++ style).

--
(findparameter)
Function parameters:

Note the terminology used - the names given in the function definition are called parameters whereas the values you supply in the function call are called arguments.

def printMax(a, b):
    if a > b:
        print(a, 'is maximum')
    elif a == b:
        print(a, 'is equal to', b)
    else:
        print(b, 'is maximum')

printMax(3, 4)

x = 5
y = 7
printMax(x, y)

--
(findreturn)
The return Statement

def maximum(x, y):
    if x > y:
        return x
    else:
        return y

maximum(2, 3) //Returns 3

Note that a return statement without a value is equivalent to return None. None is a special type in Python that represents nothingness. For example, it is used to indicate that a variable has no value if it has a value of None.

Every function implicitly contains a return None statement at the end unless you have written your own return statement. 

--
(findpass)
Pass statement

def someFunction():
    pass

The pass statement is used in Python to indicate an empty block of statements.

--
(finddocstring)
DocStrings

Python has a nifty feature called documentation strings, usually referred to by its shorter name docstrings. DocStrings are an important tool that you should make use of since it helps to document the program better and makes it easier to understand.

A string on the first logical line of a function is the docstring for that function. Note that DocStrings also apply to modules and classes.

The convention followed for a docstring is a multi-line string where the first line starts with a capital letter and ends with a dot. Then the second line is blank followed by any detailed explanation starting from the third line. You are strongly advised to follow this convention for all your docstrings for all your non-trivial functions.

We can access the docstring of the printMax function using the __doc__ (notice the double underscores) attribute (name belonging to) of the function.

If you have used help() in Python, then you have already seen the usage of docstrings! What it does is just fetch the __doc__ attribute of that function and displays it in a neat manner for you. You can try it out on the function above - just include help(printMax) in your program. Remember to press the q key to exit help.

def printMax(x, y):
    '''Prints the maximum of two numbers.

    The two values must be integers.'''
    x = int(x) # convert to integers, if possible
    y = int(y)

    if x > y:
        print(x, 'is maximum')
    else:
        print(y, 'is maximum')

printMax(3, 5) # Output: 5 is maximum

print(printMax.__doc__)
# Output:
Prints the maximum of two numbers.

    The two values must be integers.

We can use docstrings for classes as well as methods. We can access the class docstring at runtime using Robot.__doc__ and the method docstring as Robot.sayHi.__doc__

--
(findlocal)
Local variables

When you declare variables inside a function definition, they are not related in any way to other variables with the same names used outside the function i.e. variable names are local to the function. This is called the scope of the variable. All variables have the scope of the block they are declared in starting from the point of definition of the name.

x = 50

def func(x):
    print('x is', x) # Output: x is 50
    x = 2
    print('Changed local x to', x) # Output: Changed local x to 2

func(x)
print('x is still', x) # Output: x is still 50

--
(findglobal)
Global statement

If you want to assign a value to "a name defined at the top level of the program" (i.e. not inside any kind of scope such as functions or classes), then you have to tell Python that the name is not local, but it is global. We do this using the global statement. It is impossible to assign a value to a variable defined outside a function without the global statement.

x = 50

def func():
    global x
    print('x is', x) # Output: x is 50
    x = 2
    print('Changed global x to', x) # Output: Changed global x to 2

func()
print('Value of x is', x) # Output: Value of x is 2

--
(findnonlocal)
Nonlocal statement

We have seen how to access variables in the local and global scope above. There is another kind of scope called "nonlocal" scope which is in-between these two types of scopes. Nonlocal scopes are observed when you define functions inside functions. Since everything in Python is just executable code, you can define functions anywhere.

def func_outer():
    x = 2
    print('x is', x) # Output: x is 2

    def func_inner():
        nonlocal x
        x = 5

    func_inner()

    print('Changed local x to', x) # Output: Changed local x to 5

func_outer()

--
(finddefaultargument)
Default Argument Values

For some functions, you may want to make some of its parameters as optional and use default values if the user does not want to provide values for such parameters. This is done with the help of default argument values.

Only those parameters which are at the end of the parameter list can be given default argument values. This is because the values are assigned to the parameters by position.

def say(message, times = 1):
    print(message * times)

say('Hello') # Output: Hello
say('World', 5) # Output: WorldWorldWorldWorldWorld

--
(findkeywordargument)
Keyword Arguments

If you have some functions with many parameters and you want to specify only some of them, then you can give values for such parameters by naming them - this is called keyword arguments - we use the name (keyword) instead of the position (which we have been using all along) to specify the arguments to the function.

We can give values to only those parameters which we want, provided that the other parameters have default argument values.

def func(a, b=5, c=10):
    print('a is', a, 'and b is', b, 'and c is', c)

func(3, 7) # Output: a is 3 and b is 7 and c is 10
func(25, c=24) # Output: a is 25 and b is 5 and c is 24
func(c=50, a=100) # Output: a is 100 and b is 5 and c is 50

--
(findvararg)
VarArgs parameters

Sometimes you might want to define a function that can take any number of parameters, this can be achieved by using the stars:

def total(initial=5, *numbers, **keywords):
    count = initial
    for number in numbers:
        count += number
    for key in keywords:
        count += keywords[key]
    return count

total(10, 1, 2, 3, vegetables=50, fruits=100)
      --  -------  -------------------------
  initial numbers              keywords
          (numbers is a list)  (keywords is a dictionary)


When we declare a starred parameter such as *param, then all the positional arguments from that point till the end are collected as a list called 'param'.

Similarly, when we declare a double-starred parameter such as **param, then all the keyword arguments from that point till the end are collected as a dictionary called 'param'.

--
(findkeywordonly)
Keyword- only Parameters

If we want to specify certain keyword parameters to be available as keyword-only and not as positional arguments, they can be declared after a starred parameter:

def total(initial=5, *numbers, vegetables):
    count = initial
    for number in numbers:
        count += number
    count += vegetables
    return count

total(10, 1, 2, 3, vegetables=50)
total(10, 1, 2, 3) # Raises error because we have not supplied a default argument value for 'vegetables'

Declaring parameters after a starred parameter results in keyword-only arguments. If these arguments are not supplied a default value (Tao: in the function definition), then calls to the function will raise an error if the keyword argument is not supplied, as seen above.

If you want to have keyword-only arguments but have no need for a starred parameter, then simply use an empty star without using any name such as def total(initial=5, *, vegetables).

==
(findmodule)
Modules

Tao: modules in Python is like packages in Java and Scala.

What if you wanted to reuse a number of functions in other programs that you write? The answer is modules.

There are various methods of writing modules, but the simplest way is to create a file with a .py extension that contains functions and variables.

Another method is to write the modules in the native language in which the Python interpreter itself was written. For example, you can write modules in the C programming language and when compiled, they can be used from your Python code when using the standard Python interpreter.

A module can be imported by another program to make use of its functionality. This is how we can use the Python standard library as well. First, we will see how to use the standard library modules.

Example:

#!/usr/bin/python
# Filename: using_sys.py

import sys

print('The command line arguments are:')
for i in sys.argv:
    print(i)

print('\n\nThe PYTHONPATH is', sys.path, '\n')

$ python using_sys.py we are arguments

The command line arguments are:
using_sys.py
we
are
arguments
The PYTHONPATH is ['', 'C:\\Windows\\system32\\python30.zip',
'C:\\Python30\\DLLs', 'C:\\Python30\\lib',
'C:\\Python30\\lib\\plat-win', 'C:\\Python30',
'C:\\Python30\\lib\\site-packages']

When Python executes the import sys statement, it looks for the sys module. In this case, it is one of the built-in modules, and hence Python knows where to find it.

If it was not a compiled module i.e. a module written in Python, then the Python interpreter will search for it in the directories listed in its sys.path variable. If the module is found, then the statements in the body of that module is run and then the module is made available for you to use. Note that the initialization is done only the first time that we import a module.

The sys.argv variable is a list of strings. Python stores the command line arguments in the sys.argv variable for us to use.

Remember, the name of the script running is always the first argument in the sys.argv list. Notice that Python starts counting from 0 and not 1.

The sys.path contains the list of directory names where modules are imported from. Observe that the first string in sys.path is empty - this empty string indicates that the current directory is also part of the sys.path which is same as the PYTHONPATH environment variable. This means that you can directly import modules located in the current directory. Otherwise, you will have to place your module in one of the directories listed in sys.path. 

--
from . . . import . . .

Tao: in the above, even you imported sys, you still need to type sys.argv rather than only argv each time.

If you want to directly import the argv variable into your program (to avoid typing the
sys. everytime for it), then you can use the 
from sys import argv 
statement. 

If you want to import all the names used in the sys module, then you can use the 
from sys import *
statement. This works for any module. In general, you should avoid using this statement and use the import statement instead since your program will avoid name clashes and will be more readable.

--
Making Your Own Modules

Creating your own modules is easy, you've been doing it all along! This is because every
Python program is also a module. You just have to make sure it has a .py extension.

Example:

#!/usr/bin/python
# Filename: mymodule.py

def sayhi():
    print('Hi, this is mymodule speaking.')

__version__ = '0.1'

# End of mymodule.py

The above was a sample module. As you can see, there is nothing particularly special about
compared to our usual Python program.

Remember that the module should be placed in the same directory as the program that we
import it in, or the module should be in one of the directories listed in sys.path.

#!/usr/bin/python
# Filename: mymodule_demo.py

import mymodule
mymodule.sayhi()

print ('Version', mymodule.__version__)

$ python mymodule_demo.py
Hi, this is mymodule speaking.
Version 0.1

Here is a version utilising the from..import syntax:
#!/usr/bin/python
# Filename: mymodule_demo2.py

from mymodule import sayhi, __version__

sayhi()
print('Version', __version__)

You could also use:
from mymodule import *
This will import all public names such as sayhi but would not import __version__
because it starts with double underscores.

--
How to import module from different folder (from online):

By default, you can't. When importing a file, Python only searches the current directory, the directory that the entry-point script is running from, and sys.path which includes locations such as the package installation directory (it's actually a little more complex than this, but this covers most cases).

However, you can add to the Python path at runtime:

# some_file.py
import sys
sys.path.append('/path/to/application/app/folder')

import file

==
Byte- compiled .pyc files

Importing a module is a relatively costly affair, so Python does some tricks to make it faster. One way is to create byte-compiled files with the extension .pyc which is an intermediate form that Python transforms the program into. This .pyc file is useful when you import the module the next time from a different program - it will be much faster since a portion of the processing required in importing a module is already done. Also, these byte-compiled files are platform-independent.

--
_ _ name_ _

Every module has a name. This is handy in the particular situation of figuring out if the module is being run standalone or being imported. As mentioned previously, when a module is imported for the first time, the code in that module is executed. We can use this concept to alter the behavior of the module if the program was used by itself and not when it was imported from another module. This can be achieved using the __name__ attribute of the module.

Example:
#!/usr/bin/python
# Filename: using_name.py
if __name__ == '__main__':
    print('This program is being run by itself')
else:
    print('I am being imported from another module')

$ python using_name.py
This program is being run by itself

$ python
>>> import using_name
I am being imported from another module

--
dir

When you supply a module name to the dir() function, it returns the list of the names
defined in that module. When no argument is applied to it, it returns the list of names
defined in the current module.

==
Packages

What if you wanted to organize modules? That's where packages come into the
picture.

Packages are just folders of modules with a special __init__.py file that indicates to
Python that this folder is special because it contains Python modules.

==
Data Structures

There are four built-in data structures in Python - list, tuple, dictionary and set.

tuple用(), list用[], dictionary用{}, 它們依次是 小中大 括號

Python uses 0-based indexing
R uses 1-based indexing

==
(findtuple)
tuple

Tao: tuple is like array in C++

tuples are immutable.

The pair of parentheses in tuples is optional.

zoo = ('monkey', 'tiger', 'cat')
a = len(zoo)
b = zoo[0]
print(zoo)

newZoo = ('pig', 'dog', zoo) # newZoo = ('monkey', 'camel', ('python', 'elephant', 'penguin'))
newZoo[2] # Returns ('python', 'elephant', 'penguin')
newZoo[2][2] # Returns 'penguin'
len(newZoo)
len(newZoo[2])

emptyTuple = ()
singleton = (2, ) # A tuple containing the item 2 (only one item). Must define like this. (2) means a pair of parentheses surrounding the object in an expression (tao: like an arithmetic expression (2 + 1)).

# passing tuples around:
def func():
    return (3, 'helo')

num, s = func()

Can define a tuple within a tuple, or a tuple within a list, or a list within a tuple.

==
(findlist)
list

Tao: list is like vector in C++

Lists are mutatble. 
You can add, remove or search for items in the list. we say that a list is a mutable data type

The list of items should be enclosed in square brackets.

odd = [1, 3, 5]
odd.append(7)
print(odd) # Output: [1, 3, 5, 7]

shoplist = ['apple', 'mango', 'orange']
a = shoplist[0]
b = len(shoplist)

Check if a list is empty (this is also what all people do online, it looks like there is no isEmpty function for list):

if len(my_list) == 0:
    pass

shoplist.append('banana')
shoplist.sort()
print(shoplist)

for item in shoplist:
    print(item, end=' ') #加end=' '是因為print()函數會自動在 print出的東西的 末尾加一個newline character, end=' '的作用就是將這個newline character換成空格.

del shoplist[0] #將shoplist的第0個元素 從shoplist中 刪掉

# using lists as stacks (from online):
stack = [3, 4, 6]
stack.append(6) #相當於push <- It adds 6 to the original list "stack", and it returns nothing.
stack.pop()

stackNew = stack + [40] <- This returns a new list

# using lists as queues (from online, many people do this):
from collections import deque
queue = deque(["Eric", "John", "Michael"])
queue.append("Terry") #相當於offer
left = queue.popleft() #相當於poll

list1 = [3, 4, 6]
a = sum(list1)
b = max(list1)
c = min(list1)

# empty list
my_list = []

==
(finddictionary)
dictionary

Tao: dictionary is like map in C++

ages = {'John' : 30, 'Mary' : 40}
a = ages['John']
b = len(ages)
del ages['John']
ages['Mary'] = 20 # update
ages['Kate'] = 40 # add an item
new_dict = {} # Create an empty dictionary

# Traverse a dictionary:
for name, age in ages.items(): #不是 for name : age ...
    print('{0} is {1} years old'.format(name, age))

# Existence of key in dictionary:
if 'Mary' in ages: # if ages contains key 'Mary'
    print('Has Mary.')

Remember that key-value pairs in a dictionary are not ordered in any manner. If you want a particular order, then you will have to sort them yourself before using it.

==
list and dictionary as function parameter

def total(count, *numbers, **keywords):
    for i in numbers:
        count += i
    for key in keywords:
        count += keywords[key]
    return count

print(total(0, 1, 2, 3, John=4, Jack=5))

==
(findset)
set

bric = set(['brazil', 'russia', 'india']) # A set is initialized from a list
'india' in bric # 相當於Java中的bri.contains('india')
bric.add('china')
bric.remove('china')
bric.issuperset(bri) # bri is also a set
bri & bric # OR bri.intersection(bric)
bricNew = bric.copy()

Sets are unordered.

==
(findsequence)
sequences 

Lists, tuples and strings are examples of sequences. The major features of sequences is that they have membership tests (i.e. the in and not in expressions) and indexing operations. 

Sequences also have a slicing operation which allows us to retrieve a slice of the sequence i.e. a part of the sequence.

name = 'jan'
name[-1] # Returns 'n'
name[4] # 報錯: IndexError: string index out of range
name[1:3]  #包括name[1], 但不包括name[3]
name[2:]
name[:] # a copy of the whole sequence
name[::2] # 2 is the step
name[:-1] # Returns a slice of the sequence which excludes the last item of the sequence but contains everything else.
name[::-1] # Reverse the sequence


==
(findclass)
class

Tao: about self:
1. In a class, all member methods should have a self parameter: def funcName(self, x, y). When calling this method, no need to insantiate this self parameter: funcName(2, 3).
2. In a class, when using its own member variables or methods, should add self: self.variableName, self.func(). This is true even when a member function in a class calls itself recursively, it still needs to add self.

Tao: in a class, no need to define its member variables. Any variables can jump out suddenly like the self.name below.

Tao: there is no way to overload __init__ method in Python (confirmed from online). We need to use tricks to overload it or avoid having the desire to overload it.

# Example class 1:

class Person:
    # 這是constructor. self相當於Java中的this:
    def __init__(self, name): 
        self.name = name 
    
    def sayHi(self):
        print('Hi', self.name) 
    
    #def sayHello(): //Avadoles!!! 報錯, 因為所有class method必加self參數. 實踐表明, static methd不用加self參數
    #    print('Hello')

p = Person('John')
p.sayHi()

# Example class 2:

class Animal():
    pass # An empty block

anim = Animal()

Self: although, you can give any name for this parameter, it is strongly recommended that you
use the name self.

All class memembers (including the data memebers) are public. If you use data members with names using the double underscore prefix such as __privatevar, Python uses name-mangling to effectively make it a private variable.

fields vs variables, methods vs functions:

Variables that belong to an object or class are referred to as fields. Functions that belong to a class: such functions are called methods of the class. This terminology is important because it helps us to differentiate between functions and variables which are independent and those which belong to a class or object.

Tao: Functions can be out of class in Python.

--
Class And Object Variables

There are two types of fields - class variables and object variables:

Class variables are shared - they can be accessed by all instances of that class. There is only one copy of the class variable and when any one object makes a change to a class variable, that change will be seen by all the other instances. Tao: this is like static variable in Java.

Object variables are owned by each individual object/instance of the class. In this case, each object has its own copy of the field i.e. they are not shared and are not related in any way to the field by the same name in a different instance.

Example:

class Robot:
    # A class variable, counting the number of robots
    # Tao: therefore, do not explicitly define variables in a class unless you want to make it a class variable
    population = 0

    def __init__(self, name):
        self.name = name # Tao: self.name is an object variable
        print('Initializing {0}'.format(self.name))

        # When this person is created, the robot adds to the population
        Robot.population += 1

    def __del__(self):
        print('{0} is being destroyed!'.format(self.name))

        Robot.population -= 1

        if Robot.population == 0:
            print('{0} was the last one.'.format(self.name))
        else:
            print('There are still {0:d} robots working.'.format(Robot.population))

    def howMany():
        print('We have {0:d} robots.'.format(Robot.population))

    howMany = staticmethod(howMany)

droid1 = Robot('R2-D2') #Output: Initializing R2-D2)
Robot.howMany() #Output: We have 1 robots.

droid2 = Robot('C-3PO') #Output: Initializing C-3PO)
Robot.howMany() #Output: We have 2 robots.

del droid1 #Output: R2-D2 is being destroyed! (newline) There are still 1 robots working.
del droid2 #Output: C-3PO is being destroyed! (newline) C-3PO was the last one.
Robot.howMany() #Output: We have 0 robots.

Here, population belongs to the Robot class and hence is a class variable. The name variable belongs to the object (it is assigned using self) and hence is an object variable.

Thus, we refer to the population class variable as Robot.population and not as self.population. We refer to the object variable name using self.name notation in the methods of that object.

The howMany is actually a method that belongs to the class and not to the object. This means we can define it as either a classmethod or a staticmethod depending on whether we need to know which class we are part of. Since we don't need such information, we will go for staticmethod.

We could have also achieved the same using decorators. Decorators can be imagined to be a shortcut to calling an explicit statement:

@staticmethod
def howMany():
    print('We have {0:d} robots.'.format(Robot.population))

(finddestructor)
The __del__ method (see example above) is run when the object is no longer in use and there is no guarantee when that method will be run. If you want to explicitly see it in action, we have to use the del statement which is what we have done here.

==
(findinheritance)
Inheritance

       SchoolMember 
            |
     ------------------        
    |                  |
 Teacher             Student

class SchoolMember:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    def tell(self):
        print('Name:"{0}" Age:"{1}"'.format(self.name, self.age), end=" ")

class Teacher(SchoolMember):
    def __init__(self, name, age, salary):
        SchoolMember.__init__(self, name, age)
        self.salary = salary

    def tell(self):
        SchoolMember.tell(self)
        print('Salary: "{0:d}"'.format(self.salary))

class Student(SchoolMember):
    def __init__(self, name, age, marks):
        SchoolMember.__init__(self, name, age)
        self.marks = marks

    def tell(self):
        SchoolMember.tell(self)
        print('Marks: "{0:d}"'.format(self.marks))

t = Teacher('Mrs. Shrividya', 40, 30000)
s = Student('Swaroop', 25, 75)

members = [t, s]

for member in members:
    member.tell() # works for both Teachers and Students

# Output:
Name:"Mrs. Shrividya" Age:"40" Salary: "30000"
Name:"Swaroop" Age:"25" Marks: "75"

Inherit from multiply classes: class Teacher(SchoolMember, Buyer, UncleFucker)

This is very important to remember - Python does not automatically call the constructor of the base class, you have to explicitly call it yourself.

Best illurstration of polymorphism:
You can refer to a Teacher or Student object as a SchoolMember object which could be useful in some situations such as counting of the number of school members. This is called polymorphism where a sub-type can be substituted in any situation where a parent type is expected i.e. the object can be treated as an instance of the parent class.

==
(findgetter)
(findsetter)
Getters and setters:

What's the pythonic way to use getters and setters?

The "Pythonic" way is not to use "getters" and "setters", but to use plain attributes, like the question demonstrates, and del for dereferencing (but the names are changed to protect the innocent... builtins).

The sample code is:

class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        print("getter of x called")
        return self._x

    @x.setter
    def x(self, value):
        print("setter of x called")
        self._x = value

    @x.deleter
    def x(self):
        print("deleter of x called")
        del self._x


c = C()
c.x = 'foo'  # setter called
foo = c.x    # getter called
del c.x      # deleter called

==
References

When you create an object and assign it to a variable, the variable only refers to the object and does not represent the object itself. Tao: same as Java.

When you create an object and assign it to a variable, the variable only refers to the object and does not represent the object itself! That is, the variable name points to that part of your computer's memory where the object is stored. This is called as binding of the name to the object.

Remember that an assignment statement for lists does not create a copy. You have to use slicing operation to make a copy of the sequence.

The time.strftime() function takes a specification such as the one we have used in the
above program. The %Y specification will be replaced by the year without the century. The
%m specification will be replaced by the month as a decimal number between 01 and 12 and
so on.

==
(findexception)
Exceptions

We put all the statements that might raise exceptions/errors inside the try block and then put handlers for the appropriate errors/exceptions in the except clause/block. The except clause can handle a single specified error or exception, or a parenthesized list of errors/exceptions. If no names of errors or exceptions are supplied, it will handle all errors and exceptions.

Note that there has to be at least one except clause associated with every try clause. Otherwise, what's the point of having a try block?

If any error or exception is not handled, then the default Python handler is called which just
stops the execution of the program and prints an error message.

You can also have an else clause associated with a try..except block. The else clause is executed if no exception occurs.

try:
    text = input('Enter something --> ')
except EOFError:
    print('Why did you do an EOF on me?')
except KeyboardInterrupt:
    print('You cancelled the operation.')
else:
    print('You entered {0}'.format(text))

--
Raising Exceptions

You can raise exceptions using the raise statement by providing the name of the error/exception and the exception object that is to be thrown.

The error or exception that you can arise should be class which directly or indirectly must be a derived class of the Exception class.


# A user-defined exception class.'''
class ShortInputException(Exception):
    def __init__(self, length, atleast):
        Exception.__init__(self)
        self.length = length
        self.atleast = atleast

    try:
        text = input('Enter something --> ')
        if len(text) < 3:
            raise ShortInputException(len(text), 3)
        # Other work can continue as usual here

    except EOFError:
        print('Why did you do an EOF on me?')

    except ShortInputException as ex:
        print('ShortInputException: The input was {0} long, expected at least {1}'.format(ex.length, ex.atleast))

    else:
        print('No exception was raised.')

--
Finally

Suppose you are reading a file in your program. How do you ensure that the file object is closed properly whether or not an exception was raised? This can be done using the finally block.

In the following, observe that the KeyboardInterrupt exception is thrown and the program quits. However, before the program exits, the finally clause is executed and the file object is always closed.

import time

try:
    f = open('poem.txt')
    while True: # our usual file-reading idiom
        line = f.readline()
        if len(line) == 0:
            break
        print(line, end='')
        time.sleep(2) # To make sure it runs for a while

except KeyboardInterrupt:
    print('!! You cancelled the reading from the file.')

finally:
    f.close()
    print('(Cleaning up: Closed the file)')

--
The with statement

Acquiring a resource in the try block and subsequently releasing the resource in the finally block is a common pattern. Hence, there is also a with statement that enables this to be done in a clean manner:

with open("poem.txt") as f:
    for line in f:
        print(line, end='')

The output should be same as the previous example (the example of finally block). The difference here is that we are using the open function with the with statement - we leave the closing of the file to be done automatically by with open.

==
Standard Library

sys module

The sys module contains system-specific functionality. We have already seen that the sys.argv list contains the command-line arguments.

Suppose we want to check the version of the Python command being used. The first entry is the major version.

>>> import sys
>>> sys.version_info
(3, 0, 0, 'beta', 2)
>>> sys.version_info[0] >= 3
True

Ensure the program runs only under Python 3.0. We use another module from the standard library called warnings that is used to display warnings to the end-user:

import sys, warnings

if sys.version_info[0] < 3:
    warnings.warn("Need Python 3.0 for this program to run",
        RuntimeWarning)
else:
    print('Proceed as normal')

Output:

$ python2.5 versioncheck.py
versioncheck.py:6: RuntimeWarning: Need Python 3.0 for this program to run RuntimeWarning)

$ python3 versioncheck.py
Proceed as normal

--
logging module
(Tao: not important, can skip)

What if you wanted to have some debugging messages or important messages to be stored somewhere so that you can check whether your program has been running as you would expect it? How do you "store somewhere" these messages? This can be achieved using the logging module.

import os, platform, logging

if platform.platform().startswith('Windows'):
    logging_file = os.path.join(os.getenv('HOMEDRIVE'), os.getenv('HOMEPATH'), 'test.log')
else:
    logging_file = os.path.join(os.getenv('HOME'), 'test.log')

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s : %(levelname)s : %(message)s',
    filename = logging_file,
    filemode = 'w',
)

logging.debug("Start of the program")
logging.info("Doing something")
logging.warning("Dying now")

Output:
$python use_logging.py
Logging to C:\Users\swaroop\test.log

If we check the contents of test.log, it will look something like this:
2008-09-03 13:18:16,233 : DEBUG : Start of the program
2008-09-03 13:18:16,233 : INFO : Doing something
2008-09-03 13:18:16,233 : WARNING : Dying now

We use three modules from the standard library - the os module for interacting with the operating system, the platform module for information about the platform i.e. the operating system and the logging module to log information.

We use the os.path.join() function to put these three parts of the location together. The reason to use a special function rather than just adding the strings together is because this function will ensure the full location matches the format expected by the operating system.

Once the program has run, we can check this file and we will know what happened in the program, even though no information was displayed to the user running the program.

--
urllib and json modules
(Tao: not important, can skip)

How much fun would it be if we could write our own program that will get search results from the web? Let us explore that now.

This can be achieved using a few modules. First is the urllib module that we can use to fetch any webpage from the internet. We will make use of Yahoo! Search to get the search results and luckily they can give us the results in a format called JSON which is easy for us to parse because of the built-in json module in the standard library.

import sys

if sys.version_info[0] != 3:
    sys.exit('This program needs Python 3.0')

import json
import urllib, urllib.parse, urllib.request, urllib.response

# Get your own APP ID at http://developer.yahoo.com/wsregapp/

YAHOO_APP_ID =
'jl22psvV34HELWhdfUJbfDQzlJ2B57KFS_qs4I8D0Wz5U5_yCI1Awv8.lBSfPhwr'

SEARCH_BASE =
'http://search.yahooapis.com/WebSearchService/V1/webSearch'

class YahooSearchError(Exception):
    pass

# Taken from http://developer.yahoo.com/python/python-json.html
def search(query, results=20, start=1, **kwargs):
    kwargs.update({
        'appid': YAHOO_APP_ID,
        'query': query,
        'results': results,
        'start': start,
        'output': 'json'
})

url = SEARCH_BASE + '?' + urllib.parse.urlencode(kwargs)
result = json.load(urllib.request.urlopen(url))

if 'Error' in result:
    raise YahooSearchError(result['Error'])
return result['ResultSet']

query = input('What do you want to search for? ')

for result in search(query)['Result']:
    print("{0} : {1}".format(result['Title'], result['Url']))


==
More

Passing tuples around

Ever wished you could return two different values from a function? You can. All you have to do is use a tuple.

>>> def get_error_details():
... return (2, 'second error details')
...

>>> errnum, errstr = get_error_details()
>>> errnum
2

>>> errstr
'second error details'

Notice that the usage of a, b = <some expression> interprets the result of the expression as a tuple with two values.

If you want to interpret the results as (a, <everything else>), then you just need to star it just like you would in function parameters:

>>> a, *b = [1, 2, 3, 4]
>>> a
1
>>> b
[2, 3, 4]

This also means the fastest way to swap two variables in Python is:
>>> a = 5; b = 8
>>> a, b = b, a
>>> a, b
(8, 5)

--
Special Methods

There are certain methods such as the __init__ and __del__ methods which have special significance in classes.

Special methods are used to mimic certain behaviors of built-in types. For example, if you want to use the x[key] indexing operation for your class (just like you use it for lists and tuples), then all you have to do is implement the __getitem__() method and your job is done. If you think about it, this is what Python does for the list class itself! Some useful special methods are listed in the following table:

__init__(self, ...): This method is called just before the newly created object is returned for usage.

__del__(self): Called just before the object is destroyed

__str__(self): Called when we use the print function or when str() is used.

__lt__(self, other): Called when the less than operator (<) is used. Similarly, there are special
methods for all the operators (+, >, etc.)

__getitem__(self, key): Called when x[key] indexing operation is used.

__len__(self): Called when the built-in len() function is used for the sequence object.

--
Single Statement Blocks

In an if or loop, if the body has only one line, then can put this line in the same line as if or for:

>>> flag = True
>>> if flag: print 'Yes'

I strongly recommend avoiding this short-cut method, except for error checking.

--
Lambda Forms

A lambda statement is used to create new function objects and then return them at runtime.

Tao: in the below:
1. make_repeater returns a function object.
2. This function object is created by the lambda statement. The s in the lambda s is the parameter of this function object. s * n is the function body.
3. twice is such a function object, so it can be called as other functions: twice(5).

def make_repeater(n):
    return lambda s: s * n

twice = make_repeater(2)

print(twice('word')) # Output: wordword
print(twice(5)) # Output: 10

--
List Comprehension

List comprehensions are used to derive a new list from an existing list. Suppose you have a list of numbers and you want to get a corresponding list with all the numbers multiplied by 2 only when the number itself is greater than 2. List comprehensions are ideal for such situations.

listone = [2, 3, 4]
listtwo = [2*i for i in listone if i > 2]
print(listtwo) # Output: [6, 8]

--
exec and eval

The exec function is used to execute Python statements which are stored in a string or file, as opposed to written in the program itself. For example, we can generate a string containing Python code at runtime and then execute these statements using the exec statement:

>>> exec('print("Hello World")')
Hello World

Similarly, the eval function is used to evaluate valid Python expressions which are stored in a string. A simple example is shown below.

>>> eval('2*3')
6

--
The assert statement

The assert statement is used to assert that something is true. For example, if you are very sure that you will have at least one element in a list you are using and want to check this, and raise an error if it is not true, then assert statement is ideal in this situation. When the assert statement fails, an AssertionError is raised.

>>> mylist = ['item']
>>> assert len(mylist) >= 1
>>> mylist.pop()
'item'
>>> mylist
[]
>>> assert len(mylist) >= 1
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AssertionError

The assert statement should be used judiciously. Most of the time, it is better to catch exceptions, either handle the problem or display an error message to the user and then quit.

--
The repr function

The repr function is used to obtain a canonical string representation of the object. The    interesting part is that you will have eval(repr(object)) == object most of the time.

>>> i = []
>>> i.append('item')
>>> repr(i)
"['item']"
>>> eval(repr(i))
['item']
>>> eval(repr(i)) == i
True

Basically, the repr function is used to obtain a printable representation of the object. You can control what your classes return for the repr function by defining the __repr__ method in your class.

--
(findsleep)
sleep

The method sleep() suspends execution for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time.

import time
time.sleep(5) # Sleep for 5 seconds.

--
(findtime)
Output current local time

import datetime
time_str = (datetime.datetime.now() - datetime.timedelta(hours = 5)).strftime("%H:%M, %Y-%m-%d") # Minus 5 hours, may not be necessary.
print('Chicago time: {0}\n'.format(time_str)) # Output: Chicago time: 16:33, 2018-08-01

--
t1 = time.time()
print "ml4t"
t2 = time.time()
print "The time taken by print statement is ", t2 - t1, "seconds"


==
others:

# Python程序的文件名不用跟class名一樣, 甚至程序裡可以不含class.

# When you create an objet and assign it to a variable, the variable only refers to the object and does not represent the object itself.

# Python is strongly object-oriented in the sense that everything is an object including numbers, strings and functions.

# Python中的函數是可以在class之外的

# swap two variables:
# a = 5; b = 8
# a, b = b, a

# Read a list of numbers (in string form) from input and convert them into a list of int (from HackerRank):
# arr = [int(arr_temp) for arr_temp in input().strip().split(' ')]  # 此句中strip()還可以刪掉輸入末尾的newline, 當然同時也刪空格(from Haddop課).
# now arr is a list of int

# abs(-45), abs(100.12)

# 實踐表明, Python中連注釋都要正式indent, 否則報錯

==
The current date and time which we find out using the time.strftime() function. (import time).

Notice the use of os.sep variable (import os) - this gives the directory separator according to your
operating system i.e. it will be '/' in Linux, Unix, it will be '\\' in Windows and ':' in
Mac OS. Using os.sep instead of these characters directly will make our program portable
and work across these systems.

==
The zip command that we are using has some options and parameters passed. The -q
option is used to indicate that the zip command should work quietly. The -r option
specifies that the zip command should work recursively for directories i.e. it should include
all the subdirectories and files. The two options are combined and specified in a shortcut as
-qr. The options are followed by the name of the zip archive to create followed by the list of
files and directories to backup. We convert the source list into a string using the join
method of strings which we have already seen how to use.

Then, we finally run the command using the os.system function which runs the command
as if it was run from the system i.e. in the shell - it returns 0 if the command was
successfully, else it returns an error number.

==
target = today + os.sep + now + '_' + \ comment.replace(' ', '_') + '.zip'

===
# Create the subdirectory if it isn't already there
if not os.path.exists(today): #t oday is a string defined earlier.
    os.mkdir(today) # make directory

=====================================================================

=====
(findnumpy)
NumPy

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264371&utm_targetid=aud-299261629574:dsa-473406585115&utm_loc_interest_ms=&utm_loc_physical_ms=1016367&gclid=Cj0KCQiAn8nuBRCzARIsAJcdIfNaldds0HRvuMY0tIWi-Cx2HTJNxH6C9Isjr1criuRR-7s0TIGKfDIaAuOcEALw_wcB

Python Numpy Array Tutorial

A NumPy tutorial for beginners in which you'll learn how to create a NumPy array, use broadcasting, access values, manipulate arrays, and much more.

NumPy is, just like SciPy, Scikit-Learn, Pandas, etc. one of the packages that you just can’t miss when you’re learning data science, mainly because this library provides you with an array data structure that holds some benefits over Python lists, such as: being more compact, faster access in reading and writing items, being more convenient and more efficient.

Today’s post will focus precisely on this. This NumPy tutorial will not only show you what NumPy arrays actually are and how you can install Python, but you’ll also learn how to make arrays (even when your data comes from files!), how broadcasting works, how you can ask for help, how to manipulate your arrays and how to visualize them.

Content
What Is A Python Numpy Array?
How To Install Numpy
How To Make NumPy Arrays
How NumPy Broadcasting Works
How Do Array Mathematics Work?
How To Subset, Slice, And Index Arrays
How To Ask For Help
How To Manipulate Arrays
How To Visualize NumPy Arrays
Beyond Data Analysis with NumPy

What Is A Python Numpy Array?

You already read in the introduction that NumPy arrays are a bit like Python lists, but still very much different at the same time. For those of you who are new to the topic, let’s clarify what it exactly is and what it’s good for.

As the name gives away, a NumPy array is a central data structure of the numpy library. The library’s name is short for “Numeric Python” or “Numerical Python”.

This already gives an idea of what you’re dealing with, right?

In other words, NumPy is a Python library that is the core library for scientific computing in Python. It contains a collection of tools and techniques that can be used to solve on a computer mathematical models of problems in Science and Engineering. One of these tools is a high-performance multidimensional array object that is a powerful data structure for efficient computation of arrays and matrices. To work with these arrays, there’s a vast amount of high-level mathematical functions operate on these matrices and arrays.

Then, what is an array?

When you look at the print of a couple of arrays, you could see it as a grid that contains values of the same type:

# Print the array
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 2d array
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 3d array
print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

You see that, in the example above, the data are integers. The array holds and represents any regular data in a structured way.

However, you should know that, on a structural level, an array is basically nothing but pointers. It’s a combination of a memory address, a data type, a shape, and strides:

- The data pointer indicates the memory address of the first byte in the array,

- The data type or dtype pointer describes the kind of elements that are contained within the array,

- The shape indicates the shape of the array, and

- The strides are the number of bytes that should be skipped in memory to go to the next element. If your strides are (10,1), you need to proceed one byte to get to the next column and 10 bytes to locate the next row.

Or, in other words, an array contains information about the raw data, how to locate an element and how to interpret an element.

Enough of the theory. Let’s check this out ourselves:

You can easily test this by exploring the numpy array attributes:

# Print out memory address
print(my_2d_array.data)

<memory at 0x7f58cb57fa68>

# Print out the shape of `my_array`
print(my_2d_array.shape)

(2, 4)

# Print out the data type of `my_array`
print(my_2d_array.dtype)

int64

# Print out the stride of `my_array`
print(my_2d_array.strides)

(32, 8)

You see that now, you get a lot more information: for example, the data type that is printed out is ‘int64’ or signed 32-bit integer type; This is a lot more detailed! That also means that the array is stored in memory as 64 bytes (as each integer takes up 8 bytes and you have an array of 8 integers). The strides of the array tell us that you have to skip 8 bytes (one value) to move to the next column, but 32 bytes (4 values) to get to the same position in the next row. As such, the strides for the array will be (32,8).

Note that if you set the data type to int32, the strides tuple that you get back will be (16, 4), as you will still need to move one value to the next column and 4 values to get the same position. The only thing that will have changed is the fact that each integer will take up 4 bytes instead of 8.

(A picture)

The array that you see above is, as its name already suggested, a 2-dimensional array: you have rows and columns. The rows are indicated as the “axis 0”, while the columns are the “axis 1”. The number of the axis goes up accordingly with the number of the dimensions: in 3-D arrays, of which you have also seen an example in the previous code chunk, you’ll have an additional “axis 2”. Note that these axes are only valid for arrays that have at least 2 dimensions, as there is no point in having this for 1-D arrays;

These axes will come in handy later when you’re manipulating the shape of your NumPy arrays.

How To Install Numpy

Before you can start to try out these NumPy arrays for yourself, you first have to make sure that you have it installed locally (assuming that you’re working on your pc). If you have the Python library already available, go ahead and skip this section :)

If you still need to set up your environment, you must be aware that there are two major ways of installing NumPy on your pc: with the help of Python wheels or the Anaconda Python distribution.

… With Python Wheels

Make sure firstly that you have Python installed. You can go here if you still need to do this :)

If you’re working on Windows, make sure that you have added Python to the PATH environment variable. Then, don’t forget to install a package manager, such as pip, which will ensure that you’re able to use Python’s open-source libraries.

Note that recent versions of Python 3 come with pip, so double check if you have it and if you do, upgrade it before you install NumPy:

pip install pip --upgrade
pip --version

Next, you can go here or here to get your NumPy wheel. After you have downloaded it, navigate to the folder on your pc that stores it through the terminal and install it:

install "numpy-1.9.2rc1+mkl-cp34-none-win_amd64.whl"
import numpy
numpy.__version__

The two last lines allow you to verify that you have installed NumPy and check the version of the package.

After these steps, you’re ready to start using NumPy!

… With The Anaconda Python Distribution

To get NumPy, you could also download the Anaconda Python distribution. This is easy and will allow you to get started quickly! If you haven’t downloaded it already, go here to get it. Follow the instructions to install, and you're ready to start!

Do you wonder why this might actually be easier?

The good thing about getting this Python distribution is the fact that you don’t need to worry too much about separately installing NumPy or any of the major packages that you’ll be using for your data analyses, such as pandas, scikit-learn, etc.

Because, especially if you’re very new to Python, programming or terminals, it can really come as a relief that Anaconda already includes 100 of the most popular Python, R and Scala packages for data science. But also for more seasoned data scientists, Anaconda is the way to go if you want to get started quickly on tackling data science problems.

What’s more, Anaconda also includes several open source development environments such as Jupyter and Spyder. If you’d like to start working with Jupyter Notebook after this tutorial, go to this page.

In short, consider downloading Anaconda to get started on working with numpy and other packages that are relevant to data science!

How To Make NumPy Arrays

So, now that you have set up your environment, it’s time for the real work. Admittedly, you have already tried out some stuff with arrays in the above DataCamp Light chunks. However, you haven’t really gotten any real hands-on practice with them, because you first needed to install NumPy on your own pc. Now that you have done this, it’s time to see what you need to do in order to run the above code chunks on your own.

Some exercises have been included below so that you can already practice how it’s done before you start on your own!

To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it, and optionally, you can also specify the data type of the data. If you want to know more about the possible data types that you can pick, go here or consider taking a brief look at DataCamp’s NumPy cheat sheet.

There’s no need to go and memorize these NumPy data types if you’re a new user; But you do have to know and care what data you’re dealing with. The data types are there when you need more control over how your data is stored in memory and on disk. Especially in cases where you’re working with extensive data, it’s good that you know to control the storage type.

Don’t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. The NumPy library follows an import convention: when you import this library, you have to make sure that you import it as np. By doing this, you’ll make sure that other Pythonistas understand your code more easily.

In the following example you’ll create the my_array array that you have already played around with above:

# Import `numpy` as `np`
import numpy as np

# Make the array `my_array`
my_array = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.int64)

# Print `my_array`
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

If you would like to know more about how to make lists, go here.

However, sometimes you don’t know what data you want to put in your array, or you want to import data into a numpy array from another source. In those cases, you’ll make use of initial placeholders or functions to load data from text into arrays, respectively.

The following sections will show you how to do this.

How To Make An “Empty” NumPy Array
What people often mean when they say that they are creating “empty” arrays is that they want to make use of initial placeholders, which you can fill up afterward. You can initialize arrays with ones or zeros, but you can also create arrays that get filled up with evenly spaced values, constant or random values.

However, you can still make a totally empty array, too.

Luckily for us, there are quite a lot of functions to make

Try it all out below!

# Create an array of ones
print(np.ones((3,4)))

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Create an array of zeros
print(np.zeros((2,3,4),dtype=np.int16))

[[[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]

 [[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]]

# Create an array with random values
print(np.random.random((2,2)))

[[ 0.4809319   0.39211752]
 [ 0.34317802  0.72904971]]

# Create an empty array
print(np.empty((3,2)))

[[ 0.  0.]
 [ 0.  0.]
 [ 0.  0.]]

# Create a full array
print(np.full((2,2),7))

[[ 7.  7.]
 [ 7.  7.]]

# Create an array of evenly-spaced values
print(np.arange(10,25,5))

[10 15 20]

# Create an array of evenly-spaced values
print(np.linspace(0,2,9))

[ 0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.  ]

Tip: play around with the above functions so that you understand how they work!

- For some, such as np.ones(), np.random.random(), np.empty(), np.full() or np.zeros() the only thing that you need to do in order to make arrays with ones or zeros is pass the shape of the array that you want to make. As an option to np.ones() and np.zeros(), you can also specify the data type. In the case of np.full(), you also have to specify the constant value that you want to insert into the array.

- With np.linspace() and np.arange() you can make arrays of evenly spaced values. The difference between these two functions is that the last value of the three that are passed in the code chunk above designates either the step value for np.linspace() or a number of samples for np.arange(). What happens in the first is that you want, for example, an array of 9 values that lie between 0 and 2. For the latter, you specify that you want an array to start at 10 and per steps of 5, generate values for the array that you’re creating.

Remember that NumPy also allows you to create an identity array or matrix with np.eye() and np.identity(). An identity matrix is a square matrix of which all elements in the principal diagonal are ones, and all other elements are zeros. When you multiply a matrix with an identity matrix, the given matrix is left unchanged.

In other words, if you multiply a matrix by an identity matrix, the resulting product will be the same matrix again by the standard conventions of matrix multiplication.

Even though the focus of this tutorial is not on demonstrating how identity matrices work, it suffices to say that identity matrices are useful when you’re starting to do matrix calculations: they can simplify mathematical equations, which makes your computations more efficient and robust.

How To Load NumPy Arrays From Text

Creating arrays with the help of initial placeholders or with some example data is an excellent way of getting started with numpy. But when you want to get started with data analysis, you’ll need to load data from text files.

With that what you have seen up until now, you won’t really be able to do much. Make use of some specific functions to load data from your files, such as loadtxt() or genfromtxt().

Let’s say you have the following text files with data:

# This is your data in the text file
# Value1  Value2  Value3
# 0.2536  0.1008  0.3857
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  0.5929
# 0.1781  0.3049  0.8928
# 0.6253  0.3486  0.8791

# Import your data
x, y, z = np.loadtxt('data.txt',
                    skiprows=1,
                    unpack=True)

In the code above, you use loadtxt() to load the data in your environment. You see that the first argument that both functions take is the text file data.txt. Next, there are some specific arguments for each: in the first statement, you skip the first row, and you return the columns as separate arrays with unpack=TRUE. This means that the values in column Value1 will be put in x, and so on.

Note that, in case you have comma-delimited data or if you want to specify the data type, there are also the arguments delimiter and dtype that you can add to the loadtxt() arguments.

That’s easy and straightforward, right?

Let’s take a look at your second file with data:

# Your data in the text file
# Value1  Value2  Value3
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  MISSING
# 0.1781  0.3049  0.8928
# MISSING 0.5801  0.2038
# 0.5993  0.4357  0.7410

my_array2 = np.genfromtxt('data2.txt',
                      skip_header=1,
                      filling_values=-999)

You see that here, you resort to genfromtxt() to load the data. In this case, you have to handle some missing values that are indicated by the 'MISSING' strings. Since the genfromtxt() function converts character strings in numeric columns to nan, you can convert these values to other ones by specifying the filling_values argument. In this case, you choose to set the value of these missing values to -999.

If by any chance, you have values that don’t get converted to nan by genfromtxt(), there’s always the missing_values argument that allows you to specify what the missing values of your data exactly are.

But this is not all.

Tip: check out this page to see what other arguments you can add to import your data successfully.

You now might wonder what the difference between these two functions really is.

The examples indicated this maybe implicitly, but, in general, genfromtxt() gives you a little bit more flexibility; It’s more robust than loadtxt().

Let’s make this difference a little bit more practical: the latter, loadtxt(), only works when each row in the text file has the same number of values; So when you want to handle missing values easily, you’ll typically find it easier to use genfromtxt().

But this is definitely not the only reason.

A brief look on the number of arguments that genfromtxt() has to offer will teach you that there is really a lot more things that you can specify in your import, such as the maximum number of rows to read or the option to automatically strip white spaces from variables.

How To Save NumPy Arrays

Once you have done everything that you need to do with your arrays, you can also save them to a file. If you want to save the array to a text file, you can use the savetxt() function to do this:

import numpy as np
x = np.arange(0.0,5.0,1.0)
np.savetxt('test.out', x, delimiter=',')

Remember that np.arange() creates a NumPy array of evenly-spaced values. The third value that you pass to this function is the step value.

There are, of course, other ways to save your NumPy arrays to text files. Check out the functions in the table below if you want to get your data to binary files or archives:

save(): Save an array to a binary file in NumPy .npy format

savez(): Save several arrays into an uncompressed .npz archive

savez_compressed(): Save several arrays into a compressed .npz archive

For more information or examples of how you can use the above functions to save your data, go here or make use of one of the help functions that NumPy has to offer to get to know more instantly!

Are you not sure what these NumPy help functions are?

No worries! You’ll learn more about them in one of the next sections!

How To Inspect Your NumPy Arrays

Besides the array attributes that have been mentioned above, namely, data, shape, dtype and strides, there are some more that you can use to easily get to know more about your arrays. The ones that you might find interesting to use when you’re just starting out are the following:

# Print the number of `my_array`'s dimensions
print(my_array.ndim)

2

# Print the number of `my_array`'s elements
print(my_array.size)

8

# Print information about `my_array`'s memory layout
print(my_array.flags)

C_CONTIGUOUS : True
F_CONTIGUOUS : False
OWNDATA : True
WRITEABLE : True
ALIGNED : True
UPDATEIFCOPY : False

# Print the length of one array element in bytes
print(my_array.itemsize)

8

# Print the total consumed bytes by `my_array`'s elements
print(my_array.nbytes)

64

These are almost all the attributes that an array can have.

Don’t worry if you don’t feel that all of them are useful for you at this point; This is fairly normal, because, just like you read in the previous section, you’ll only get to worry about memory when you’re working with large data sets.

Also note that, besides the attributes, you also have some other ways of gaining more information on and even tweaking your array slightly:

# Print the length of `my_array`
print(len(my_array))

2

# Change the data type of `my_array`
print(my_array.astype(float))

[[ 1.  2.  3.  4.]
 [ 5.  6.  7.  8.]]

Now that you have made your array, either by making one yourself with the np.array() or one of the initial placeholder functions, or by loading in your data through the loadtxt() or genfromtxt() functions, it’s time to look more closely into the second key element that really defines the NumPy library: scientific computing.

How NumPy Broadcasting Works

Before you go deeper into scientific computing, it might be a good idea to first go over what broadcasting exactly is: it’s a mechanism that allows NumPy to work with arrays of different shapes when you’re performing arithmetic operations.

To put it in a more practical context, you often have an array that’s somewhat larger and another one that’s slightly smaller. Ideally, you want to use the smaller array multiple times to perform an operation (such as a sum, multiplication, etc.) on the larger array.

To do this, you use the broadcasting mechanism.

However, there are some rules if you want to use it. And, before you already sigh, you’ll see that these “rules” are very simple and kind of straightforward!

- First off, to make sure that the broadcasting is successful, the dimensions of your arrays need to be compatible. Two dimensions are compatible when they are equal. Consider the following example:

# Initialize `x`
x = np.ones((3,4))

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.random.random((3,4))

print(y)

[[ 0.69646919  0.28613933  0.22685145  0.55131477]
 [ 0.71946897  0.42310646  0.9807642   0.68482974]
 [ 0.4809319   0.39211752  0.34317802  0.72904971]]

# Check shape of `y`
print(y.shape)

(3, 4)

# Add `x` and `y`
print(x + y)

[[ 1.69646919  1.28613933  1.22685145  1.55131477]
 [ 1.71946897  1.42310646  1.9807642   1.68482974]
 [ 1.4809319   1.39211752  1.34317802  1.72904971]]

- Two dimensions are also compatible when one of them is 1:

# Import `numpy` as `np`
import numpy as np

# Initialize `x`
x = np.ones((3,4))

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.arange(4)

print(y)

[0 1 2 3]

# Check shape of `y`
print(y.shape)

(4,)

# Subtract `x` and `y`
print(x - y)

[[ 1.  0. -1. -2.]
 [ 1.  0. -1. -2.]
 [ 1.  0. -1. -2.]]

Note that if the dimensions are not compatible, you will get a ValueError.

Tip: also test what the size of the resulting array is after you have done the computations! You’ll see that the size is actually the maximum size along each dimension of the input arrays.

In other words, you see that the result of x-y gives an array with shape (3,4): y had a shape of (4,) and x had a shape of (3,4). The maximum size along each dimension of x and y is taken to make up the shape of the new, resulting array.

-Lastly, the arrays can only be broadcast together if they are compatible in all dimensions. Consider the following example:

# Import `numpy` as `np`
import numpy as np

# Initialize `x` and `y`
x = np.ones((3,4))
print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

y = np.random.random((5,1,4))
print(y)

[[[ 0.63440096  0.84943179  0.72445532  0.61102351]]

 [[ 0.72244338  0.32295891  0.36178866  0.22826323]]

 [[ 0.29371405  0.63097612  0.09210494  0.43370117]]

 [[ 0.43086276  0.4936851   0.42583029  0.31226122]]

 [[ 0.42635131  0.89338916  0.94416002  0.50183668]]]

# Add `x` and `y`
print(x + y)

[[[ 1.63440096  1.84943179  1.72445532  1.61102351]
  [ 1.63440096  1.84943179  1.72445532  1.61102351]
  [ 1.63440096  1.84943179  1.72445532  1.61102351]]

 [[ 1.72244338  1.32295891  1.36178866  1.22826323]
  [ 1.72244338  1.32295891  1.36178866  1.22826323]
  [ 1.72244338  1.32295891  1.36178866  1.22826323]]

 [[ 1.29371405  1.63097612  1.09210494  1.43370117]
  [ 1.29371405  1.63097612  1.09210494  1.43370117]
  [ 1.29371405  1.63097612  1.09210494  1.43370117]]

 [[ 1.43086276  1.4936851   1.42583029  1.31226122]
  [ 1.43086276  1.4936851   1.42583029  1.31226122]
  [ 1.43086276  1.4936851   1.42583029  1.31226122]]

 [[ 1.42635131  1.89338916  1.94416002  1.50183668]
  [ 1.42635131  1.89338916  1.94416002  1.50183668]
  [ 1.42635131  1.89338916  1.94416002  1.50183668]]]

You see that, even though x and y seem to have somewhat different dimensions, the two can be added together.

That is because they are compatible in all dimensions:

- Array x has dimensions 3 X 4,
- Array y has dimensions 5 X 1 X 4

Since you have seen above that dimensions are also compatible if one of them is equal to 1, you see that these two arrays are indeed a good candidate for broadcasting!

What you will notice is that in the dimension where y has size 1, and the other array has a size greater than 1 (that is, 3), the first array behaves as if it were copied along that dimension.

Note that the shape of the resulting array will again be the maximum size along each dimension of x and y: the dimension of the result will be (5,3,4)

In short, if you want to make use of broadcasting, you will rely a lot on the shape and dimensions of the arrays with which you’re working.

But what if the dimensions are not compatible?

What if they are not equal or if one of them is not equal to 1?

You’ll have to fix this by manipulating your array! You’ll see how to do this in one of the next sections.

How Do Array Mathematics Work?

You’ve seen that broadcasting is handy when you’re doing arithmetic operations. In this section, you’ll discover some of the functions that you can use to do mathematics with arrays.

As such, it probably won’t surprise you that you can just use +, -, *, / or % to add, subtract, multiply, divide or calculate the remainder of two (or more) arrays. However, a big part of why NumPy is so handy, is because it also has functions to do this. The equivalent functions of the operations that you have seen just now are, respectively, np.add(), np.subtract(), np.multiply(), np.divide() and np.remainder().

You can also easily do exponentiation and taking the square root of your arrays with np.exp() and np.sqrt(), or calculate the sines or cosines of your array with np.sin() and np.cos(). Lastly, its’ also useful to mention that there’s also a way for you to calculate the natural logarithm with np.log() or calculate the dot product by applying the dot() to your array.

Try it all out in the DataCamp Light chunk below.

Just a tip: make sure to check out first the arrays that have been loaded for this exercise!

print(x)

[[1 2 3]
 [3 4 5]]

print(y)

[6 7 8]

# Add `x` and `y`
print(np.add(x,y))

[[ 7  9 11]
 [ 9 11 13]]

# Subtract `x` and `y`
print(np.subtract(x,y))

[[-5 -5 -5]
 [-3 -3 -3]]

# Multiply `x` and `y`
print(np.multiply(x,y))

[[ 6 14 24]
 [18 28 40]]

# Divide `x` and `y`
print(np.divide(x,y))

[[ 0.16666667  0.28571429  0.375     ]
 [ 0.5         0.57142857  0.625     ]]

# Calculate the remainder of `x` and `y`
print(np.remainder(x,y))

[[1 2 3]
 [3 4 5]]


Remember how broadcasting works? Check out the dimensions and the shapes of both x and y in your IPython shell. Are the rules of broadcasting respected?

But there is more.

Check out this small list of aggregate functions:

a.sum(): Array-wise sum
a.min(): Array-wise minimum value
b.max(axis=0): Maximum value of an array row
b.cumsum(axis=1): Cumulative sum of the elements
a.mean(): Mean
b.median(): Median
a.corrcoef(): Correlation coefficient
np.std(b): Standard deviation

Besides all of these functions, you might also find it useful to know that there are mechanisms that allow you to compare array elements. For example, if you want to check whether the elements of two arrays are the same, you might use the == operator. To check whether the array elements are smaller or bigger, you use the < or > operators.

This all seems quite straightforward, yes?

However, you can also compare entire arrays with each other! In this case, you use the np.array_equal() function. Just pass in the two arrays that you want to compare with each other, and you’re done.

Note that, besides comparing, you can also perform logical operations on your arrays. You can start with np.logical_or(), np.logical_not() and np.logical_and(). This basically works like your typical OR, NOT and AND logical operations;

In the simplest example, you use OR to see whether your elements are the same (for example, 1), or if one of the two array elements is 1. If both of them are 0, you’ll return FALSE. You would use AND to see whether your second element is also 1 and NOT to see if the second element differs from 1.

Test this out in the code chunk below:

print(a)

[ True  True False False]

print(b)

[ True False  True False]

# `a` AND `b` 
print(np.logical_and(a, b))

[ True False False False]

# `a` OR `b`
print(np.logical_or(a, b))

[ True  True  True False]

# `a` NOT `b`
print(np.logical_not(a,b))

[False False  True  True]

How To Subset, Slice, And Index Arrays

Besides mathematical operations, you might also consider taking just a part of the original array (or the resulting array) or just some array elements to use in further analysis or other operations. In such case, you will need to subset, slice and/or index your arrays.

These operations are very similar to when you perform them on Python lists. If you want to check out the similarities for yourself, or if you want a more elaborate explanation, you might consider checking out DataCamp’s Python list tutorial.

If you have no clue at all on how these operations work, it suffices for now to know these two basic things:

- You use square brackets [] as the index operator, and

- Generally, you pass integers to these square brackets, but you can also put a colon : or a combination of the colon with integers in it to designate the elements/rows/columns you want to select.

Besides from these two points, the easiest way to see how this all fits together is by looking at some examples of subsetting:

# Select the element at the 1st index
print(my_array[1])

2

# Select the element at row 1 column 2
print(my_2d_array[1][2])

7

# Select the element at row 1 column 2
print(my_2d_array[1,2])

7

# Select the element at row 1, column 2 and
print(my_3d_array[1,1,2])

11

Something a little bit more advanced than subsetting, if you will, is slicing. Here, you consider not just particular values of your arrays, but you go to the level of rows and columns. You’re basically working with “regions” of data instead of pure “locations”.

You can see what is meant with this analogy in these code examples:

# Select items at index 0 and 1
print(my_array[0:2])

[1 2]

# Select items at row 0 and 1, column 1
print(my_2d_array[0:2,1])

[2 6]

# Select items at row 1
# This is the same as saying `my_3d_array[1,:,:]
print(my_3d_array[1,...])

[[ 1  2  3  4]
 [ 9 10 11 12]]

You’ll see that, in essence, the following holds:

a[start:end] # items start through the end (but the end is not included!)
a[start:]    # items start through the rest of the array
a[:end]      # items from the beginning through the end (but the end is not included!)

Lastly, there’s also indexing. When it comes to NumPy, there are boolean indexing and advanced or “fancy” indexing.

(In case you’re wondering, this is true NumPy jargon, I didn’t make the last one up!)

First up is boolean indexing. Here, instead of selecting elements, rows or columns based on index number, you select those values from your array that fulfill a certain condition.

Putting this into code can be pretty easy:

print(my_array)

[1 2 3 4]

# Try out a simple example
print(my_array[my_array<2])

[1]

print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

# Specify a condition
bigger_than_3 = (my_3d_array >= 3)

# Use the condition to index our 3d array
print(my_3d_array[bigger_than_3])

[ 3  4  5  6  7  8  3  4  9 10 11 12]

Note that, to specify a condition, you can also make use of the logical operators | (OR) and & (AND). If you would want to rewrite the condition above in such a way (which would be inefficient, but I demonstrate it here for educational purposes :)), you would get bigger_than_3 = (my_3d_array > 3) | (my_3d_array == 3).

With the arrays that have been loaded in, there aren’t too many possibilities, but with arrays that contain for example, names or capitals, the possibilities could be endless!

When it comes to fancy indexing, that what you basically do with it is the following: you pass a list or an array of integers to specify the order of the subset of rows you want to select out of the original array.

Does this sound a little bit abstract to you?

No worries, just try it out in the code chunk below:

# Select elements at (1,0), (0,1), (1,2) and (0,0)
print(my_2d_array[[1, 0, 1, 0],[0, 1, 2, 0]])

[5 2 7 1]

# Select a subset of the rows and columns
print(my_2d_array[[1, 0, 1, 0]][:,[0,1,2,0]])

[[5 6 7 5]
 [1 2 3 1]
 [5 6 7 5]
 [1 2 3 1]]

Now, the second statement might seem to make less sense to you at first sight. This is normal. It might make more sense if you break it down:

- If you just execute my_2d_array[[1,0,1,0]], the result is the following:

array([[5, 6, 7, 8],
   [1, 2, 3, 4],
   [5, 6, 7, 8],
   [1, 2, 3, 4]])

- What the second part, namely, [:,[0,1,2,0]], is tell you that you want to keep all the rows of this result, but that you want to change the order of the columns around a bit. You want to display the columns 0, 1, and 2 as they are right now, but you want to repeat column 0 as the last column instead of displaying column number 3. This will give you the following result:

array([[5, 6, 7, 5],
   [1, 2, 3, 1],
   [5, 6, 7, 5],
   [1, 2, 3, 1]])

Advanced indexing clearly holds no secrets for you any more!

How To Ask For Help

As a short intermezzo, you should know that you can always ask for more information about the modules, functions or classes that you’re working with, especially becauseNumPy can be quite something when you first get started on working with it.

Asking for help is fairly easy.

You just make use of the specific help functions that numpy offers to set you on your way:

- Use lookfor() to do a keyword search on docstrings. This is specifically handy if you’re just starting out, as the ‘theory’ behind it all might fade in your memory. The one downside is that you have to go through all of the search results if your query is not that specific, as is the case in the code example below. This might make it even less overviewable for you.

- Use info() for quick explanations and code examples of functions, classes, or modules. If you’re a person that learns by doing, this is the way to go! The only downside about using this function is probably that you need to be aware of the module in which certain attributes or functions are in. If you don’t know immediately what is meant by that, check out the code example below.

You see, both functions have their advantages and disadvantages, but you’ll see for yourself why both of them can be useful: try them out for yourself in the DataCamp Light code chunk below!

# Look up info on `mean` with `np.lookfor()`
print(np.lookfor("mean"))

Search results for 'mean'
-------------------------
numpy.mean
    Compute the arithmetic mean along the specified axis.
numpy.nanmean
    Compute the arithmetic mean along the specified axis, ignoring NaNs.
numpy.ma.mean
    Returns the average of the array elements along given axis.
numpy.matrix.mean
    Returns the average of the matrix elements along the given axis.
numpy.array_equiv
    Returns True if input arrays are shape consistent and all elements equal.
numpy.ma.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.chararray.mean
    Returns the average of the array elements along given axis.
numpy.ma.fix_invalid
    Return input with invalid data masked and replaced by a fill value.
numpy.ma.MaskedArray.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.polynomial.polyutils.trimcoef
    Remove "small" "trailing" coefficients from a polynomial.
numpy.exp
    Calculate the exponential of all elements in the input array.
numpy.pad
    Pads an array.
numpy.put
    Replaces specified elements of an array with given values.
numpy.std
    Compute the standard deviation along the specified axis.
numpy.sum
    Sum of array elements over a given axis.
numpy.var
    Compute the variance along the specified axis.
numpy.copy
    Return an array copy of the given object.
numpy.prod
    Return the product of array elements over a given axis.
numpy.take
    Take elements from an array along an axis.
numpy.isnan
    Test element-wise for NaN and return result as a boolean array.
numpy.ravel
    Return a contiguous flattened array.
numpy.copyto
    Copies values from one array to another, broadcasting as necessary.
numpy.einsum
    Evaluates the Einstein summation convention on the operands.
numpy.kaiser
    Return the Kaiser window.
numpy.median
    Compute the median along the specified axis.
numpy.nanmax
    Return the maximum of an array or maximum along an axis, ignoring any
numpy.nanmin
    Return minimum of an array or minimum along an axis, ignoring any NaNs.
numpy.nanstd
    Compute the standard deviation along the specified axis, while
numpy.nansum
    Return the sum of array elements over a given axis treating Not a
numpy.nanvar
    Compute the variance along the specified axis, while ignoring NaNs.
numpy.nditer
    Efficient multi-dimensional iterator object to iterate over arrays.
numpy.average
    Compute the weighted average along the specified axis.
numpy.hamming
    Return the Hamming window.
numpy.hanning
    Return the Hanning window.
numpy.polyfit
    Least squares polynomial fit.
numpy.reshape
    Gives a new shape to an array without changing its data.
numpy.bartlett
    Return the Bartlett window.
numpy.blackman
    Return the Blackman window.
numpy.can_cast
    Returns True if cast between data types can occur according to the
numpy.digitize
    Return the indices of the bins to which each value in input array belongs.
numpy.fromfile
    Construct an array from data in a text or binary file.
numpy.fromiter
    Create a new 1-dimensional array from an iterable object.
numpy.isfinite
    Test element-wise for finiteness (not infinity or not Not a Number).
numpy.full_like
    Return a full array with the same shape and type as a given array.
numpy.histogram
    Compute the histogram of a set of data.
numpy.nanmedian
    Compute the median along the specified axis, while ignoring NaNs.
numpy.ones_like
    Return an array of ones with the same shape and type as a given array.
numpy.empty_like
    Return a new array with the same shape and type as a given array.
numpy.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.nan_to_num
    Replace nan with zero and inf with finite numbers.
numpy.percentile
    Compute the qth percentile of the data along the specified axis.
numpy.recfromcsv
    Load ASCII data stored in a comma-separated file.
numpy.recfromtxt
    Load ASCII data from a file and return it in a record array.
numpy.trim_zeros
    Trim the leading and/or trailing zeros from a 1-D array or sequence.
numpy.zeros_like
    Return an array of zeros with the same shape and type as a given array.
numpy.ma.dot
    Return the dot product of two arrays.
numpy.ma.exp
    Calculate the exponential of all elements in the input array.
numpy.ma.var
    Compute the variance along the specified axis.
numpy.ma.copy
    a.copy(order='C')
numpy.fft.fft2
    Compute the 2-dimensional discrete Fourier Transform
numpy.fft.fftn
    Compute the N-dimensional discrete Fourier Transform.
numpy.fft.rfft
    Compute the one-dimensional discrete Fourier Transform for real input.
numpy.busday_offset
    First adjusts the date to fall on a valid day according to
numpy.ma.ravel
    Returns a 1D version of self, as a view.
numpy.fft.ifft2
    Compute the 2-dimensional inverse discrete Fourier Transform.
numpy.fft.ifftn
    Compute the N-dimensional inverse discrete Fourier Transform.
numpy.fft.rfftn
    Compute the N-dimensional discrete Fourier Transform for real input.
numpy.nanpercentile
    Compute the qth percentile of the data along the specified axis,
numpy.ma.median
    Compute the median along the specified axis.
numpy.fft.irfftn
    Compute the inverse of the N-dimensional FFT of real input.
numpy.linalg.svd
    Singular Value Decomposition.
numpy.ma.polyfit
    Least squares polynomial fit.
numpy.bytes0.split
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.linalg.cond
    Compute the condition number of a matrix.
numpy.linalg.norm
    Matrix or vector norm.
numpy.may_share_memory
    Determine if two arrays might share memory
numpy.bytes0.decode
    Decode the bytes using the codec registered for encoding.
numpy.bytes0.rsplit
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.str0.encode
    Encode S using the codec registered for encoding. Default encoding
numpy.matlib.randn
    Return a random matrix with data from the "standard normal" distribution.
numpy.matrix.ravel
    Return a flattened matrix.
numpy.bytes0.replace
    Return a copy with all occurrences of substring old replaced by new.
numpy.chararray.copy
    Return a copy of the array.
numpy.chararray.sort
    Sort an array, in-place.
numpy.chararray.view
    New view of array with the same data.
numpy.ma.empty_like
    Return a new array with the same shape and type as a given array.
numpy.ma.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.matrix.flatten
    Return a flattened copy of the matrix.
numpy.chararray.astype
    Copy of the array, cast to a specified type.
numpy.testing.Tester
    Nose test runner.
numpy.chararray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.chararray.tobytes
    Construct Python bytes containing the raw data bytes in the array.
numpy.matrix.partition
    Rearranges the elements in the array in such a way that value of the
numpy.chararray.tostring
    Construct Python bytes containing the raw data bytes in the array.
numpy.chararray.transpose
    a.transpose(*axes)
numpy.ma.MaskedArray.mean
    Returns the average of the array elements along given axis.
numpy.testing.noseclasses.NumpyDocTestCase._formatMessage
    Honour the longMessage attribute when generating failure messages.
numpy.ma.MaskedArray.dot
    Masked dot product of two arrays. Note that `out` and `strict` are
numpy.ma.MaskedArray.var
    Compute the variance along the specified axis.
numpy.ma.MaskedArray.copy
    Return a copy of the array.
numpy.ma.MaskedArray.view
    New view of array with the same data.
numpy.core.multiarray.scalar
    Return a new scalar array of the given type initialized with obj.
numpy.ma.MaskedArray.ravel
    Returns a 1D version of self, as a view.
numpy.ma.MaskedArray.filled
    Return a copy of self, with masked values filled with a given value.
numpy.lib.format.open_memmap
    Open a .npy file as a memory-mapped array.
numpy.lib.format.write_array
    Write an array to an NPY file, including a header.
numpy.ma.MaskedArray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.polynomial.Hermite._fit
    Least squares fit of Hermite series to data.
numpy.ma.MaskedArray.transpose
    a.transpose(*axes)
numpy.polynomial.HermiteE._fit
    Least squares fit of Hermite series to data.
numpy.polynomial.Laguerre._fit
    Least squares fit of Laguerre series to data.
numpy.polynomial.Legendre._fit
    Least squares fit of Legendre series to data.
numpy.polynomial.Chebyshev._fit
    Least squares fit of Chebyshev series to data.
numpy.polynomial.Polynomial._fit
    Least-squares fit of a polynomial to data.
numpy.distutils.command.bdist_rpm.bdist_rpm.set_undefined_options
    Set the values of any "undefined" options from corresponding
numpy.lib.format._write_array_header
    Write the header for an array and returns the version used
numpy.distutils.numpy_distribution.NumpyDistribution._set_command_options
    Set the options for 'command_obj' from 'option_dict'.  Basically
numpy.testing.Tester._get_custom_doctester
    Return instantiated plugin for doctests
numpy.distutils.fcompiler.FCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.msvccompiler.MSVCCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.intelccompiler.IntelCCompiler.link
    Link a bunch of stuff together to create an executable orNone

# Get info on data types with `np.info()`
print(np.info(np.ndarray.dtype))

None

np.info(np.ndarray.dtype)

(Did not print anything)

Note that you indeed need to know that dtype is an attribute of ndarray. Also, make sure that you don’t forget to put np in front of the modules, classes or terms you’re asking information about, otherwise you will get an error message like this:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'ndarray' is not defined
You now know how to ask for help, and that’s a good thing. The next topic that this NumPy tutorial covers is array manipulation.

Not that you can not overcome this topic on your own, quite the contrary!

But some of the functions might raise questions, because, what is the difference between resizing and reshaping?

And what is the difference between stacking your arrays horizontally and vertically?

The next section is all about answering these questions, but if you ever feel in doubt, feel free to use the help functions that you have just seen to quickly get up to speed.

How To Manipulate Arrays

Performing mathematical operations on your arrays is one of the things that you’ll be doing, but probably most importantly to make this and the broadcasting work is to know how to manipulate your arrays.

Below are some of the most common manipulations that you’ll be doing.

How To Transpose Your Arrays

What transposing your arrays actually does is permuting the dimensions of it. Or, in other words, you switch around the shape of the array. Let’s take a small example to show you the effect of transposition:

# Print `my_2d_array`
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Transpose `my_2d_array`
print(np.transpose(my_2d_array))

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

# Or use `T` to transpose `my_2d_array`
print(my_2d_array.T)

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

Tip: if the visual comparison between the array and its transposed version is not entirely clear, inspect the shape of the two arrays to make sure that you understand why the dimensions are permuted.

Note that there are two transpose functions. Both do the same; There isn’t too much difference. You do have to take into account that T seems more of a convenience function and that you have a lot more flexibility with np.transpose(). That’s why it’s recommended to make use of this function if you want to more arguments.

All is well when you transpose arrays that are bigger than one dimension, but what happens when you just have a 1-D array? Will there be any effect, you think?

Try it out for yourself in the code chunk below. Your 1-D array has already been loaded in:

# Print `my_2d_array`
print(my_array)

[1 2 3 4]

# Transpose `my_2d_array`
print(np.transpose(my_array))

[1 2 3 4]

# Or use `T` to transpose `my_2d_array`
print(my_array.T)

[1 2 3 4]

You’re absolutely right! There is no effect when you transpose a 1-D array!

Reshaping Versus Resizing Your Arrays

You might have read in the broadcasting section that the dimensions of your arrays need to be compatible if you want them to be good candidates for arithmetic operations. But the question of what you should do when that is not the case, was not answered yet.

Well, this is where you get the answer!

What you can do if the arrays don’t have the same dimensions, is resize your array. You will then return a new array that has the shape that you passed to the np.resize() function. If you pass your original array together with the new dimensions, and if that new array is larger than the one that you originally had, the new array will be filled with copies of the original array that are repeated as many times as is needed.

However, if you just apply np.resize() to the array and you pass the new shape to it, the new array will be filled with zeros.

Let’s try this out with an example:

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Print the shape of `x`
print(x.shape)

(3, 4)

# Resize `x` to ((6,4))
print(np.resize(x, (6,4)))

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Try out this as well
print(x.resize((6,4)))

None

# Print out `x`
print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

Besides resizing, you can also reshape your array. This means that you give a new shape to an array without changing its data. The key to reshaping is to make sure that the total size of the new array is unchanged. If you take the example of array x that was used above, which has a size of 3 X 4 or 12, you have to make sure that the new array also has a size of 12.

Psst… If you want to calculate the size of an array with code, make sure to use the size attribute: x.size or x.reshape((2,6)).size:

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Print the size of `x` to see what's possible
print(x.size)

12

# Reshape `x` to (2,6)
print(x.reshape((2,6)))

[[ 1.  1.  1.  1.  1.  1.]
 [ 1.  1.  1.  1.  1.  1.]]

print(x) # tao: x did not change

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Flatten `x`
z = x.ravel()

# Print `z`
print(z)

[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]

If all else fails, you can also append an array to your original one or insert or delete array elements to make sure that your dimensions fit with the other array that you want to use for your computations.

Another operation that you might keep handy when you’re changing the shape of arrays is ravel(). This function allows you to flatten your arrays. This means that if you ever have 2D, 3D or n-D arrays, you can just use this function to flatten it all out to a 1-D array.

Pretty handy, isn’t it?

How To Append Arrays

When you append arrays to your original array, they are “glued” to the end of that original array. If you want to make sure that what you append does not come at the end of the array, you might consider inserting it. Go to the next section if you want to know more.

Appending is a pretty easy thing to do thanks to the NumPy library; You can just make use of the np.append().

Check how it’s done in the code chunk below. Don’t forget that you can always check which arrays are loaded in by typing, for example, my_array in the IPython shell and pressing ENTER.

print(my_array)

[1 2 3 4]

# Append a 1D array to your `my_array`
new_array = np.append(my_array, [7, 8, 9, 10])

[ 1  2  3  4  7  8  9 10]

# Print `new_array`
print(new_array)

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Append an extra column to your `my_2d_array`
new_2d_array = np.append(my_2d_array, [[7], [8]], axis=1)

# Print `new_2d_array`
print(new_2d_array)

[[1 2 3 4 7]
 [5 6 7 8 8]]

Note how, when you append an extra column to my_2d_array, the axis is specified. Remember that axis 1 indicates the columns, while axis 0 indicates the rows in 2-D arrays.

How To Insert And Delete Array Elements

Next to appending, you can also insert and delete array elements. As you might have guessed by now, the functions that will allow you to do these operations are np.insert() and np.delete():

print(my_array)

[1 2 3 4]

# Insert `5` at index 1
print(np.insert(my_array, 1, 5))

[1 5 2 3 4]

print(my_array) # tao: my_array did not change

[1 2 3 4]

# Delete the value at index 1
print(np.delete(my_array,[1]))

[1 3 4]

print(my_array) # tao: my_array did not change

[1 2 3 4]

How To Join And Split Arrays

You can also ‘merge’ or join your arrays. There are a bunch of functions that you can use for that purpose and most of them are listed below.

Try them out, but also make sure to test out what the shape of the arrays is in the IPython shell. The arrays that have been loaded are x, my_array, my_resized_array and my_2d_array.

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

print(x)

[ 1.  1.  1.  1.]

# Concatentate `my_array` and `x`
print(np.concatenate((my_array,x)))

[ 1.  2.  3.  4.  1.  1.  1.  1.]

# Stack arrays row-wise
print(np.vstack((my_array, my_2d_array)))

[[1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

print(my_resized_array)

[[1 2 3 4]
 [1 2 3 4]]

# Stack arrays row-wise
print(np.r_[my_resized_array, my_2d_array])

[[1 2 3 4]
 [1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

# Stack arrays horizontally
print(np.hstack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.column_stack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.c_[my_resized_array, my_2d_array])

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

You’ll note a few things as you go through the functions:

- The number of dimensions needs to be the same if you want to concatenate two arrays with np.concatenate(). As such, if you want to concatenate an array with my_array, which is 1-D, you’ll need to make sure that the second array that you have, is also 1-D.

- With np.vstack(), you effortlessly combine my_array with my_2d_array. You just have to make sure that, as you’re stacking the arrays row-wise, that the number of columns in both arrays is the same. As such, you could also add an array with shape (2,4) or (3,4) to my_2d_array, as long as the number of columns matches. Stated differently, the arrays must have the same shape along all but the first axis. The same holds also for when you want to use np.r[].

- For np.hstack(), you have to make sure that the number of dimensions is the same and that the number of rows in both arrays is the same. That means that you could stack arrays such as (2,3) or (2,4) to my_2d_array, which itself as a shape of (2,4). Anything is possible as long as you make sure that the number of rows matches. This function is still supported by NumPy, but you should prefer np.concatenate() or np.stack().

- With np.column_stack(), you have to make sure that the arrays that you input have the same first dimension. In this case, both shapes are the same, but if my_resized_array were to be (2,1) or (2,), the arrays still would have been stacked.

- np.c_[] is another way to concatenate. Here also, the first dimension of both arrays needs to match.

When you have joined arrays, you might also want to split them at some point. Just like you can stack them horizontally, you can also do the same but then vertically. You use np.hsplit() and np.vsplit(), respectively:

print(my_stacked_array)

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Split `my_stacked_array` horizontally at the 2nd index
print(np.hsplit(my_stacked_array, 2))

[array([[1, 2, 3, 4],
       [1, 2, 3, 4]]), array([[1, 2, 3, 4],
       [5, 6, 7, 8]])]

# Split `my_stacked_array` vertically at the 2nd index
print(np.vsplit(my_stacked_array, 2))

[array([[1, 2, 3, 4, 1, 2, 3, 4]]), array([[1, 2, 3, 4, 5, 6, 7, 8]])]

What you need to keep in mind when you’re using both of these split functions is probably the shape of your array. Let’s take the above case as an example: my_stacked_array has a shape of (2,8). If you want to select the index at which you want the split to occur, you have to keep the shape in mind.

How To Visualize NumPy Arrays

Lastly, something that will definitely come in handy is to know how you can plot your arrays. This can especially be handy in data exploration, but also in later stages of the data science workflow, when you want to visualize your arrays.

With np.histogram()

Contrary to what the function might suggest, the np.histogram() function doesn’t draw the histogram but it does compute the occurrences of the array that fall within each bin; This will determine the area that each bar of your histogram takes up.

What you pass to the np.histogram() function then is first the input data or the array that you’re working with. The array will be flattened when the histogram is computed.

# Import `numpy` as `np`
import numpy as np

# Initialize your array
my_3d_array = np.array([[[1,2,3,4], [5,6,7,8]], [[1,2,3,4], [9,10,11,12]]], dtype=np.int64)

# Pass the array to `np.histogram()`
print(np.histogram(my_3d_array))

(array([4, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([  1. ,   2.1,   3.2,   4.3,   5.4,   6.5,   7.6,   8.7,   9.8,
        10.9,  12. ]))

# Specify the number of bins
print(np.histogram(my_3d_array, bins=range(0,13)))

(array([0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]))

You’ll see that as a result, the histogram will be computed: the first array lists the frequencies for all the elements of your array, while the second array lists the bins that would be used if you don’t specify any bins.

If you do specify a number of bins, the result of the computation will be different: the floats will be gone and you’ll see all integers for the bins.

There are still some other arguments that you can specify that can influence the histogram that is computed. You can find all of them here.

But what is the point of computing such a histogram if you can’t visualize it?

Visualization is a piece of cake with the help of Matplotlib, but you don’t need np.histogram() to compute the histogram. plt.hist() does this for itself when you pass it the (flattened) data and the bins:

# Import numpy and matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Construct the histogram with a flattened 3d array and a range of bins
plt.hist(my_3d_array.ravel(), bins=range(0,13))

# Add a title to the plot
plt.title('Frequency of My 3D Array Elements')

# Show the plot
plt.show()

The above code will then give you the following (basic) histogram:

(A picture)

Using np.meshgrid()

Another way to (indirectly) visualize your array is by using np.meshgrid(). The problem that you face with arrays is that you need 2-D arrays of x and y coordinate values. With the above function, you can create a rectangular grid out of an array of x values and an array of y values: the np.meshgrid() function takes two 1D arrays and produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays. Then, you can use these matrices to make all sorts of plots.

np.meshgrid() is particularly useful if you want to evaluate functions on a grid, as the code below demonstrates:

# Import NumPy and Matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Create an array
points = np.arange(-5, 5, 0.01)

# Make a meshgrid
xs, ys = np.meshgrid(points, points)
z = np.sqrt(xs ** 2 + ys ** 2)

# Display the image on the axes
plt.imshow(z, cmap=plt.cm.gray)

# Draw a color bar
plt.colorbar()

# Show the plot
plt.show()
The code above gives the following result:

(A picture)

Beyond Data Analysis with NumPy

Congratulations, you have reached the end of the NumPy tutorial!

You have covered a lot of ground, so now you have to make sure to retain the knowledge that you have gained. Don’t forget to get your copy of DataCamp’s NumPy cheat sheet to support you in doing this!

After all this theory, it’s also time to get some more practice with the concepts and techniques that you have learned in this tutorial. One way to do this is to go back to the scikit-learn tutorial and start experimenting with further with the data arrays that are used to build machine learning models.

If this is not your cup of tea, check again whether you have downloaded Anaconda. Then, get started with NumPy arrays in Jupyter with this Definitive Guide to Jupyter Notebook. Also make sure to check out this Jupyter Notebook, which also guides you through data analysis in Python with NumPy and some other libraries in the interactive data science environment of the Jupyter Notebook.

Lastly, consider checking out DataCamp’s courses on data manipulation and visualization. Especially our latest courses in collaboration with Continuum Analytics will definitely interest you! Take a look at the Manipulating DataFrames with Pandas or the Pandas Foundations courses.

==
From https://www.hackerearth.com/practice/machine-learning/data-manipulation-visualisation-r-python/tutorial-data-manipulation-numpy-pandas-python/tutorial/

Practical Tutorial on Data Manipulation with Numpy and Pandas in Python

TUTORIAL
Introduction

The pandas library has emerged into a power house of data manipulation tasks in python since it was developed in 2008. With its intuitive syntax and flexible data structure, it's easy to learn and enables faster data computation. The development of numpy and pandas libraries has extended python's multi-purpose nature to solve machine learning problems as well. The acceptance of python language in machine learning has been phenomenal since then.

This is just one more reason underlining the need for you to learn these libraries now. Published in early 2017, this blog claimed that python jobs outnumbered R jobs.

In this tutorial, we'll learn about using numpy and pandas libraries for data manipulation from scratch. Instead of going into theory, we'll take a practical approach.

First, we'll understand the syntax and commonly used functions of the respective libraries. Later, we'll work on a real-life data set.

Note: This tutorial is best suited for people who know the basics of python. No further knowledge is expected. Make sure you have python installed on your laptop.

Table of Contents

6 Important things you should know about Numpy and Pandas
Starting with Numpy
Starting with Pandas
Exploring an ML Data Set
Building a Random Forest Model
6 Important things you should know about Numpy and Pandas

The data manipulation capabilities of pandas are built on top of the numpy library. In a way, numpy is a dependency of the pandas library.
Pandas is best at handling tabular data sets comprising different variable types (integer, float, double, etc.). In addition, the pandas library can also be used to perform even the most naive of tasks such as loading data or doing feature engineering on time series data.
Numpy is most suitable for performing basic numerical computations such as mean, median, range, etc. Alongside, it also supports the creation of multi-dimensional arrays.
Numpy library can also be used to integrate C/C++ and Fortran code.
Remember, python is a zero indexing language unlike R where indexing starts at one.
The best part of learning pandas and numpy is the strong active community support you'll get from around the world.
Just to give you a flavor of the numpy library, we'll quickly go through its syntax structures and some important commands such as slicing, indexing, concatenation, etc. All these commands will come in handy when using pandas as well. Let's get started!

Starting with Numpy

#load the library and check its version, just to make sure we aren't using an older version
import numpy as np
np.__version__
'1.12.1'

#create a list comprising numbers from 0 to 9
L = list(range(10))

#converting integers to string - this style of handling lists is known as list comprehension.
#List comprehension offers a versatile way to handle list manipulations tasks easily. We'll learn about them in future tutorials. Here's an example.  

[str(c) for c in L]
['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

[type(item) for item in L]
[int, int, int, int, int, int, int, int, int, int]

Creating Arrays

Numpy arrays are homogeneous in nature, i.e., they comprise one data type (integer, float, double, etc.) unlike lists.

#creating arrays
np.zeros(10, dtype='int')
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

#creating a 3 row x 5 column matrix
np.ones((3,5), dtype=float)
array([[ 1.,  1.,  1.,  1.,  1.],
      [ 1.,  1.,  1.,  1.,  1.],
      [ 1.,  1.,  1.,  1.,  1.]])

#creating a matrix with a predefined value
np.full((3,5),1.23)
array([[ 1.23,  1.23,  1.23,  1.23,  1.23],
      [ 1.23,  1.23,  1.23,  1.23,  1.23],
      [ 1.23,  1.23,  1.23,  1.23,  1.23]])

#create an array with a set sequence
np.arange(0, 20, 2)
array([0, 2, 4, 6, 8,10,12,14,16,18])

#create an array of even space between the given range of values
np.linspace(0, 1, 5)
array([ 0., 0.25, 0.5 , 0.75, 1.])

#create a 3x3 array with mean 0 and standard deviation 1 in a given dimension
np.random.normal(0, 1, (3,3))
array([[ 0.72432142, -0.90024075,  0.27363808],
      [ 0.88426129,  1.45096856, -1.03547109],
      [-0.42930994, -1.02284441, -1.59753603]])

#create an identity matrix
np.eye(3)
array([[ 1.,  0.,  0.],
      [ 0.,  1.,  0.],
      [ 0.,  0.,  1.]])

#set a random seed
np.random.seed(0)

x1 = np.random.randint(10, size=6) #one dimension
x2 = np.random.randint(10, size=(3,4)) #two dimension
x3 = np.random.randint(10, size=(3,4,5)) #three dimension

print("x3 ndim:", x3.ndim)
print("x3 shape:", x3.shape)
print("x3 size: ", x3.size)
('x3 ndim:', 3)
('x3 shape:', (3, 4, 5))
('x3 size: ', 60)

Array Indexing

The important thing to remember is that indexing in python starts at zero.

x1 = np.array([4, 3, 4, 4, 8, 4])
x1
array([4, 3, 4, 4, 8, 4])

#assess value to index zero
x1[0]
4

#assess fifth value
x1[4]
8

#get the last value
x1[-1]
4

#get the second last value
x1[-2]
8

#in a multidimensional array, we need to specify row and column index
x2
array([[3, 7, 5, 5],
      [0, 1, 5, 9],
      [3, 0, 5, 0]])

#1st row and 2nd column value
x2[2,3]
0

#3rd row and last value from the 3rd column
x2[2,-1]
0

#replace value at 0,0 index
x2[0,0] = 12
x2
array([[12,  7,  5,  5],
      [ 0,  1,  5,  9],
      [ 3,  0,  5,  0]])

Array Slicing

Now, we'll learn to access multiple or a range of elements from an array.

x = np.arange(10)
x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

#from start to 4th position
x[:5]
array([0, 1, 2, 3, 4])

#from 4th position to end
x[4:]
array([4, 5, 6, 7, 8, 9])

#from 4th to 6th position
x[4:7]
array([4, 5, 6])

#return elements at even place
x[ : : 2]
array([0, 2, 4, 6, 8])

#return elements from first position step by two
x[1::2]
array([1, 3, 5, 7, 9])

#reverse the array
x[::-1]
array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])

Array Concatenation

Many a time, we are required to combine different arrays. So, instead of typing each of their elements manually, you can use array concatenation to handle such tasks easily.

#You can concatenate two or more arrays at once.
x = np.array([1, 2, 3])
y = np.array([3, 2, 1])
z = [21,21,21]
np.concatenate([x, y,z])
array([ 1,  2,  3,  3,  2,  1, 21, 21, 21])

#You can also use this function to create 2-dimensional arrays.
grid = np.array([[1,2,3],[4,5,6]])
np.concatenate([grid,grid])
array([[1, 2, 3],
      [4, 5, 6],
      [1, 2, 3],
      [4, 5, 6]])

#Using its axis parameter, you can define row-wise or column-wise matrix
np.concatenate([grid,grid],axis=1)
array([[1, 2, 3, 1, 2, 3],
      [4, 5, 6, 4, 5, 6]])

Until now, we used the concatenation function of arrays of equal dimension. But, what if you are required to combine a 2D array with 1D array? In such situations, np.concatenate might not be the best option to use. Instead, you can use np.vstack or np.hstack to do the task. Let's see how!
x = np.array([3,4,5])
grid = np.array([[1,2,3],[17,18,19]])
np.vstack([x,grid])
array([[ 3,  4,  5],
      [ 1,  2,  3],
      [17, 18, 19]])


#Similarly, you can add an array using np.hstack
z = np.array([[9],[9]])
np.hstack([grid,z])
array([[ 1,  2,  3,  9],
      [17, 18, 19,  9]])

Also, we can split the arrays based on pre-defined positions. Let's see how!
x = np.arange(10)
x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

x1,x2,x3 = np.split(x,[3,6])
print x1,x2,x3
[0 1 2] [3 4 5] [6 7 8 9]

grid = np.arange(16).reshape((4,4))
grid
upper,lower = np.vsplit(grid,[2])
print (upper, lower)
(array([[0, 1, 2, 3],
       [4, 5, 6, 7]]), array([[ 8,  9, 10, 11],
       [12, 13, 14, 15]]))

In addition to the functions we learned above, there are several other mathematical functions available in the numpy library such as sum, divide, multiple, abs, power, mod, sin, cos, tan, log, var, min, mean, max, etc. which you can be used to perform basic arithmetic calculations. Feel free to refer to numpy documentation for more information on such functions.

--
From ml4t:

--
Any python can run numpy, no need to use some annaconda environment (confirmed from tao's practice).

--
To use NumPy:
import numpy as np

The NumPy numerical library: NumPy is a Python library that acts as a wrapper around underlying C and Fortran code. Because of that, it's very, very fast. NumPy focuses on matrices which are called ndarrays. NumPy is one of the important reasons people use Python for financial research.

Now, how does NumPy relate to Pandas? Well, I said just a moment ago that NumPy is a wrapper for numerical libraries, well it turns out that Pandas is a kind of wrapper for NumPy. So remember our traditional data frame here, with our columns being symbols and our rows being dates. This data frame is just a wrapper around this ndarray, access the columns with symbols and the rows by dates. But you can, in fact, just treat this inside part (tao: the part of the dataframe without the header line and the index column) as an ndarray directly. If you use this syntax (nd1 = df1.values) in Python, that pulls these values out and lets you access it directly and then ndarray. You don't really need to do that though, you can, if you like, ** treat a data frame just like a NumPy ndarray **. And so we're going to assume in the rest of this lesson that we're just working with an ndarray

==
NumPy access ndarray cells:

nd[0, 0] # The element at row 0, column 0
nd[3, 2] # The element at row 3, column 2
nd[0:3, 1:3] # The block from row 0 to row 2, column 1 to column 2. Notices that 0:3 means 0,1,2.
nd[:, 3] # All the rows, column 3.
nd[-1, 1:3] # -1 means last row. Similary, -2 means second to last row.
nd[0, 1:3] # For the 0 row, get values from column 1 to column 2

nd[0, 0:3:2] # Slice n:m:t sepcifies a range that starts at n, and stops before m, in steps of size t.

nd1[0:2, 0:2] = nd2[-2:, 2:4] # Replace some of the values in nd1, with these values from nd2. "-2:" means from "the second to last row" to "the last row". 

--
Assign values:

nd[0, 0] = 1 # Assign value
nd[0, :] = 2 # Assign a single value to an entire row
nd[:, 3] = [1, 2, 3, 4, 5] # Assign a list to a column in an array.

==
Create NumPy arrays from scratch:

np.array() can take as input a list, a template, or other sequence.

import numpy as np

a = np.array([2, 3, 4]) # List to 1D array

b = np.array([(2, 3, 4), (5, 6, 7)]) # List of tuples to 2D array. Each tuple serves as one row. We could also have passed a list of lists.

print b

Output:
[[2 3 4]
  5 6 7]]

b.shape # Returns [2, 3]  
b.shape[0] # Number of rows
b.shape[1] # Number of columns
b.size # Number of elements in the array

print b.dtype # Data type of each element

--
Create empty ndarray:

np.empty(5)
np.empty((5, 4))

The empty function takes the shape of the array as input. The shape can be defined as a single integer, as we did over here, for creating a one dimensional array, or a sequence of integers denoting the size in each dimension. For a two dimensional array, a sequence of two integers is needed. That is the number of rows and the number of columns.

print np.empty((5, 4))

Now let's check the output. Hm, strange. The empty array is not actually empty. What happens is that when we call numpy.empty to create an array, the elements of the array read in whatever values were present in the corresponding memory location.

--
Create ndarray with ones or zeroes:

np.ones((5, 4)) # Create an array full of ones.
np.zeroes((5, 4)) # Create an array full of zeroes.

We notice that the default data type of all the values in the array is float. Fortunately, you can change this when creating the array:

np.ones((5, 4), dtype = np.int_) # Here we defined the values to be integers

--
Create ndarray with random values:

Generate an array full of random numbers, uniformly sampled from [0.0, 1.0). Pass in a size tuple:

np.random.random((5, 4)) # 5 rows, 4 columns

A slightly variation of this function is rand. We directly pass the
values of the rows and columns through the function and did not define a tuple:

np.random.rand(5, 4) # 5 rows, 4 columns

--
Create ndarray with normal (Gaussian) distribution.

Standard normal (mean = 0, s.d. = 1):

np.random.normal(size = (2, 3)) # 2 rows, 3 columns

Mean = 50, s.d. = 10:

np.random.normal(50, 10, size = (2, 3))

--
Create ndarray with random integers:

np.random.randint(10) # A single integer in [0, 10)
np.random.randint(0, 10) # Same as above, specifying [low, high) explicit
np.random.randint(0, 10, size = 5) # 5 random integers as a 1D array 
np.random.randint(0, 10, size = (2, 3)) # 2*3 array of random integers 

--
np.random.seed(693)

a = np.random.randint(0, 10, size = (5, 4))

Note how we used seed, the random number generator with the constant, to get the same sequence of numbers every time.

--
We can also sum in a specific direction of the array.  What I mean by direction is along rows or columns. NumPy gives this direction a special name.  It is called axis.  Axis = 0 signifies rows, and axis =  1 indicates columns. 

a:

[[2 0 5 1
  1 3 4 4
  9 2 9 1
  9 3 7 5 
  4 7 0 3]]

# Iterate over rows, to compute sum of each column:
a.sum(axis = 0) # Returns sum of each column: [25 15 25 14]

# Iterate over columns, to compute sum of each row:
a.sum(axis = 1) # Returns sum of each row: [8 12 21 24 14]

# Returns min across rows, to compute min of each column:
a.min(axis = 0) # Returns min of each column: [1 0 0 1]

# Returns max across columns, to compute max of each row:
a.max(axis = 1) # Returns max of each row: [5 4 9 9 7]

a.mean() # Returns the mean all elements: 3.95. Of course we can get mean along each axis as we did for max and min.  

--
Find the position of some element in an ndarray:

a.argmx() # Returns the index of the maximum value in given 1D array

For multidimensional arrays, finding and representing indices is a little tricky. 

--
We want to get all the values from the array, which is less than mean of the entire array.  

mean = a.mean()
a[a < mean] # Returns the wanted values above
a[a < mean] = mean # all the values previously less than mean have been replaced by the mean

--
Arithmetic operations on arrays are always applied element wise (tao: ie, element by elment).

2 * a # Every element multiplied by 2.
a + b # Add every element from a and b.
a * b # Normal matrix multiplication (as in Linear Algebra).

What about matrix multiplication?  How do you achieve that?  Like, for everything, Num Pi has a function.  It has function called dot, which performs matrix multiplication.  

--
(findrandomnumber)                   
Random number

import numpy as np  
np.random.random() # Returns a random number, not sure about the range.


=============
(findpandas)
Pandas

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python

Pandas Tutorial: DataFrames in Python

Explore data analysis with Python. Pandas DataFrames make manipulating your data easy, from selecting or replacing columns and indices to reshaping your data.

Pandas is a popular Python package for data science, and with good reason: it offers powerful, expressive and flexible data structures that make data manipulation and analysis easy, among many other things. The DataFrame is one of these structures.

This tutorial covers Pandas DataFrames, from basic manipulations to advanced operations, by tackling 11 of the most popular questions so that you understand -and avoid- the doubts of the Pythonistas who have gone before you.

Content
How To Create a Pandas DataFrame
How To Select an Index or Column From a DataFrame
How To Add an Index, Row or Column to a DataFrame
How To Delete Indices, Rows or Columns From a DataFrame
How To Rename the Columns or Indices of a DataFrame
How To Format the Data in Your DataFrame
How To Create an Empty DataFrame
Does Pandas Recognize Dates When Importing Data?
When, Why and How You Should Reshape Your DataFrame
How To Iterate Over a DataFrame
How To Write a DataFrame to a File

(For more practice, try the first chapter of this Pandas DataFrames course for free!)

What Are Pandas Data Frames?

Before you start, let’s have a brief recap of what DataFrames are.

Those who are familiar with R know the data frame as a way to store data in rectangular grids that can easily be overviewed. Each row of these grids corresponds to measurements or values of an instance, while each column is a vector containing data for a specific variable. This means that a data frame’s rows do not need to contain, but can contain, the same type of values: they can be numeric, character, logical, etc.

Now, DataFrames in Python are very similar: they come with the Pandas library, and they are defined as two-dimensional labeled data structures with columns of potentially different types.

In general, you could say that the Pandas DataFrame consists of three main components: the data, the index, and the columns.

Firstly, the DataFrame can contain data that is:

- a Pandas DataFrame

- a Pandas Series: a one-dimensional labeled array capable of holding any data type with axis labels or index. An example of a Series object is one column from a DataFrame.

- a NumPy ndarray, which can be a record or structured

- a two-dimensional ndarray

- dictionaries of one-dimensional ndarray’s, lists, dictionaries or Series.

Note the difference between np.ndarray and np.array() . The former is an actual data type, while the latter is a function to make arrays from other data structures.

Structured arrays allow users to manipulate the data by named fields: in the example below, a structured array of three tuples is created. The first element of each tuple will be called foo and will be of type int, while the second element will be named bar and will be a float.

Record arrays, on the other hand, expand the properties of structured arrays. They allow users to access fields of structured arrays by attribute rather than by index. You see below that the foo values are accessed in the r2 record array.

An example:

# A structured array
my_array = np.ones(3, dtype=([('foo', int), ('bar', float)]))
print(my_array['foo'])

[1 1 1]

# A record array
my_array2 = my_array.view(np.recarray)
print(my_array2.foo)

[1 1 1]

Besides data, you can also specify the index and column names for your DataFrame. The index, on the one hand, indicates the difference in rows, while the column names indicate the difference in columns. You will see later that these two components of the DataFrame will come in handy when you’re manipulating your data.

If you’re still in doubt about Pandas DataFrames and how they differ from other data structures such as a NumPy array or a Series, you can watch the small presentation below:

(A video)

Note that in this post, most of the times, the libraries that you need have already been loaded in. The Pandas library is usually imported under the alias pd, while the NumPy library is loaded as np. Remember that when you code in your own data science environment, you shouldn’t forget this import step, which you write just like this:

import numpy as np
import pandas as pd

Now that there is no doubt in your mind about what DataFrames are, what they can do and how they differ from other structures, it’s time to tackle the most common questions that users have about working with them!

1. How To Create a Pandas DataFrame

Obviously, making your DataFrames is your first step in almost anything that you want to do when it comes to data munging in Python. Sometimes, you will want to start from scratch, but you can also convert other data structures, such as lists or NumPy arrays, to Pandas DataFrames. In this section, you’ll only cover the latter. However, if you want to read more on making empty DataFrames that you can fill up with data later, go to question 7.

Among the many things that can serve as input to make a ‘DataFrame’, a NumPy ndarray is one of them. To make a data frame from a NumPy array, you can just pass it to the DataFrame() function in the data argument.

data = np.array([['','Col1','Col2'],
                ['Row1',1,2],
                ['Row2',3,4]])

print(data)

[['' 'Col1' 'Col2']
 ['Row1' '1' '2']
 ['Row2' '3' '4']]

print(pd.DataFrame(data=data[1:,1:],
                  index=data[1:,0],
                  columns=data[0,1:]))

     Col1 Col2
Row1    1    2
Row2    3    4

Pay attention to how the code chunks above select elements from the NumPy array to construct the DataFrame: you first select the values that are contained in the lists that start with Row1 and Row2, then you select the index or row numbers Row1 and Row2 and then the column names Col1 and Col2.

Next, you also see that, in the DataCamp Light chunk above, you printed out a small selection of the data. This works the same as subsetting 2D NumPy arrays: you first indicate the row that you want to look in for your data, then the column. Don’t forget that the indices start at 0! For data in the example above, you go and look in the rows at index 1 to end, and you select all elements that come after index 1. As a result, you end up selecting 1, 2, 3 and 4.

This approach to making DataFrames will be the same for all the structures that DataFrame() can take on as input.

Try it out in the code chunk below:

Remember that the Pandas library has already been imported for you as pd.

# Take a 2D array as input to your DataFrame 
my_2darray = np.array([[1, 2, 3], [4, 5, 6]])
print(pd.DataFrame(my_2darray))

   0  1  2
0  1  2  3
1  4  5  6

# Take a dictionary as input to your DataFrame 
my_dict = {1: ['1', '3'], 2: ['1', '2'], 3: ['2', '4']}
print(pd.DataFrame(my_dict))

   1  2  3
0  1  1  2
1  3  2  4

# Take a DataFrame as input to your DataFrame 
my_df = pd.DataFrame(data=[4,5,6,7], index=range(0,4), columns=['A'])
print(pd.DataFrame(my_df))

   A
0  4
1  5
2  6
3  7

# Take a Series as input to your DataFrame
my_series = pd.Series({"United Kingdom":"London", "India":"New Delhi", "United States":"Washington", "Belgium":"Brussels"})
print(pd.DataFrame(my_series))

                         0
Belgium           Brussels
India            New Delhi
United Kingdom      London
United States   Washington

Note that the index of your Series (and DataFrame) contains the keys of the original dictionary, but that they are sorted: Belgium will be the index at 0, while the United States will be the index at 3.

After you have created your DataFrame, you might want to know a little bit more about it. You can use the shape property or the len() function in combination with the .index property:

df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6]]))

# Use the `shape` property
print(df.shape)

(2, 3)

# Or use the `len()` function with the `index` property
print(len(df.index))

2

These two options give you slightly different information on your DataFrame: the shape property will provide you with the dimensions of your DataFrame. That means that you will get to know the width and the height of your DataFrame. On the other hand, the len() function, in combination with the index property, will only give you information on the height of your DataFrame.

This all is totally not extraordinary, though, as you explicitly give in the index property.

You could also use df[0].count() to get to know more about the height of your DataFrame, but this will exclude the NaN values (if there are any). That is why calling .count() on your DataFrame is not always the better option.

If you want more information on your DataFrame columns, you can always execute list(my_dataframe.columns.values). Try this out for yourself in the DataCamp Light block above!

Fundamental DataFrame Operations

Now that you have put your data in a more convenient Pandas DataFrame structure, it’s time to get to the real work!

This first section will guide you through the first steps of working with DataFrames in Python. It will cover the basic operations that you can do on your newly created DataFrame: adding, selecting, deleting, renaming, … You name it!

2. How To Select an Index or Column From a Pandas DataFrame

Before you start with adding, deleting and renaming the components of your DataFrame, you first need to know how you can select these elements. So, how do you do this?

Even though you might still remember how to do it from the previous section: selecting an index, column or value from your DataFrame isn’t that hard, quite the contrary. It’s similar to what you see in other languages (or packages!) that are used for data analysis. If you aren’t convinced, consider the following:

In R, you use the [,] notation to access the data frame’s values.

Now, let’s say you have a DataFrame like this one:

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9
And you want to access the value that is at index 0, in column ‘A’.

Various options exist to get your value 1 back:

# Using `iloc[]`
print(df.iloc[0][0])

1

# Using `loc[]`
print(df.loc[0]['A'])

1

# Using `at[]`
print(df.at[0,'A'])

1

# Using `iat[]`
print(df.iat[0,0])

1

The most important ones to remember are, without a doubt, .loc[] and .iloc[]. The subtle differences between these two will be discussed in the next sections.

Enough for now about selecting values from your DataFrame. What about selecting rows and columns? In that case, you would use:

# Use `iloc[]` to select a row
print(df.iloc[0])

A    1
B    2
C    3
Name: 0, dtype: int64

# Use `loc[]` to select a column
print(df.loc[:,'A'])

0    1
1    4
2    7
Name: A, dtype: int64

For now, it’s enough to know that you can either access the values by calling them by their label or by their position in the index or column. If you don’t see this, look again at the slight differences in the commands: one time, you see [0][0], the other time, you see [0,'A'] to retrieve your value 1.

3. How To Add an Index, Row or Column to a Pandas DataFrame

Now that you have learned how to select a value from a DataFrame, it’s time to get to the real work and add an index, row or column to it!

Adding an Index to a DataFrame

When you create a DataFrame, you have the option to add input to the ‘index’ argument to make sure that you have the index that you desire. When you don’t specify this, your DataFrame will have, by default, a numerically valued index that starts with 0 and continues until the last row of your DataFrame.

However, even when your index is specified for you automatically, you still have the power to re-use one of your columns and make it your index. You can easily do this by calling set_index() on your DataFrame. Try this out below!

# Print out your DataFrame `df` to check it out
print(df)

   A  B  C
0  1  2  3
1  4  5  6

# Set 'C' as the index of your DataFrame
df.set_index('C')

   A  B
C      
3  1  2
6  4  5

Adding Rows to a DataFrame

Before you can get to the solution, it’s first a good idea to grasp the concept of loc and how it differs from other indexing attributes such as .iloc[] and .ix[]:

- .loc[] works on labels of your index. This means that if you give in loc[2], you look for the values of your DataFrame that have an index labeled 2.

- .iloc[] works on the positions in your index. This means that if you give in iloc[2], you look for the values of your DataFrame that are at index ’2`.

- .ix[] is a more complex case: when the index is integer-based, you pass a label to .ix[]. ix[2] then means that you’re looking in your DataFrame for values that have an index labeled 2. This is just like .loc[]! However, if your index is not solely integer-based, ix will work with positions, just like .iloc[].

This all might seem very complicated. Let’s illustrate all of this with a small example:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2, 'A', 4], columns=[48, 49, 50])

# Pass `2` to `loc`
print(df.loc[2])

48    1
49    2
50    3
Name: 2, dtype: int64

# Pass `2` to `iloc`
print(df.iloc[2])

48    7
49    8
50    9
Name: 4, dtype: int64

# Pass `2` to `ix`
print(df.ix[2])

48    7
49    8
50    9
Name: 4, dtype: int64

Note that in this case, you used an example of a DataFrame that is not solely integer-based as to make it easier for you to understand the differences. You clearly see that passing 2 to .loc[] or .iloc[]/.ix[] does not give back the same result!

- You know that .loc[] will go and look at the values that are at label 2. The result that you get back will be

48    1
49    2
50    3

- You also know that .iloc[] will go and look at the positions in the index. When you pass 2, you will get back:

48    7
49    8
50    9

- Since the index doesn’t only contain integers, .ix[] will have the same behavior as iloc and look at the positions in the index. You will get back the same result as .iloc[].

Now that the difference between .iloc[], .loc[] and .ix[] is clear, you are ready to give adding rows to your DataFrame a go!

Tip: as a consequence of what you have just read, you understand now also that the general recommendation is that you use .loc to insert rows in your DataFrame. That is because if you would use df.ix[], you might try to reference a numerically valued index with the index value and accidentally overwrite an existing row of your DataFrame. You better avoid this!

Check out the difference once more in the DataFrame below:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2.5, 12.6, 4.8], columns=[48, 49, 50])

# There's no index labeled `2`, so you will change the index at position `2`
df.ix[2] = [60, 50, 40]
print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8   60  50  40

# This will make an index labeled `2` and add the new values
df.loc[2] = [11, 12, 13]
print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8   60  50  40
2.0   11  12  13

You can see why all of this can be confusing, right?

Adding a Column to Your DataFrame

In some cases, you want to make your index part of your DataFrame. You can easily do this by taking a column from your DataFrame or by referring to a column that you haven’t made yet and assigning it to the .index property, just like this:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

print(df)

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Use `.index`
df['D'] = df.index

# Print `df`
print(df)

   A  B  C  D
0  1  2  3  0
1  4  5  6  1
2  7  8  9  2

In other words, you tell your DataFrame that it should take column A as its index.

However, if you want to append columns to your DataFrame, you could also follow the same approach as when you would add an index to your DataFrame: you use .loc[] or .iloc[]. In this case, you add a Series to an existing DataFrame with the help of .loc[]:

# Study the DataFrame `df`
print(df)

   1  2  3
0  1  1  2
1  3  2  4

# Append a column to `df`
df.loc[:, 4] = pd.Series(['5', '6'], index=df.index)

# Print out `df` again to see the changes
print(df)

   1  2  3  4
0  1  1  2  5
1  3  2  4  6

Remember a Series object is much like a column of a DataFrame. That explains why you can easily add a Series to an existing DataFrame. Note also that the observation that was made earlier about .loc[] still stays valid, even when you’re adding columns to your DataFrame!

Resetting the Index of Your DataFrame

When your index doesn’t look entirely the way you want it to, you can opt to reset it. You can easily do this with .reset_index(). However, you should still watch out, as you can pass several arguments that can make or break the success of your reset:

# Check out the weird index of your dataframe
print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8    7   8   9

# Use `reset_index()` to reset the values
df_reset = df.reset_index(level=0, drop=True)

# Print `df_reset`
print(df_reset)

   48  49  50
0   1   2   3
1   4   5   6
2   7   8   9

Now try replacing the drop argument by inplace in the code chunk above and see what happens!

Note how you use the drop argument to indicate that you want to get rid of the index that was there. If you would have used inplace, the original index with floats is added as an extra column to your DataFrame.

4. How to Delete Indices, Rows or Columns From a Pandas Data Frame

Now that you have seen how to select and add indices, rows, and columns to your DataFrame, it’s time to consider another use case: removing these three from your data structure.

Deleting an Index from Your DataFrame

If you want to remove the index from your DataFrame, you should reconsider because DataFrames and Series always have an index.

However, what you *can* do is, for example:

- resetting the index of your DataFrame (go back to the previous section to see how it is done) or

- remove the index name, if there is any, by executing del df.index.name,

- remove duplicate index values by resetting the index, dropping the duplicates of the index column that has been added to your DataFrame and reinstating that duplicateless column again as the index:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [40, 50, 60], [23, 35, 37]]), 
                  index= [2.5, 12.6, 4.8, 4.8, 2.5], 
                  columns=[48, 49, 50])

print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8    7   8   9
4.8   40  50  60
2.5   23  35  37

print(df.reset_index().drop_duplicates(subset='index', keep='last').set_index('index'))

       48  49  50
index            
12.6    4   5   6
4.8    40  50  60
2.5    23  35  37

- and lastly, remove an index, and with it a row. This is elaborated further on in this tutorial.

Now that you know how to remove an index from your DataFrame, you can go on to removing columns and rows!

Deleting a Column from Your DataFrame

To get rid of (a selection of) columns from your DataFrame, you can use the drop() method:

# Check out the DataFrame `df`
print(df)

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Drop the column with label 'A'                  
df.drop('A', axis=1, inplace=True)

print(df)

   B  C
0  2  3
1  5  6
2  8  9

# Drop the column at position 1
print(df.drop(df.columns[[1]], axis=1))

   B
0  2
1  5
2  8

print(df)

   B  C
0  2  3
1  5  6
2  8  9

You might think now: well, this is not so straightforward; There are some extra arguments that are passed to the drop() method!

- The axis argument is either 0 when it indicates rows and 1 when it is used to drop columns.

- You can set inplace to True to delete the column without having to reassign the DataFrame.

Removing a Row from Your DataFrame

You can remove duplicate rows from your DataFrame by executing df.drop_duplicates(). You can also remove rows from your DataFrame, taking into account only the duplicate values that exist in one column.

Check out this example:

# Check out your DataFrame `df`
print(df)

      48  49  50  50
2.5    1   2   3   4
12.6   4   5   6   5
4.8    7   8   9   6
4.8   23  50  60   7
2.5   23  35  37  23

# Drop the duplicates in `df`
print(df.drop_duplicates([48], keep='last'))

      48  49  50  50
2.5    1   2   3   4
12.6   4   5   6   5
4.8    7   8   9   6
2.5   23  35  37  23

If there is no uniqueness criterion to the deletion that you want to perform, you can use the drop() method, where you use the index property to specify the index of which rows you want to remove from your DataFrame:

# Check out the DataFrame `df`
print(df)

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Drop the index at position 1
print(df.drop(df.index[1]))

   A  B  C
0  1  2  3
2  7  8  9

After this command, you might want to reset the index again.

Tip: try resetting the index of the resulting DataFrame for yourself! Don’t forget to use the drop argument if you deem it necessary.

5. How to Rename the Index or Columns of a Pandas DataFrame

To give the columns or your index values of your dataframe a different value, it’s best to use the .rename() method.

# Check out your DataFrame `df`
print(df)

       A  B  C
    0  1  2  3
    1  4  5  6
    2  7  8  9

# Define the new names of your columns
newcols = {
    'A': 'new_column_1', 
    'B': 'new_column_2', 
    'C': 'new_column_3'
}

# Use `rename()` to rename your columns
print(df.rename(columns=newcols, inplace=True))

None

print(df)

   new_column_1  new_column_2  new_column_3
0             1             2             3
1             4             5             6
2             7             8             9

# Rename your index
print(df.rename(index={1: 'a'}))

   new_column_1  new_column_2  new_column_3
0             1             2             3
a             4             5             6
2             7             8             9

Tip: try changing the inplace argument in the first task (renaming your columns) to False and see what the script now renders as a result. You see that now the DataFrame hasn’t been reassigned when renaming the columns. As a result, the second task takes the original DataFrame as input and not the one that you just got back from the first rename() operation.

Beyond The Pandas DataFrame Basics

Now that you have gone through a first set of questions about Pandas’ DataFrames, it’s time to go beyond the basics and get your hands dirty for real because there is far more to DataFrames than what you have seen in the first section.

6. How To Format The Data in Your Pandas DataFrame

Most of the times, you will also want to be able to do some operations on the actual values that are contained within your DataFrame. In the following sections, you’ll cover several ways in which you can format your DataFrame’s values

Replacing All Occurrences of a String in a DataFrame
To replace certain strings in your DataFrame, you can easily use replace(): pass the values that you would like to change, followed by the values you want to replace them by.

Just like this:

# Study the DataFrame `df` first
print(df)

     Student1 Student2    Student3
0          OK  Perfect  Acceptable
1       Awful    Awful     Perfect
2  Acceptable       OK        Poor

# Replace the strings by numerical values (0-4)
print(df.replace(['Awful', 'Poor', 'OK', 'Acceptable', 'Perfect'], [0, 1, 2, 3, 4]))

   Student1  Student2  Student3
0         2         4         3
1         0         0         4
2         3         2         1

Note that there is also a regex argument that can help you out tremendously when you’re faced with strange string combinations:

# Check out your DataFrame `df`
print(df)

     0    1    2
0  1\n    2  3\n
1    4    5  6\n
2    7  8\n    9

# Replace strings by others with `regex`
print(df.replace({'\n': '<br>'}, regex=True))

       0      1      2
0  1<br>      2  3<br>
1      4      5  6<br>
2      7  8<br>      9

In short, replace() is mostly what you need to deal with when you want to replace values or strings in your DataFrame by others!

Removing Parts From Strings in the Cells of Your DataFrame
Removing unwanted parts of strings is cumbersome work. Luckily, there is an easy solution to this problem!

# Check out your DataFrame
print(df)

  class test result
0     1    2    +3b
1     4    5    -6B
2     7    8    +9A

# Delete unwanted parts from the strings in the `result` column
df['result'] = df['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))

# Check out the result again
print(df)

  class test result
0     1    2      3
1     4    5      6
2     7    8      9

You use map() on the column result to apply the lambda function over each element or element-wise of the column. The function in itself takes the string value and strips the + or - that’s located on the left, and also strips away any of the six aAbBcC on the right.

Splitting Text in a Column into Multiple Rows in a DataFrame
This is somewhat a more difficult formatting task. However, the next code chunk will walk you through the steps:

# Inspect your DataFrame `df`
print(df)

  Age PlusOne             Ticket
0  34       0           23:44:55
1  22       0           66:77:88
2  19       1  43:68:05 56:34:12

# Split out the two values in the third row
# Make it a Series
# Stack the values
ticket_series = df['Ticket'].str.split(' ').apply(pd.Series, 1).stack()

# Get rid of the stack:
# Drop the level to line up with the DataFrame
ticket_series.index = ticket_series.index.droplevel(-1)

# Make your series a dataframe 
ticketdf = pd.DataFrame(ticket_series)

print(ticketdf)

          0
0  23:44:55
1  66:77:88
2  43:68:05
2  56:34:12

# Delete the `Ticket` column from your DataFrame
del df['Ticket']

# Join the ticket DataFrame to `df`
df.join(ticketdf)

# Check out the new `df`
print(df)

  Age PlusOne
0  34       0
1  22       0
2  19       1

In short, what you do is:

- First, you inspect the DataFrame at hand. You see that the values in the last row and in the last column are a bit too long. It appears there are two tickets because a guest has taken a plus-one to the concert.

- You take the Ticket column from the DataFrame df and strings on a space. This will make sure that the two tickets will end up in two separate rows in the end. Next, you take these four values (the four ticket numbers) and put them into a Series object:

      0         1
0  23:44:55       NaN
1  66:77:88       NaN
2  43:68:05  56:34:12

That still doesn’t seem quite right. You have NaN values in there! You have to stack the Series to make sure you don’t have any NaN values in the resulting Series.

- Next, you see that your Series is stacked.

0  0    23:44:55
1  0    66:77:88
2  0    43:68:05
   1    56:34:12

That is not ideal either. That is why you drop the level to line up with the DataFrame:

0    23:44:55
1    66:77:88
2    43:68:05
2    56:34:12
dtype: object

That is what you’re looking for.

- Transform your Series to a DataFrame to make sure you can join it back to your initial DataFrame. However, to avoid having any duplicates in your DataFrame, you can delete the original Ticket column.

Applying A Function to Your Pandas DataFrame’s Columns or Rows

You might want to adjust the data in your DataFrame by applying a function to it. Let’s begin answering this question by making your own lambda function:

doubler = lambda x: x*2

Tip: if you want to know more about functions in Python, consider taking this Python functions tutorial.

# Study the `df` DataFrame
print(df)

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Apply the `doubler` function to the `A` DataFrame column
print(df['A'].apply(doubler))

0     2
1     8
2    14
Name: A, dtype: int64

Note that you can also select the row of your DataFrame and apply the doubler lambda function to it. Remember that you can easily select a row from your DataFrame by using .loc[] or .iloc[].

Then, you would execute something like this, depending on whether you want to select your index based on its position or based on its label:

df.loc[0].apply(doubler)

Note that the apply() function only applies the doubler function along the axis of your DataFrame. That means that you target either the index or the columns. Or, in other words, either a row or a column.

However, if you want to apply it to each element or element-wise, you can make use of the map() function. You can just replace the apply() function in the code chunk above with map(). Don’t forget to still pass the doubler function to it to make sure you multiply the values by 2.

Let’s say you want to apply this doubling function not only to the A column of your DataFrame but to the whole of it. In this case, you can use applymap() to apply the doubler function to every single element in the entire DataFrame:

doubled_df = df.applymap(doubler)
print(doubled_df)

    A   B   C
0   2   4   6
1   8  10  12
2  14  16  18

Note that in these cases, we have been working with lambda functions or anonymous functions that get created at runtime. However, you can also write your own function. For example:

def doubler(x):
    if x % 2 == 0:
        return x
    else:
        return x * 2

# Use `applymap()` to apply `doubler()` to your DataFrame
doubled_df = df.applymap(doubler)

# Check the DataFrame
print(doubled_df)

    A   B   C
0   2   2   6
1   4  10   6
2  14   8  18

If you want more information on the flow of control in Python, you can always read up on it here.

7. How To Create an Empty DataFrame

The function that you will use is the Pandas Dataframe() function: it requires you to pass the data that you want to put in, the indices and the columns.

Remember that the data that is contained within the data frame doesn’t have to be homogenous. It can be of different data types!

There are several ways in which you can use this function to make an empty DataFrame. Firstly, you can use numpy.nan to initialize your data frame with NaNs. Note that numpy.nan has type float.

df = pd.DataFrame(np.nan, index=[0,1,2,3], columns=['A'])
print(df)

    A
0 NaN
1 NaN
2 NaN
3 NaN

Right now, the data type of the data frame is inferred by default: because numpy.nan has type float, the data frame will also contain values of type float. You can, however, also force the DataFrame to be of a particular type by adding the attribute dtype and filling in the desired type. Just like in this example:

df = pd.DataFrame(index=range(0,4),columns=['A'], dtype='float')
print(df)

    A
0 NaN
1 NaN
2 NaN
3 NaN

Note that if you don’t specify the axis labels or index, they will be constructed from the input data based on common sense rules.

8. Does Pandas Recognize Dates When Importing Data?

Pandas can recognize it, but you need to help it a tiny bit: add the argument parse_dates when you’reading in data from, let’s say, a comma-separated value (CSV) file:

import pandas as pd
pd.read_csv('yourFile', parse_dates=True)

# or this option:
pd.read_csv('yourFile', parse_dates=['columnName'])

There are, however, always weird date-time formats.

No worries! In such cases, you can construct your own parser to deal with this. You could, for example, make a lambda function that takes your DateTime and controls it with a format string.

import pandas as pd
dateparser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

# Which makes your read command:
pd.read_csv(infile, parse_dates=['columnName'], date_parser=dateparse)

# Or combine two columns into a single DateTime column
pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)

9. When, Why And How You Should Reshape Your Pandas DataFrame

Reshaping your DataFrame is transforming it so that the resulting structure makes it more suitable for your data analysis. In other words, reshaping is not so much concerned with formatting the values that are contained within the DataFrame, but more about transforming the shape of it.

This answers the when and why. But how would you reshape your DataFrame?

There are three ways of reshaping that frequently raise questions with users: pivoting, stacking and unstacking and melting.

Pivotting Your DataFrame

You can use the pivot() function to create a new derived table out of your original one. When you use the function, you can pass three arguments:

values: this argument allows you to specify which values of your original DataFrame you want to see in your pivot table.

columns: whatever you pass to this argument will become a column in your resulting table.

index: whatever you pass to this argument will become an index in your resulting table.

# Import pandas
import pandas as pd

# Create your DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
        'price':[11.42, 23.50, 19.99, 15.95, 55.75, 111.55],
        'testscore': [4, 3, 5, 7, 5, 8]})

print(products)

        category   price    store  testscore
0       Cleaning   11.42  Walmart          4
1       Cleaning   23.50      Dia          3
2  Entertainment   19.99  Walmart          5
3  Entertainment   15.95     Fnac          7
4           Tech   55.75      Dia          5
5           Tech  111.55  Walmart          8

# Use `pivot()` to pivot the DataFrame
pivot_products = products.pivot(index='category', columns='store', values='price')

# Check out the result
print(pivot_products)

store            Dia   Fnac  Walmart
category                            
Cleaning       23.50    NaN    11.42
Entertainment    NaN  15.95    19.99
Tech           55.75    NaN   111.55

When you don’t specifically fill in what values you expect to be present in your resulting table, you will pivot by multiple columns:

# Import the Pandas library
import pandas as pd

# Construct the DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
                        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
                        'price':[11.42, 23.50, 19.99, 15.95, 55.75, 111.55],
                        'testscore': [4, 3, 5, 7, 5, 8]})

print(products)

            category   price    store  testscore
    0       Cleaning   11.42  Walmart          4
    1       Cleaning   23.50      Dia          3
    2  Entertainment   19.99  Walmart          5
    3  Entertainment   15.95     Fnac          7
    4           Tech   55.75      Dia          5
    5           Tech  111.55  Walmart          8

# Use `pivot()` to pivot your DataFrame
pivot_products = products.pivot(index='category', columns='store')

# Check out the results
print(pivot_products)

                   price                testscore             
    store            Dia   Fnac Walmart       Dia Fnac Walmart
    category                                                  
    Cleaning       23.50    NaN   11.42       3.0  NaN     4.0
    Entertainment    NaN  15.95   19.99       NaN  7.0     5.0
    Tech           55.75    NaN  111.55       5.0  NaN     8.0

Note that your data can not have rows with duplicate values for the columns that you specify. If this is not the case, you will get an error message. If you can’t ensure the uniqueness of your data, you will want to use the pivot_table method instead:

# Import the Pandas library
import pandas as pd

# Your DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
                        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
                        'price':[11.42, 23.50, 19.99, 15.95, 19.99, 111.55],
                        'testscore': [4, 3, 5, 7, 5, 8]})

print(products)

        category   price    store  testscore
0       Cleaning   11.42  Walmart          4
1       Cleaning   23.50      Dia          3
2  Entertainment   19.99  Walmart          5
3  Entertainment   15.95     Fnac          7
4           Tech   19.99      Dia          5
5           Tech  111.55  Walmart          8

# Pivot your `products` DataFrame with `pivot_table()`
pivot_products = products.pivot_table(index='category', columns='store', values='price', aggfunc='mean')

# Check out the results
print(pivot_products) 

store            Dia   Fnac  Walmart
category                            
Cleaning       23.50    NaN    11.42
Entertainment    NaN  15.95    19.99
Tech           19.99    NaN   111.55

Note the additional argument aggfunc that gets passed to the pivot_table method. This argument indicates that you use an aggregation function used to combine multiple values. In this example, you can clearly see that the mean function is used.

Using stack() and unstack() to Reshape Your Pandas DataFrame

You have already seen an example of stacking in the answer to question 5! In essence, you might still remember that, when you stack a DataFrame, you make it taller. You move the innermost column index to become the innermost row index. You return a DataFrame with an index with a new inner-most level of row labels.

Go back to the full walk-through of the answer to question 5 if you’re unsure of the workings of stack().
The inverse of stacking is called unstacking. Much like stack(), you use unstack() to move the innermost row index to become the innermost column index.

For a good explanation of pivoting, stacking and unstacking, go to this page.

Reshape Your DataFrame With melt()

Melting is considered useful in cases where you have data that has one or more columns that are identifier variables, while all other columns are considered measured variables.

These measured variables are all “unpivoted” to the row axis. That is, while the measured variables that were spread out over the width of the DataFrame, the melt will make sure that they will be placed in the height of it. Or, yet in other words, your DataFrame will now become longer instead of wider.

As a result, you have two non-identifier columns, namely, ‘variable’ and ‘value’.

Let’s illustrate this with an example:

# The `people` DataFrame
people = pd.DataFrame({'FirstName' : ['John', 'Jane'],
                       'LastName' : ['Doe', 'Austen'],
                       'BloodType' : ['A-', 'B+'],
                       'Weight' : [90, 64]})

print(people)

  BloodType FirstName LastName  Weight
0        A-      John      Doe      90
1        B+      Jane   Austen      64

# Use `melt()` on the `people` DataFrame
print(pd.melt(people, id_vars=['FirstName', 'LastName'], var_name='measurements'))  

  FirstName LastName measurements value
0      John      Doe    BloodType    A-
1      Jane   Austen    BloodType    B+
2      John      Doe       Weight    90
3      Jane   Austen       Weight    64

If you’re looking for more ways to reshape your data, check out the documentation.

10. How To Iterate Over a Pandas DataFrame

You can iterate over the rows of your DataFrame with the help of a for loop in combination with an iterrows() call on your DataFrame:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

print(df)

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

for index, row in df.iterrows() :
    print(row['A'], row['B'])

1 2
4 5
7 8

iterrows() allows you to efficiently loop over your DataFrame rows as (index, Series) pairs. In other words, it gives you (index, row) tuples as a result.

11. How To Write a Pandas DataFrame to a File

When you have done your data munging and manipulation with Pandas, you might want to export the DataFrame to another format. This section will cover two ways of outputting your DataFrame: to a CSV or to an Excel file.

Output a DataFrame to CSV

To write a DataFrame as a CSV file, you can use to_csv():

import pandas as pd
df.to_csv('myDataFrame.csv')

That piece of code seems quite simple, but this is just where the difficulties begin for most people because you will have specific requirements for the output of your data. Maybe you don’t want a comma as a delimiter, or you want to specify a specific encoding, …

Don’t worry! You can pass some additional arguments to to_csv() to make sure that your data is outputted the way you want it to be!

- To delimit by a tab, use the sep argument:

import pandas as pd
df.to_csv('myDataFrame.csv', sep='\t')

- To use a specific character encoding, you can use the encoding argument:

import pandas as pd
df.to_csv('myDataFrame.csv', sep='\t', encoding='utf-8')

- Furthermore, you can specify how you want your NaN or missing values to be represented, whether or not you want to output the header, whether or not you want to write out the row names, whether you want compression, … Read up on the options here.

Writing a DataFrame to Excel

Similarly to what you did to output your DataFrame to CSV, you can use to_excel() to write your table to Excel. However, it is a bit more complicated:

import pandas as pd
writer = pd.ExcelWriter('myDataFrame.xlsx')
df.to_excel(writer, 'DataFrame')
writer.save()

Note, however, that, just like with to_csv(), you have a lot of extra arguments such as startcol, startrow, and so on, to make sure output your data correctly. Go to this page to read up on them.

If, however, you want more information on IO tools in Pandas, you check out this page.

Python For Data Science Is More Than DataFrames

That’s it! You've successfully completed the Pandas DataFrame tutorial!

The answers to the 11 frequently asked Pandas questions represent essential functions that you will need to import, clean and manipulate your data for your data science work. Are you not sure that you have gone deep enough into this matter? Our Importing Data In Python course will help you out! If you’ve got the hang out of this, you might want to see Pandas at work in a real-life project. The Importance of Preprocessing in Data Science and the Machine Learning Pipeline tutorial series is a must-read, and the open course Introduction to Python & Machine Learning is a must-complete.

==
Good:

https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python

Python Numpy Array Tutorial
A NumPy tutorial for beginners in which you'll learn how to create a NumPy array, use broadcasting, access values, manipulate arrays, and much more.
NumPy is, just like SciPy, Scikit-Learn, Pandas, etc. one of the packages that you just can’t miss when you’re learning data science, mainly because this library provides you with an array data structure that holds some benefits over Python lists, such as: being more compact, faster access in reading and writing items, being more convenient and more efficient.

Today’s post will focus precisely on this. This NumPy tutorial will not only show you what NumPy arrays actually are and how you can install Python, but you’ll also learn how to make arrays (even when your data comes from files!), how broadcasting works, how you can ask for help, how to manipulate your arrays and how to visualize them.

Content
What Is A Python Numpy Array?
How To Install Numpy
How To Make NumPy Arrays
How NumPy Broadcasting Works
How Do Array Mathematics Work?
How To Subset, Slice, And Index Arrays
How To Ask For Help
How To Manipulate Arrays
How To Visualize NumPy Arrays
Beyond Data Analysis with NumPy

What Is A Python Numpy Array?
You already read in the introduction that NumPy arrays are a bit like Python lists, but still very much different at the same time. For those of you who are new to the topic, let’s clarify what it exactly is and what it’s good for.

As the name gives away, a NumPy array is a central data structure of the numpy library. The library’s name is short for “Numeric Python” or “Numerical Python”.

This already gives an idea of what you’re dealing with, right?

In other words, NumPy is a Python library that is the core library for scientific computing in Python. It contains a collection of tools and techniques that can be used to solve on a computer mathematical models of problems in Science and Engineering. One of these tools is a high-performance multidimensional array object that is a powerful data structure for efficient computation of arrays and matrices. To work with these arrays, there’s a vast amount of high-level mathematical functions operate on these matrices and arrays.

Then, what is an array?

When you look at the print of a couple of arrays, you could see it as a grid that contains values of the same type:


# Print the array
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 2d array
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 3d array
print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

You see that, in the example above, the data are integers. The array holds and represents any regular data in a structured way.

However, you should know that, on a structural level, an array is basically nothing but pointers. It’s a combination of a memory address, a data type, a shape, and strides:

The data pointer indicates the memory address of the first byte in the array,
The data type or dtype pointer describes the kind of elements that are contained within the array,
The shape indicates the shape of the array, and
The strides are the number of bytes that should be skipped in memory to go to the next element. If your strides are (10,1), you need to proceed one byte to get to the next column and 10 bytes to locate the next row.

Or, in other words, an array contains information about the raw data, how to locate an element and how to interpret an element.

Enough of the theory. Let’s check this out ourselves:

You can easily test this by exploring the numpy array attributes:

# Print out memory address
print(my_2d_array.data)

<memory at 0x7f58cb57fa68>

# Print out the shape of `my_array`
print(my_2d_array.shape)

(2, 4)

# Print out the data type of `my_array`
print(my_2d_array.dtype)

int64

# Print out the stride of `my_array`
print(my_2d_array.strides)

(32, 8)

You see that now, you get a lot more information: for example, the data type that is printed out is ‘int64’ or signed 32-bit integer type; This is a lot more detailed! That also means that the array is stored in memory as 64 bytes (as each integer takes up 8 bytes and you have an array of 8 integers). The strides of the array tell us that you have to skip 8 bytes (one value) to move to the next column, but 32 bytes (4 values) to get to the same position in the next row. As such, the strides for the array will be (32,8).

Note that if you set the data type to int32, the strides tuple that you get back will be (16, 4), as you will still need to move one value to the next column and 4 values to get the same position. The only thing that will have changed is the fact that each integer will take up 4 bytes instead of 8.

(A picture)

The array that you see above is, as its name already suggested, a 2-dimensional array: you have rows and columns. The rows are indicated as the “axis 0”, while the columns are the “axis 1”. The number of the axis goes up accordingly with the number of the dimensions: in 3-D arrays, of which you have also seen an example in the previous code chunk, you’ll have an additional “axis 2”. Note that these axes are only valid for arrays that have at least 2 dimensions, as there is no point in having this for 1-D arrays;

These axes will come in handy later when you’re manipulating the shape of your NumPy arrays.

How To Install Numpy
Before you can start to try out these NumPy arrays for yourself, you first have to make sure that you have it installed locally (assuming that you’re working on your pc). If you have the Python library already available, go ahead and skip this section :)

If you still need to set up your environment, you must be aware that there are two major ways of installing NumPy on your pc: with the help of Python wheels or the Anaconda Python distribution.

… With Python Wheels
Make sure firstly that you have Python installed. You can go here if you still need to do this :)

If you’re working on Windows, make sure that you have added Python to the PATH environment variable. Then, don’t forget to install a package manager, such as pip, which will ensure that you’re able to use Python’s open-source libraries.

Note that recent versions of Python 3 come with pip, so double check if you have it and if you do, upgrade it before you install NumPy:

pip install pip --upgrade
pip --version

Next, you can go here or here to get your NumPy wheel. After you have downloaded it, navigate to the folder on your pc that stores it through the terminal and install it:

install "numpy-1.9.2rc1+mkl-cp34-none-win_amd64.whl"
import numpy
numpy.__version__

The two last lines allow you to verify that you have installed NumPy and check the version of the package.

After these steps, you’re ready to start using NumPy!

… With The Anaconda Python Distribution

To get NumPy, you could also download the Anaconda Python distribution. This is easy and will allow you to get started quickly! If you haven’t downloaded it already, go here to get it. Follow the instructions to install, and you're ready to start!

Do you wonder why this might actually be easier?

The good thing about getting this Python distribution is the fact that you don’t need to worry too much about separately installing NumPy or any of the major packages that you’ll be using for your data analyses, such as pandas, scikit-learn, etc.

Because, especially if you’re very new to Python, programming or terminals, it can really come as a relief that Anaconda already includes 100 of the most popular Python, R and Scala packages for data science. But also for more seasoned data scientists, Anaconda is the way to go if you want to get started quickly on tackling data science problems.

What’s more, Anaconda also includes several open source development environments such as Jupyter and Spyder. If you’d like to start working with Jupyter Notebook after this tutorial, go to this page.

In short, consider downloading Anaconda to get started on working with numpy and other packages that are relevant to data science!

How To Make NumPy Arrays

So, now that you have set up your environment, it’s time for the real work. Admittedly, you have already tried out some stuff with arrays in the above DataCamp Light chunks. However, you haven’t really gotten any real hands-on practice with them, because you first needed to install NumPy on your own pc. Now that you have done this, it’s time to see what you need to do in order to run the above code chunks on your own.

Some exercises have been included below so that you can already practice how it’s done before you start on your own!

To make a numpy array, you can just use the np.array() function. All you need to do is pass a list to it, and optionally, you can also specify the data type of the data. If you want to know more about the possible data types that you can pick, go here or consider taking a brief look at DataCamp’s NumPy cheat sheet.

There’s no need to go and memorize these NumPy data types if you’re a new user; But you do have to know and care what data you’re dealing with. The data types are there when you need more control over how your data is stored in memory and on disk. Especially in cases where you’re working with extensive data, it’s good that you know to control the storage type.

Don’t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. The NumPy library follows an import convention: when you import this library, you have to make sure that you import it as np. By doing this, you’ll make sure that other Pythonistas understand your code more easily.

In the following example you’ll create the my_array array that you have already played around with above:

# Import `numpy` as `np`
import numpy as np

# Make the array `my_array`
my_array = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.int64)

# Print `my_array`
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

If you would like to know more about how to make lists, go here.

However, sometimes you don’t know what data you want to put in your array, or you want to import data into a numpy array from another source. In those cases, you’ll make use of initial placeholders or functions to load data from text into arrays, respectively.

The following sections will show you how to do this.

How To Make An “Empty” NumPy Array

What people often mean when they say that they are creating “empty” arrays is that they want to make use of initial placeholders, which you can fill up afterward. You can initialize arrays with ones or zeros, but you can also create arrays that get filled up with evenly spaced values, constant or random values.

However, you can still make a totally empty array, too.

Luckily for us, there are quite a lot of functions to make

Try it all out below!

# Create an array of ones
np.ones((3,4))

# Create an array of zeros
np.zeros((2,3,4),dtype=np.int16)

# Create an array with random values
np.random.random((2,2))

# Create an empty array
np.empty((3,2))

# Create a full array
np.full((2,2),7)

# Create an array of evenly-spaced values
np.arange(10,25,5)

# Create an array of evenly-spaced values
np.linspace(0,2,9)

Tip: play around with the above functions so that you understand how they work!

- For some, such as np.ones(), np.random.random(), np.empty(), np.full() or np.zeros() the only thing that you need to do in order to make arrays with ones or zeros is pass the shape of the array that you want to make. As an option to np.ones() and np.zeros(), you can also specify the data type. In the case of np.full(), you also have to specify the constant value that you want to insert into the array.

- With np.linspace() and np.arange() you can make arrays of evenly spaced values. The difference between these two functions is that the last value of the three that are passed in the code chunk above designates either the step value for np.linspace() or a number of samples for np.arange(). What happens in the first is that you want, for example, an array of 9 values that lie between 0 and 2. For the latter, you specify that you want an array to start at 10 and per steps of 5, generate values for the array that you’re creating.

Remember that NumPy also allows you to create an identity array or matrix with np.eye() and np.identity(). An identity matrix is a square matrix of which all elements in the principal diagonal are ones, and all other elements are zeros. When you multiply a matrix with an identity matrix, the given matrix is left unchanged.

In other words, if you multiply a matrix by an identity matrix, the resulting product will be the same matrix again by the standard conventions of matrix multiplication.

Even though the focus of this tutorial is not on demonstrating how identity matrices work, it suffices to say that identity matrices are useful when you’re starting to do matrix calculations: they can simplify mathematical equations, which makes your computations more efficient and robust.

How To Load NumPy Arrays From Text

Creating arrays with the help of initial placeholders or with some example data is an excellent way of getting started with numpy. But when you want to get started with data analysis, you’ll need to load data from text files.

With that what you have seen up until now, you won’t really be able to do much. Make use of some specific functions to load data from your files, such as loadtxt() or genfromtxt().

Let’s say you have the following text files with data:

# This is your data in the text file
# Value1  Value2  Value3
# 0.2536  0.1008  0.3857
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  0.5929
# 0.1781  0.3049  0.8928
# 0.6253  0.3486  0.8791

# Import your data
x, y, z = np.loadtxt('data.txt',
                    skiprows=1,
                    unpack=True)

In the code above, you use loadtxt() to load the data in your environment. You see that the first argument that both functions take is the text file data.txt. Next, there are some specific arguments for each: in the first statement, you skip the first row, and you return the columns as separate arrays with unpack=TRUE. This means that the values in column Value1 will be put in x, and so on.

Note that, in case you have comma-delimited data or if you want to specify the data type, there are also the arguments delimiter and dtype that you can add to the loadtxt() arguments.

That’s easy and straightforward, right?

Let’s take a look at your second file with data:

# Your data in the text file
# Value1  Value2  Value3
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  MISSING
# 0.1781  0.3049  0.8928
# MISSING 0.5801  0.2038
# 0.5993  0.4357  0.7410

my_array2 = np.genfromtxt('data2.txt',
                      skip_header=1,
                      filling_values=-999)

You see that here, you resort to genfromtxt() to load the data. In this case, you have to handle some missing values that are indicated by the 'MISSING' strings. Since the genfromtxt() function converts character strings in numeric columns to nan, you can convert these values to other ones by specifying the filling_values argument. In this case, you choose to set the value of these missing values to -999.

If by any chance, you have values that don’t get converted to nan by genfromtxt(), there’s always the missing_values argument that allows you to specify what the missing values of your data exactly are.

But this is not all.

Tip: check out this page to see what other arguments you can add to import your data successfully.

You now might wonder what the difference between these two functions really is.

The examples indicated this maybe implicitly, but, in general, genfromtxt() gives you a little bit more flexibility; It’s more robust than loadtxt().

Let’s make this difference a little bit more practical: the latter, loadtxt(), only works when each row in the text file has the same number of values; So when you want to handle missing values easily, you’ll typically find it easier to use genfromtxt().

But this is definitely not the only reason.

A brief look on the number of arguments that genfromtxt() has to offer will teach you that there is really a lot more things that you can specify in your import, such as the maximum number of rows to read or the option to automatically strip white spaces from variables.

How To Save NumPy Arrays

Once you have done everything that you need to do with your arrays, you can also save them to a file. If you want to save the array to a text file, you can use the savetxt() function to do this:

import numpy as np
x = np.arange(0.0,5.0,1.0)
np.savetxt('test.out', x, delimiter=',')

Remember that np.arange() creates a NumPy array of evenly-spaced values. The third value that you pass to this function is the step value.

There are, of course, other ways to save your NumPy arrays to text files. Check out the functions in the table below if you want to get your data to binary files or archives:

save(): Save an array to a binary file in NumPy .npy format

savez(): Save several arrays into an uncompressed .npz archive

savez_compressed(): Save several arrays into a compressed .npz archive

For more information or examples of how you can use the above functions to save your data, go here or make use of one of the help functions that NumPy has to offer to get to know more instantly!

Are you not sure what these NumPy help functions are?

No worries! You’ll learn more about them in one of the next sections!

How To Inspect Your NumPy Arrays

Besides the array attributes that have been mentioned above, namely, data, shape, dtype and strides, there are some more that you can use to easily get to know more about your arrays. The ones that you might find interesting to use when you’re just starting out are the following:

# Print the number of `my_array`'s dimensions
print(my_array.ndim)

2

# Print the number of `my_array`'s elements
print(my_array.size)

8

# Print information about `my_array`'s memory layout
print(my_array.flags)

C_CONTIGUOUS : True
F_CONTIGUOUS : False
OWNDATA : True
WRITEABLE : True
ALIGNED : True
UPDATEIFCOPY : False

# Print the length of one array element in bytes
print(my_array.itemsize)

8

# Print the total consumed bytes by `my_array`'s elements
print(my_array.nbytes)

64

These are almost all the attributes that an array can have.

Don’t worry if you don’t feel that all of them are useful for you at this point; This is fairly normal, because, just like you read in the previous section, you’ll only get to worry about memory when you’re working with large data sets.

Also note that, besides the attributes, you also have some other ways of gaining more information on and even tweaking your array slightly:

# Print the length of `my_array`
print(len(my_array))

2

# Change the data type of `my_array`
my_array.astype(float)

array([[ 1.,  2.,  3.,  4.],
       [ 5.,  6.,  7.,  8.]])

Now that you have made your array, either by making one yourself with the np.array() or one of the initial placeholder functions, or by loading in your data through the loadtxt() or genfromtxt() functions, it’s time to look more closely into the second key element that really defines the NumPy library: scientific computing.

How NumPy Broadcasting Works

Before you go deeper into scientific computing, it might be a good idea to first go over what broadcasting exactly is: it’s a mechanism that allows NumPy to work with arrays of different shapes when you’re performing arithmetic operations.

To put it in a more practical context, you often have an array that’s somewhat larger and another one that’s slightly smaller. Ideally, you want to use the smaller array multiple times to perform an operation (such as a sum, multiplication, etc.) on the larger array.

To do this, you use the broadcasting mechanism.

However, there are some rules if you want to use it. And, before you already sigh, you’ll see that these “rules” are very simple and kind of straightforward!

- First off, to make sure that the broadcasting is successful, the dimensions of your arrays need to be compatible. Two dimensions are compatible when they are equal. Consider the following example:

# Initialize `x`
x = np.ones((3,4))

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.random.random((3,4))

# Check shape of `y`
print(y.shape)

(3, 4)

# Add `x` and `y`
x + y

array([[ 1.69646919,  1.28613933,  1.22685145,  1.55131477],
       [ 1.71946897,  1.42310646,  1.9807642 ,  1.68482974],
       [ 1.4809319 ,  1.39211752,  1.34317802,  1.72904971]])

- Two dimensions are also compatible when one of them is 1:

# Import `numpy` as `np`
import numpy as np

# Initialize `x`
x = np.ones((3,4))

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.arange(4)

# Check shape of `y`
print(y.shape)

(4,)

# Subtract `x` and `y`
x - y 

array([[ 1.,  0., -1., -2.],
       [ 1.,  0., -1., -2.],
       [ 1.,  0., -1., -2.]])

Note that if the dimensions are not compatible, you will get a ValueError.

Tip: also test what the size of the resulting array is after you have done the computations! You’ll see that the size is actually the maximum size along each dimension of the input arrays.

In other words, you see that the result of x-y gives an array with shape (3,4): y had a shape of (4,) and x had a shape of (3,4). The maximum size along each dimension of x and y is taken to make up the shape of the new, resulting array.

- Lastly, the arrays can only be broadcast together if they are compatible in all dimensions. Consider the following example:

# Import `numpy` as `np`
import numpy as np

# Initialize `x` and `y`
x = np.ones((3,4))
y = np.random.random((5,1,4))

# Add `x` and `y`
x + y

array([[[ 1.69646919,  1.28613933,  1.22685145,  1.55131477],
        [ 1.69646919,  1.28613933,  1.22685145,  1.55131477],
        [ 1.69646919,  1.28613933,  1.22685145,  1.55131477]],

       [[ 1.71946897,  1.42310646,  1.9807642 ,  1.68482974],
        [ 1.71946897,  1.42310646,  1.9807642 ,  1.68482974],
        [ 1.71946897,  1.42310646,  1.9807642 ,  1.68482974]],

       [[ 1.4809319 ,  1.39211752,  1.34317802,  1.72904971],
        [ 1.4809319 ,  1.39211752,  1.34317802,  1.72904971],
        [ 1.4809319 ,  1.39211752,  1.34317802,  1.72904971]],

       [[ 1.43857224,  1.0596779 ,  1.39804426,  1.73799541],
        [ 1.43857224,  1.0596779 ,  1.39804426,  1.73799541],
        [ 1.43857224,  1.0596779 ,  1.39804426,  1.73799541]],

       [[ 1.18249173,  1.17545176,  1.53155137,  1.53182759],
        [ 1.18249173,  1.17545176,  1.53155137,  1.53182759],
        [ 1.18249173,  1.17545176,  1.53155137,  1.53182759]]])

You see that, even though x and y seem to have somewhat different dimensions, the two can be added together.

That is because they are compatible in all dimensions:

- Array x has dimensions 3 X 4,
- Array y has dimensions 5 X 1 X 4

Since you have seen above that dimensions are also compatible if one of them is equal to 1, you see that these two arrays are indeed a good candidate for broadcasting!

What you will notice is that in the dimension where y has size 1, and the other array has a size greater than 1 (that is, 3), the first array behaves as if it were copied along that dimension.

Note that the shape of the resulting array will again be the maximum size along each dimension of x and y: the dimension of the result will be (5,3,4)

In short, if you want to make use of broadcasting, you will rely a lot on the shape and dimensions of the arrays with which you’re working.

But what if the dimensions are not compatible?

What if they are not equal or if one of them is not equal to 1?

You’ll have to fix this by manipulating your array! You’ll see how to do this in one of the next sections.

How Do Array Mathematics Work?

You’ve seen that broadcasting is handy when you’re doing arithmetic operations. In this section, you’ll discover some of the functions that you can use to do mathematics with arrays.

As such, it probably won’t surprise you that you can just use +, -, *, / or % to add, subtract, multiply, divide or calculate the remainder of two (or more) arrays. However, a big part of why NumPy is so handy, is because it also has functions to do this. The equivalent functions of the operations that you have seen just now are, respectively, np.add(), np.subtract(), np.multiply(), np.divide() and np.remainder().

You can also easily do exponentiation and taking the square root of your arrays with np.exp() and np.sqrt(), or calculate the sines or cosines of your array with np.sin() and np.cos(). Lastly, its’ also useful to mention that there’s also a way for you to calculate the natural logarithm with np.log() or calculate the dot product by applying the dot() to your array.

Try it all out in the DataCamp Light chunk below.

Just a tip: make sure to check out first the arrays that have been loaded for this exercise!

# Add `x` and `y`
np.add(x,y)

# Subtract `x` and `y`
np.subtract(x,y)

# Multiply `x` and `y`
np.multiply(x,y)

# Divide `x` and `y`
np.divide(x,y)

# Calculate the remainder of `x` and `y`
np.remainder(x,y)


Remember how broadcasting works? Check out the dimensions and the shapes of both x and y in your IPython shell. Are the rules of broadcasting respected?

But there is more.

Check out this small list of aggregate functions:

a.sum(): Array-wise sum
a.min(): Array-wise minimum value
b.max(axis=0): Maximum value of an array row
b.cumsum(axis=1): Cumulative sum of the elements
a.mean(): Mean
b.median(): Median
a.corrcoef(): Correlation coefficient
np.std(b): Standard deviation

Besides all of these functions, you might also find it useful to know that there are mechanisms that allow you to compare array elements. For example, if you want to check whether the elements of two arrays are the same, you might use the == operator. To check whether the array elements are smaller or bigger, you use the < or > operators.

This all seems quite straightforward, yes?

However, you can also compare entire arrays with each other! In this case, you use the np.array_equal() function. Just pass in the two arrays that you want to compare with each other, and you’re done.

Note that, besides comparing, you can also perform logical operations on your arrays. You can start with np.logical_or(), np.logical_not() and np.logical_and(). This basically works like your typical OR, NOT and AND logical operations;

In the simplest example, you use OR to see whether your elements are the same (for example, 1), or if one of the two array elements is 1. If both of them are 0, you’ll return FALSE. You would use AND to see whether your second element is also 1 and NOT to see if the second element differs from 1.

Test this out in the code chunk below:

# `a` AND `b` 
np.logical_and(a, b)

# `a` OR `b`
np.logical_or(a, b)

# `a` NOT `b`
np.logical_not(a,b)

How To Subset, Slice, And Index Arrays

Besides mathematical operations, you might also consider taking just a part of the original array (or the resulting array) or just some array elements to use in further analysis or other operations. In such case, you will need to subset, slice and/or index your arrays.

These operations are very similar to when you perform them on Python lists. If you want to check out the similarities for yourself, or if you want a more elaborate explanation, you might consider checking out DataCamp’s Python list tutorial.

If you have no clue at all on how these operations work, it suffices for now to know these two basic things:

- You use square brackets [] as the index operator, and

- Generally, you pass integers to these square brackets, but you can also put a colon : or a combination of the colon with integers in it to designate the elements/rows/columns you want to select.

Besides from these two points, the easiest way to see how this all fits together is by looking at some examples of subsetting:

# Select the element at the 1st index
print(my_array[1])

2

# Select the element at row 1 column 2
print(my_2d_array[1][2])

7

# Select the element at row 1 column 2
print(my_2d_array[1,2])

7

# Select the element at row 1, column 2 and 
print(my_3d_array[1,1,2])

11

Something a little bit more advanced than subsetting, if you will, is slicing. Here, you consider not just particular values of your arrays, but you go to the level of rows and columns. You’re basically working with “regions” of data instead of pure “locations”.

You can see what is meant with this analogy in these code examples:

# Select items at index 0 and 1
print(my_array[0:2])

[1 2]

# Select items at row 0 and 1, column 1
print(my_2d_array[0:2,1])

[2 6]

# Select items at row 1
# This is the same as saying `my_3d_array[1,:,:]
print(my_3d_array[1,...])

[[ 1  2  3  4]
 [ 9 10 11 12]]

You’ll see that, in essence, the following holds:

a[start:end] # items start through the end (but the end is not included!)
a[start:]    # items start through the rest of the array
a[:end]      # items from the beginning through the end (but the end is not included!)

Lastly, there’s also indexing. When it comes to NumPy, there are boolean indexing and advanced or “fancy” indexing.

(In case you’re wondering, this is true NumPy jargon, I didn’t make the last one up!)

First up is boolean indexing. Here, instead of selecting elements, rows or columns based on index number, you select those values from your array that fulfill a certain condition.

Putting this into code can be pretty easy:

# Try out a simple example
print(my_array[my_array<2])

[1]

# Specify a condition
bigger_than_3 = (my_3d_array >= 3)

# Use the condition to index our 3d array
print(my_3d_array[bigger_than_3])

[ 3  4  5  6  7  8  3  4  9 10 11 12]

Note that, to specify a condition, you can also make use of the logical operators | (OR) and & (AND). If you would want to rewrite the condition above in such a way (which would be inefficient, but I demonstrate it here for educational purposes :)), you would get 

bigger_than_3 = (my_3d_array > 3) | (my_3d_array == 3).

With the arrays that have been loaded in, there aren’t too many possibilities, but with arrays that contain for example, names or capitals, the possibilities could be endless!

When it comes to fancy indexing, that what you basically do with it is the following: you pass a list or an array of integers to specify the order of the subset of rows you want to select out of the original array.

Does this sound a little bit abstract to you?

No worries, just try it out in the code chunk below:

# Select elements at (1,0), (0,1), (1,2) and (0,0)
print(my_2d_array[[1, 0, 1, 0],[0, 1, 2, 0]])

[5 2 7 1]

# Select a subset of the rows and columns
print(my_2d_array[[1, 0, 1, 0]][:,[0,1,2,0]])

[[5 6 7 5]
 [1 2 3 1]
 [5 6 7 5]
 [1 2 3 1]]

Now, the second statement might seem to make less sense to you at first sight. This is normal. It might make more sense if you break it down:

- If you just execute my_2d_array[[1,0,1,0]], the result is the following:

array([[5, 6, 7, 8],
   [1, 2, 3, 4],
   [5, 6, 7, 8],
   [1, 2, 3, 4]])

- What the second part, namely, [:,[0,1,2,0]], is tell you that you want to keep all the rows of this result, but that you want to change the order of the columns around a bit. You want to display the columns 0, 1, and 2 as they are right now, but you want to repeat column 0 as the last column instead of displaying column number 3. This will give you the following result:

array([[5, 6, 7, 5],
   [1, 2, 3, 1],
   [5, 6, 7, 5],
   [1, 2, 3, 1]])

Advanced indexing clearly holds no secrets for you any more!

How To Ask For Help

As a short intermezzo, you should know that you can always ask for more information about the modules, functions or classes that you’re working with, especially becauseNumPy can be quite something when you first get started on working with it.

Asking for help is fairly easy.

You just make use of the specific help functions that numpy offers to set you on your way:

- Use lookfor() to do a keyword search on docstrings. This is specifically handy if you’re just starting out, as the ‘theory’ behind it all might fade in your memory. The one downside is that you have to go through all of the search results if your query is not that specific, as is the case in the code example below. This might make it even less overviewable for you.

- Use info() for quick explanations and code examples of functions, classes, or modules. If you’re a person that learns by doing, this is the way to go! The only downside about using this function is probably that you need to be aware of the module in which certain attributes or functions are in. If you don’t know immediately what is meant by that, check out the code example below.

You see, both functions have their advantages and disadvantages, but you’ll see for yourself why both of them can be useful: try them out for yourself in the DataCamp Light code chunk below!

# Look up info on `mean` with `np.lookfor()` 
print(np.lookfor("mean"))

Search results for 'mean'
-------------------------
numpy.mean
    Compute the arithmetic mean along the specified axis.
numpy.nanmean
    Compute the arithmetic mean along the specified axis, ignoring NaNs.
numpy.ma.mean
    Returns the average of the array elements along given axis.
numpy.matrix.mean
    Returns the average of the matrix elements along the given axis.
numpy.array_equiv
    Returns True if input arrays are shape consistent and all elements equal.
numpy.ma.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.chararray.mean
    Returns the average of the array elements along given axis.
numpy.ma.fix_invalid
    Return input with invalid data masked and replaced by a fill value.
numpy.ma.MaskedArray.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.polynomial.polyutils.trimcoef
    Remove "small" "trailing" coefficients from a polynomial.
numpy.exp
    Calculate the exponential of all elements in the input array.
numpy.pad
    Pads an array.
numpy.put
    Replaces specified elements of an array with given values.
numpy.std
    Compute the standard deviation along the specified axis.
numpy.sum
    Sum of array elements over a given axis.
numpy.var
    Compute the variance along the specified axis.
numpy.copy
    Return an array copy of the given object.
numpy.prod
    Return the product of array elements over a given axis.
numpy.take
    Take elements from an array along an axis.
numpy.isnan
    Test element-wise for NaN and return result as a boolean array.
numpy.ravel
    Return a contiguous flattened array.
numpy.copyto
    Copies values from one array to another, broadcasting as necessary.
numpy.einsum
    Evaluates the Einstein summation convention on the operands.
numpy.kaiser
    Return the Kaiser window.
numpy.median
    Compute the median along the specified axis.
numpy.nanmax
    Return the maximum of an array or maximum along an axis, ignoring any
numpy.nanmin
    Return minimum of an array or minimum along an axis, ignoring any NaNs.
numpy.nanstd
    Compute the standard deviation along the specified axis, while
numpy.nansum
    Return the sum of array elements over a given axis treating Not a
numpy.nanvar
    Compute the variance along the specified axis, while ignoring NaNs.
numpy.nditer
    Efficient multi-dimensional iterator object to iterate over arrays.
numpy.average
    Compute the weighted average along the specified axis.
numpy.hamming
    Return the Hamming window.
numpy.hanning
    Return the Hanning window.
numpy.polyfit
    Least squares polynomial fit.
numpy.reshape
    Gives a new shape to an array without changing its data.
numpy.bartlett
    Return the Bartlett window.
numpy.blackman
    Return the Blackman window.
numpy.can_cast
    Returns True if cast between data types can occur according to the
numpy.digitize
    Return the indices of the bins to which each value in input array belongs.
numpy.fromfile
    Construct an array from data in a text or binary file.
numpy.fromiter
    Create a new 1-dimensional array from an iterable object.
numpy.isfinite
    Test element-wise for finiteness (not infinity or not Not a Number).
numpy.full_like
    Return a full array with the same shape and type as a given array.
numpy.histogram
    Compute the histogram of a set of data.
numpy.nanmedian
    Compute the median along the specified axis, while ignoring NaNs.
numpy.ones_like
    Return an array of ones with the same shape and type as a given array.
numpy.empty_like
    Return a new array with the same shape and type as a given array.
numpy.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.nan_to_num
    Replace nan with zero and inf with finite numbers.
numpy.percentile
    Compute the qth percentile of the data along the specified axis.
numpy.recfromcsv
    Load ASCII data stored in a comma-separated file.
numpy.recfromtxt
    Load ASCII data from a file and return it in a record array.
numpy.trim_zeros
    Trim the leading and/or trailing zeros from a 1-D array or sequence.
numpy.zeros_like
    Return an array of zeros with the same shape and type as a given array.
numpy.ma.dot
    Return the dot product of two arrays.
numpy.ma.exp
    Calculate the exponential of all elements in the input array.
numpy.ma.var
    Compute the variance along the specified axis.
numpy.ma.copy
    a.copy(order='C')
numpy.fft.fft2
    Compute the 2-dimensional discrete Fourier Transform
numpy.fft.fftn
    Compute the N-dimensional discrete Fourier Transform.
numpy.fft.rfft
    Compute the one-dimensional discrete Fourier Transform for real input.
numpy.busday_offset
    First adjusts the date to fall on a valid day according to
numpy.ma.ravel
    Returns a 1D version of self, as a view.
numpy.fft.ifft2
    Compute the 2-dimensional inverse discrete Fourier Transform.
numpy.fft.ifftn
    Compute the N-dimensional inverse discrete Fourier Transform.
numpy.fft.rfftn
    Compute the N-dimensional discrete Fourier Transform for real input.
numpy.nanpercentile
    Compute the qth percentile of the data along the specified axis,
numpy.ma.median
    Compute the median along the specified axis.
numpy.fft.irfftn
    Compute the inverse of the N-dimensional FFT of real input.
numpy.linalg.svd
    Singular Value Decomposition.
numpy.ma.polyfit
    Least squares polynomial fit.
numpy.bytes0.split
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.linalg.cond
    Compute the condition number of a matrix.
numpy.linalg.norm
    Matrix or vector norm.
numpy.may_share_memory
    Determine if two arrays might share memory
numpy.bytes0.decode
    Decode the bytes using the codec registered for encoding.
numpy.bytes0.rsplit
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.str0.encode
    Encode S using the codec registered for encoding. Default encoding
numpy.matlib.randn
    Return a random matrix with data from the "standard normal" distribution.
numpy.matrix.ravel
    Return a flattened matrix.
numpy.bytes0.replace
    Return a copy with all occurrences of substring old replaced by new.
numpy.chararray.copy
    Return a copy of the array.
numpy.chararray.sort
    Sort an array, in-place.
numpy.chararray.view
    New view of array with the same data.
numpy.ma.empty_like
    Return a new array with the same shape and type as a given array.
numpy.ma.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.matrix.flatten
    Return a flattened copy of the matrix.
numpy.chararray.astype
    Copy of the array, cast to a specified type.
numpy.testing.Tester
    Nose test runner.
numpy.chararray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.chararray.tobytes
    Construct Python bytes containing the raw data bytes in the array.
numpy.matrix.partition
    Rearranges the elements in the array in such a way that value of the
numpy.chararray.tostring
    Construct Python bytes containing the raw data bytes in the array.
numpy.chararray.transpose
    a.transpose(*axes)
numpy.ma.MaskedArray.mean
    Returns the average of the array elements along given axis.
numpy.testing.noseclasses.NumpyDocTestCase._formatMessage
    Honour the longMessage attribute when generating failure messages.
numpy.ma.MaskedArray.dot
    Masked dot product of two arrays. Note that `out` and `strict` are
numpy.ma.MaskedArray.var
    Compute the variance along the specified axis.
numpy.ma.MaskedArray.copy
    Return a copy of the array.
numpy.ma.MaskedArray.view
    New view of array with the same data.
numpy.core.multiarray.scalar
    Return a new scalar array of the given type initialized with obj.
numpy.ma.MaskedArray.ravel
    Returns a 1D version of self, as a view.
numpy.ma.MaskedArray.filled
    Return a copy of self, with masked values filled with a given value.
numpy.lib.format.open_memmap
    Open a .npy file as a memory-mapped array.
numpy.lib.format.write_array
    Write an array to an NPY file, including a header.
numpy.ma.MaskedArray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.polynomial.Hermite._fit
    Least squares fit of Hermite series to data.
numpy.ma.MaskedArray.transpose
    a.transpose(*axes)
numpy.polynomial.HermiteE._fit
    Least squares fit of Hermite series to data.
numpy.polynomial.Laguerre._fit
    Least squares fit of Laguerre series to data.
numpy.polynomial.Legendre._fit
    Least squares fit of Legendre series to data.
numpy.polynomial.Chebyshev._fit
    Least squares fit of Chebyshev series to data.
numpy.polynomial.Polynomial._fit
    Least-squares fit of a polynomial to data.
numpy.distutils.command.bdist_rpm.bdist_rpm.set_undefined_options
    Set the values of any "undefined" options from corresponding
numpy.lib.format._write_array_header
    Write the header for an array and returns the version used
numpy.distutils.numpy_distribution.NumpyDistribution._set_command_options
    Set the options for 'command_obj' from 'option_dict'.  Basically
numpy.testing.Tester._get_custom_doctester
    Return instantiated plugin for doctests
numpy.distutils.fcompiler.FCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.msvccompiler.MSVCCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.intelccompiler.IntelCCompiler.link
    Link a bunch of stuff together to create an executable orNone

# Get info on data types with `np.info()`
np.info(np.ndarray.dtype)

(Tao tried print(np.info(np.ndarray.dtype)), but it did not print anything)

Note that you indeed need to know that dtype is an attribute of ndarray. Also, make sure that you don’t forget to put np in front of the modules, classes or terms you’re asking information about, otherwise you will get an error message like this:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'ndarray' is not defined

You now know how to ask for help, and that’s a good thing. The next topic that this NumPy tutorial covers is array manipulation.

Not that you can not overcome this topic on your own, quite the contrary!

But some of the functions might raise questions, because, what is the difference between resizing and reshaping?

And what is the difference between stacking your arrays horizontally and vertically?

The next section is all about answering these questions, but if you ever feel in doubt, feel free to use the help functions that you have just seen to quickly get up to speed.

How To Manipulate Arrays

Performing mathematical operations on your arrays is one of the things that you’ll be doing, but probably most importantly to make this and the broadcasting work is to know how to manipulate your arrays.

Below are some of the most common manipulations that you’ll be doing.

How To Transpose Your Arrays

What transposing your arrays actually does is permuting the dimensions of it. Or, in other words, you switch around the shape of the array. Let’s take a small example to show you the effect of transposition:

# Print `my_2d_array`
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Transpose `my_2d_array`
print(np.transpose(my_2d_array))

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

# Or use `T` to transpose `my_2d_array`
print(my_2d_array.T)

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

Tip: if the visual comparison between the array and its transposed version is not entirely clear, inspect the shape of the two arrays to make sure that you understand why the dimensions are permuted.

Note that there are two transpose functions. Both do the same; There isn’t too much difference. You do have to take into account that T seems more of a convenience function and that you have a lot more flexibility with np.transpose(). That’s why it’s recommended to make use of this function if you want to more arguments.

All is well when you transpose arrays that are bigger than one dimension, but what happens when you just have a 1-D array? Will there be any effect, you think?

Try it out for yourself in the code chunk below. Your 1-D array has already been loaded in:

# Print `my_2d_array`
print(my_array)

[1 2 3 4]

# Transpose `my_2d_array`
print(np.transpose(my_array))

[1 2 3 4]

# Or use `T` to transpose `my_2d_array`
print(my_array.T)

[1 2 3 4]

You’re absolutely right! There is no effect when you transpose a 1-D array!

Reshaping Versus Resizing Your Arrays

You might have read in the broadcasting section that the dimensions of your arrays need to be compatible if you want them to be good candidates for arithmetic operations. But the question of what you should do when that is not the case, was not answered yet.

Well, this is where you get the answer!

What you can do if the arrays don’t have the same dimensions, is resize your array. You will then return a new array that has the shape that you passed to the np.resize() function. If you pass your original array together with the new dimensions, and if that new array is larger than the one that you originally had, the new array will be filled with copies of the original array that are repeated as many times as is needed.

However, if you just apply np.resize() to the array and you pass the new shape to it, the new array will be filled with zeros.

Let’s try this out with an example:

# Print the shape of `x`
print(x.shape)

(3, 4)

# Resize `x` to ((6,4))
np.resize(x, (6,4))

# Try out this as well
x.resize((6,4))

# Print out `x`
print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

Besides resizing, you can also reshape your array. This means that you give a new shape to an array without changing its data. The key to reshaping is to make sure that the total size of the new array is unchanged. If you take the example of array x that was used above, which has a size of 3 X 4 or 12, you have to make sure that the new array also has a size of 12.

Psst… If you want to calculate the size of an array with code, make sure to use the size attribute: x.size or x.reshape((2,6)).size:

# Print the size of `x` to see what's possible
print(x.size)

12

# Reshape `x` to (2,6)
print(x.reshape((2,6)))

[[ 1.  1.  1.  1.  1.  1.]
 [ 1.  1.  1.  1.  1.  1.]]

# Flatten `x`
z = x.ravel()

# Print `z`
print(z)

[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]

If all else fails, you can also append an array to your original one or insert or delete array elements to make sure that your dimensions fit with the other array that you want to use for your computations.

Another operation that you might keep handy when you’re changing the shape of arrays is ravel(). This function allows you to flatten your arrays. This means that if you ever have 2D, 3D or n-D arrays, you can just use this function to flatten it all out to a 1-D array.

Pretty handy, isn’t it?

How To Append Arrays

When you append arrays to your original array, they are “glued” to the end of that original array. If you want to make sure that what you append does not come at the end of the array, you might consider inserting it. Go to the next section if you want to know more.

Appending is a pretty easy thing to do thanks to the NumPy library; You can just make use of the np.append().

Check how it’s done in the code chunk below. Don’t forget that you can always check which arrays are loaded in by typing, for example, my_array in the IPython shell and pressing ENTER.

# Append a 1D array to your `my_array`
new_array = np.append(my_array, [7, 8, 9, 10])

# Print `new_array`
print(new_array)

# Append an extra column to your `my_2d_array`
new_2d_array = np.append(my_2d_array, [[7], [8]], axis=1)

# Print `new_2d_array`
print(new_2d_array)

Note how, when you append an extra column to my_2d_array, the axis is specified. Remember that axis 1 indicates the columns, while axis 0 indicates the rows in 2-D arrays.

How To Insert And Delete Array Elements

Next to appending, you can also insert and delete array elements. As you might have guessed by now, the functions that will allow you to do these operations are np.insert() and np.delete():

# Insert `5` at index 1
np.insert(my_array, 1, 5)

# Delete the value at index 1
np.delete(my_array,[1])

How To Join And Split Arrays

You can also ‘merge’ or join your arrays. There are a bunch of functions that you can use for that purpose and most of them are listed below.

Try them out, but also make sure to test out what the shape of the arrays is in the IPython shell. The arrays that have been loaded are x, my_array, my_resized_array and my_2d_array.

# Concatentate `my_array` and `x`
print(np.concatenate((my_array,x)))

[ 1.  2.  3.  4.  1.  1.  1.  1.]

# Stack arrays row-wise
print(np.vstack((my_array, my_2d_array)))

[[1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

# Stack arrays row-wise
print(np.r_[my_resized_array, my_2d_array])

[[1 2 3 4]
 [1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

# Stack arrays horizontally
print(np.hstack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.column_stack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.c_[my_resized_array, my_2d_array])

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

You’ll note a few things as you go through the functions:

- The number of dimensions needs to be the same if you want to concatenate two arrays with np.concatenate(). As such, if you want to concatenate an array with my_array, which is 1-D, you’ll need to make sure that the second array that you have, is also 1-D.

- With np.vstack(), you effortlessly combine my_array with my_2d_array. You just have to make sure that, as you’re stacking the arrays row-wise, that the number of columns in both arrays is the same. As such, you could also add an array with shape (2,4) or (3,4) to my_2d_array, as long as the number of columns matches. Stated differently, the arrays must have the same shape along all but the first axis. The same holds also for when you want to use np.r[].

- For np.hstack(), you have to make sure that the number of dimensions is the same and that the number of rows in both arrays is the same. That means that you could stack arrays such as (2,3) or (2,4) to my_2d_array, which itself as a shape of (2,4). Anything is possible as long as you make sure that the number of rows matches. This function is still supported by NumPy, but you should prefer np.concatenate() or np.stack().

- With np.column_stack(), you have to make sure that the arrays that you input have the same first dimension. In this case, both shapes are the same, but if my_resized_array were to be (2,1) or (2,), the arrays still would have been stacked.

- np.c_[] is another way to concatenate. Here also, the first dimension of both arrays needs to match.

When you have joined arrays, you might also want to split them at some point. Just like you can stack them horizontally, you can also do the same but then vertically. You use np.hsplit() and np.vsplit(), respectively:

# Split `my_stacked_array` horizontally at the 2nd index
print(np.hsplit(my_stacked_array, 2))

# Split `my_stacked_array` vertically at the 2nd index
print(np.vsplit(my_stacked_array, 2))

What you need to keep in mind when you’re using both of these split functions is probably the shape of your array. Let’s take the above case as an example: my_stacked_array has a shape of (2,8). If you want to select the index at which you want the split to occur, you have to keep the shape in mind.

How To Visualize NumPy Arrays

Lastly, something that will definitely come in handy is to know how you can plot your arrays. This can especially be handy in data exploration, but also in later stages of the data science workflow, when you want to visualize your arrays.

With np.histogram()

Contrary to what the function might suggest, the np.histogram() function doesn’t draw the histogram but it does compute the occurrences of the array that fall within each bin; This will determine the area that each bar of your histogram takes up.

What you pass to the np.histogram() function then is first the input data or the array that you’re working with. The array will be flattened when the histogram is computed.

# Import `numpy` as `np`
import numpy as np

# Initialize your array
my_3d_array = np.array([[[1,2,3,4], [5,6,7,8]], [[1,2,3,4], [9,10,11,12]]], dtype=np.int64)

# Pass the array to `np.histogram()`
print(np.histogram(my_3d_array))

(array([4, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([  1. ,   2.1,   3.2,   4.3,   5.4,   6.5,   7.6,   8.7,   9.8,
        10.9,  12. ]))

# Specify the number of bins
print(np.histogram(my_3d_array, bins=range(0,13)))

(array([0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]))

You’ll see that as a result, the histogram will be computed: the first array lists the frequencies for all the elements of your array, while the second array lists the bins that would be used if you don’t specify any bins.

If you do specify a number of bins, the result of the computation will be different: the floats will be gone and you’ll see all integers for the bins.

There are still some other arguments that you can specify that can influence the histogram that is computed. You can find all of them here.

But what is the point of computing such a histogram if you can’t visualize it?

Visualization is a piece of cake with the help of Matplotlib, but you don’t need np.histogram() to compute the histogram. plt.hist() does this for itself when you pass it the (flattened) data and the bins:

# Import numpy and matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Construct the histogram with a flattened 3d array and a range of bins
plt.hist(my_3d_array.ravel(), bins=range(0,13))

# Add a title to the plot
plt.title('Frequency of My 3D Array Elements')

# Show the plot
plt.show()

The above code will then give you the following (basic) histogram:

(A picture)

Using np.meshgrid()

Another way to (indirectly) visualize your array is by using np.meshgrid(). The problem that you face with arrays is that you need 2-D arrays of x and y coordinate values. With the above function, you can create a rectangular grid out of an array of x values and an array of y values: the np.meshgrid() function takes two 1D arrays and produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays. Then, you can use these matrices to make all sorts of plots.

np.meshgrid() is particularly useful if you want to evaluate functions on a grid, as the code below demonstrates:

# Import NumPy and Matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Create an array
points = np.arange(-5, 5, 0.01)

# Make a meshgrid
xs, ys = np.meshgrid(points, points)
z = np.sqrt(xs ** 2 + ys ** 2)

# Display the image on the axes
plt.imshow(z, cmap=plt.cm.gray)

# Draw a color bar
plt.colorbar()

# Show the plot
plt.show()

The code above gives the following result:

(A picture)

Beyond Data Analysis with NumPy

Congratulations, you have reached the end of the NumPy tutorial!

You have covered a lot of ground, so now you have to make sure to retain the knowledge that you have gained. Don’t forget to get your copy of DataCamp’s NumPy cheat sheet to support you in doing this!

After all this theory, it’s also time to get some more practice with the concepts and techniques that you have learned in this tutorial. One way to do this is to go back to the scikit-learn tutorial and start experimenting with further with the data arrays that are used to build machine learning models.

If this is not your cup of tea, check again whether you have downloaded Anaconda. Then, get started with NumPy arrays in Jupyter with this Definitive Guide to Jupyter Notebook. Also make sure to check out this Jupyter Notebook, which also guides you through data analysis in Python with NumPy and some other libraries in the interactive data science environment of the Jupyter Notebook.

Lastly, consider checking out DataCamp’s courses on data manipulation and visualization. Especially our latest courses in collaboration with Continuum Analytics will definitely interest you! Take a look at the Manipulating DataFrames with Pandas or the Pandas Foundations courses.

==
From 
https://www.hackerearth.com/practice/machine-learning/data-manipulation-visualisation-r-python/tutorial-data-manipulation-numpy-pandas-python/tutorial/

Let's move on to pandas now. Make sure you following each line below because it'll help you in doing data manipulation using pandas.

Let's start with Pandas

#load library - pd is just an alias. I used pd because it's short and literally abbreviates pandas.
#You can use any name as an alias. 
import pandas as pd
#create a data frame - dictionary is used here where keys get converted to column names and values to row values.
data = pd.DataFrame({'Country': ['Russia','Colombia','Chile','Equador','Nigeria'],
                    'Rank':[121,40,100,130,11]})
data
Country	Rank
0	Russia	121
1	Colombia	40
2	Chile	100
3	Equador	130
4	Nigeria	11
#We can do a quick analysis of any data set using:
data.describe()
Rank
count	5.000000
mean	80.400000
std	52.300096
min	11.000000
25%	40.000000
50%	100.000000
75%	121.000000
max	130.000000
Remember, describe() method computes summary statistics of integer / double variables. To get the complete information about the data set, we can use info() function.

#Among other things, it shows the data set has 5 rows and 2 columns with their respective names.
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5 entries, 0 to 4
Data columns (total 2 columns):
Country    5 non-null object
Rank       5 non-null int64
dtypes: int64(1), object(1)
memory usage: 152.0+ bytes


#Let's create another data frame.
data = pd.DataFrame({'group':['a', 'a', 'a', 'b','b', 'b', 'c', 'c','c'],'ounces':[4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
group	ounces
0	a	4.0
1	a	3.0
2	a	12.0
3	b	6.0
4	b	7.5
5	b	8.0
6	c	3.0
7	c	5.0
8	c	6.0
#Let's sort the data frame by ounces - inplace = True will make changes to the data
data.sort_values(by=['ounces'],ascending=True,inplace=False)
group	ounces
1	a	3.0
6	c	3.0
0	a	4.0
7	c	5.0
3	b	6.0
8	c	6.0
4	b	7.5
5	b	8.0
2	a	12.0
We can sort the data by not just one column but multiple columns as well.

data.sort_values(by=['group','ounces'],ascending=[True,False],inplace=False)
group	ounces
2	a	12.0
0	a	4.0
1	a	3.0
5	b	8.0
4	b	7.5
3	b	6.0
8	c	6.0
7	c	5.0
6	c	3.0
Often, we get data sets with duplicate rows, which is nothing but noise. Therefore, before training the model, we need to make sure we get rid of such inconsistencies in the data set. Let's see how we can remove duplicate rows.

#create another data with duplicated rows
data = pd.DataFrame({'k1':['one']*3 + ['two']*4, 'k2':[3,2,1,3,3,4,4]})
data
k1	k2
0	one	3
1	one	2
2	one	1
3	two	3
4	two	3
5	two	4
6	two	4
#sort values 
data.sort_values(by='k2')
k1	k2
2	one	1
1	one	2
0	one	3
3	two	3
4	two	3
5	two	4
6	two	4
#remove duplicates - ta da! 
data.drop_duplicates()
k1	k2
0	one	3
1	one	2
2	one	1
3	two	3
5	two	4
Here, we removed duplicates based on matching row values across all columns. Alternatively, we can also remove duplicates based on a particular column. Let's remove duplicate values from the k1 column.

data.drop_duplicates(subset='k1')
k1	k2
0	one	3
3	two	3
Now, we will learn to categorize rows based on a predefined criteria. It happens a lot while data processing where you need to categorize a variable. For example, say we have got a column with country names and we want to create a new variable 'continent' based on these country names. In such situations, we will require the steps below:

data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami','corned beef', 'Bacon', 'pastrami', 'honey ham','nova lox'],
                 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
food	ounces
0	bacon	4.0
1	pulled pork	3.0
2	bacon	12.0
3	Pastrami	6.0
4	corned beef	7.5
5	Bacon	8.0
6	pastrami	3.0
7	honey ham	5.0
8	nova lox	6.0
Now, we want to create a new variable which indicates the type of animal which acts as the source of the food. To do that, first we'll create a dictionary to map the food to the animals. Then, we'll use map function to map the dictionary's values to the keys. Let's see how is it done.

meat_to_animal = {
'bacon': 'pig',
'pulled pork': 'pig',
'pastrami': 'cow',
'corned beef': 'cow',
'honey ham': 'pig',
'nova lox': 'salmon'
}

def meat_2_animal(series):
    if series['food'] == 'bacon':
        return 'pig'
    elif series['food'] == 'pulled pork':
        return 'pig'
    elif series['food'] == 'pastrami':
        return 'cow'
    elif series['food'] == 'corned beef':
        return 'cow'
    elif series['food'] == 'honey ham':
        return 'pig'
    else:
        return 'salmon'


#create a new variable
data['animal'] = data['food'].map(str.lower).map(meat_to_animal)
data
food	ounces	animal
0	bacon	4.0	pig
1	pulled pork	3.0	pig
2	bacon	12.0	pig
3	Pastrami	6.0	cow
4	corned beef	7.5	cow
5	Bacon	8.0	pig
6	pastrami	3.0	cow
7	honey ham	5.0	pig
8	nova lox	6.0	salmon
#another way of doing it is: convert the food values to the lower case and apply the function
lower = lambda x: x.lower()
data['food'] = data['food'].apply(lower)
data['animal2'] = data.apply(meat_2_animal, axis='columns')
data
food	ounces	animal	animal2
0	bacon	4.0	pig	pig
1	pulled pork	3.0	pig	pig
2	bacon	12.0	pig	pig
3	pastrami	6.0	cow	cow
4	corned beef	7.5	cow	cow
5	bacon	8.0	pig	pig
6	pastrami	3.0	cow	cow
7	honey ham	5.0	pig	pig
8	nova lox	6.0	salmon	salmon
Another way to create a new variable is by using the assign function. With this tutorial, as you keep discovering the new functions, you'll realize how powerful pandas is.

data.assign(new_variable = data['ounces']*10)
food	ounces	animal	animal2	new_variable
0	bacon	4.0	pig	pig	40.0
1	pulled pork	3.0	pig	pig	30.0
2	bacon	12.0	pig	pig	120.0
3	pastrami	6.0	cow	cow	60.0
4	corned beef	7.5	cow	cow	75.0
5	bacon	8.0	pig	pig	80.0
6	pastrami	3.0	cow	cow	30.0
7	honey ham	5.0	pig	pig	50.0
8	nova lox	6.0	salmon	salmon	60.0
Let's remove the column animal2 from our data frame.

data.drop('animal2',axis='columns',inplace=True)
data
food	ounces	animal
0	bacon	4.0	pig
1	pulled pork	3.0	pig
2	bacon	12.0	pig
3	Pastrami	6.0	cow
4	corned beef	7.5	cow
5	Bacon	8.0	pig
6	pastrami	3.0	cow
7	honey ham	5.0	pig
8	nova lox	6.0	salmon
We frequently find missing values in our data set. A quick method for imputing missing values is by filling the missing value with any random number. Not just missing values, you may find lots of outliers in your data set, which might require replacing. Let's see how can we replace values.

#Series function from pandas are used to create arrays
data = pd.Series([1., -999., 2., -999., -1000., 3.])
data
0       1.0
1    -999.0
2       2.0
3    -999.0
4   -1000.0
5       3.0
dtype: float64


#replace -999 with NaN values
data.replace(-999, np.nan,inplace=True)
data
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64


#We can also replace multiple values at once.
data = pd.Series([1., -999., 2., -999., -1000., 3.])
data.replace([-999,-1000],np.nan,inplace=True)
data
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64
Now, let's learn how to rename column names and axis (row names).

data = pd.DataFrame(np.arange(12).reshape((3, 4)),index=['Ohio', 'Colorado', 'New York'],columns=['one', 'two', 'three', 'four'])
data
one	two	three	four
Ohio	0	1	2	3
Colorado	4	5	6	7
New York	8	9	10	11
#Using rename function
data.rename(index = {'Ohio':'SanF'}, columns={'one':'one_p','two':'two_p'},inplace=True)
data
one_p	two_p	three	four
SanF	0	1	2	3
Colorado	4	5	6	7
New York	8	9	10	11
#You can also use string functions
data.rename(index = str.upper, columns=str.title,inplace=True)
data
One_p	Two_p	Three	Four
SANF	0	1	2	3
COLORADO	4	5	6	7
NEW YORK	8	9	10	11
Next, we'll learn to categorize (bin) continuous variables.

ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
We'll divide the ages into bins such as 18-25, 26-35,36-60 and 60 and above.

#Understand the output - '(' means the value is included in the bin, '[' means the value is excluded
bins = [18, 25, 35, 60, 100]
cats = pd.cut(ages, bins)
cats
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 12
Categories (4, object): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]


#To include the right bin value, we can do:
pd.cut(ages,bins,right=False)
[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]
Length: 12
Categories (4, object): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]


#pandas library intrinsically assigns an encoding to categorical variables.
cats.labels
array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)


#Let's check how many observations fall under each bin
pd.value_counts(cats)
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64
Also, we can pass a unique name to each label.

bin_names = ['Youth', 'YoungAdult', 'MiddleAge', 'Senior']
new_cats = pd.cut(ages, bins,labels=bin_names)

pd.value_counts(new_cats)
Youth	5
MiddleAge	3
YoungAdult	3
Senior	1
dtype: int64	
#we can also calculate their cumulative sum
pd.value_counts(new_cats).cumsum()
Youth	5
MiddleAge	3
YoungAdult	3
Senior	1
dtype: int64	
Let's proceed and learn about grouping data and creating pivots in pandas. It's an immensely important data analysis method which you'd probably have to use on every data set you work with.

df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
df
data1	data2	key1	key2
0	0.973599	0.001761	a
1	0.207283	-0.990160	a
2	1.099642	1.872394	b
3	0.939897	-0.241074	b
4	0.606389	0.053345	a
#calculate the mean of data1 column by key1
grouped = df['data1'].groupby(df['key1'])
grouped.mean()
key1
a    0.595757
b    1.019769
Name: data1, dtype: float64
Now, let's see how to slice the data frame.
dates = pd.date_range('20130101',periods=6)
df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
df
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
#get first n rows from the data frame
df[:3]
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
#slice based on date range
df['20130101':'20130104']
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
#slicing based on column names
df.loc[:,['A','B']]
A	B
2013-01-01	1.030816	-1.276989
2013-01-02	-1.070215	-0.209129
2013-01-03	1.524227	1.863575
2013-01-04	0.918203	-0.158800
2013-01-05	0.089731	0.114854
2013-01-06	0.222260	0.435183
#slicing based on both row index labels and column names
df.loc['20130102':'20130103',['A','B']]
A	B
2013-01-02	-1.070215	-0.209129
2013-01-03	1.524227	1.863575
#slicing based on index of columns
df.iloc[3] #returns 4th row (index is 3rd)
A    0.918203
B   -0.158800
C   -0.964063
D   -1.990779
Name: 2013-01-04 00:00:00, dtype: float64


#returns a specific range of rows
df.iloc[2:4, 0:2]
A	B
2013-01-03	1.524227	1.863575
2013-01-04	0.918203	-0.158800
#returns specific rows and columns using lists containing columns or row indexes
df.iloc[[1,5],[0,2]] 
A	C
2013-01-02	-1.070215	0.604572
2013-01-06	0.222260	-0.045748
Similarly, we can do Boolean indexing based on column values as well. This helps in filtering a data set based on a pre-defined condition.

df[df.A > 1]
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-03	1.524227	1.863575	1.291378	1.300696
#we can copy the data set
df2 = df.copy()
df2['E']=['one', 'one','two','three','four','three']
df2
A	B	C	D	E
2013-01-01	1.030816	-1.276989	0.837720	-1.490111	one
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058	one
2013-01-03	1.524227	1.863575	1.291378	1.300696	two
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779	three
2013-01-05	0.089731	0.114854	-0.585815	0.298772	four
2013-01-06	0.222260	0.435183	-0.045748	0.049898	three
#select rows based on column values
df2[df2['E'].isin(['two','four'])]
A	B	C	D	E
2013-01-03	1.524227	1.863575	1.291378	1.300696	two
2013-01-05	0.089731	0.114854	-0.585815	0.298772	four
#select all rows except those with two and four
df2[~df2['E'].isin(['two','four'])]
A	B	C	D	E
2013-01-01	1.030816	-1.276989	0.837720	-1.490111	one
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058	one
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779	three
2013-01-06	0.222260	0.435183	-0.045748	0.049898	three
We can also use a query method to select columns based on a criterion. Let's see how!

#list all columns where A is greater than C
df.query('A > C')
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
#using OR condition
df.query('A < B | C > A')
A	B	C	D
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
Pivot tables are extremely useful in analyzing data using a customized tabular format. I think, among other things, Excel is popular because of the pivot table option. It offers a super-quick way to analyze data.

#create a data frame
data = pd.DataFrame({'group': ['a', 'a', 'a', 'b','b', 'b', 'c', 'c','c'],
                 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
group	ounces
0	a	4.0
1	a	3.0
2	a	12.0
3	b	6.0
4	b	7.5
5	b	8.0
6	c	3.0
7	c	5.0
8	c	6.0
#calculate means of each group
data.pivot_table(values='ounces',index='group',aggfunc=np.mean)
group
a    6.333333
b    7.166667
c    4.666667
Name: ounces, dtype: float64


#calculate count by each group
data.pivot_table(values='ounces',index='group',aggfunc='count')
group
a    3
b    3
c    3
Name: ounces, dtype: int64
Up till now, we've become familiar with the basics of pandas library using toy examples. Now, we'll take up a real-life data set and use our newly gained knowledge to explore it.

Exploring ML Data Set

We'll work with the popular adult data set.The data set has been taken from UCI Machine Learning Repository. You can download the data from here. In this data set, the dependent variable is "target." It is a binary classification problem. We need to predict if the salary of a given person is less than or more than 50K.

#load the data
train  = pd.read_csv("~/Adult/train.csv")
test = pd.read_csv("~/Adult/test.csv")
#check data set
train.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 32561 entries, 0 to 32560
Data columns (total 15 columns):
age               32561 non-null int64
workclass         30725 non-null object
fnlwgt            32561 non-null int64
education         32561 non-null object
education.num     32561 non-null int64
marital.status    32561 non-null object
occupation        30718 non-null object
relationship      32561 non-null object
race              32561 non-null object
sex               32561 non-null object
capital.gain      32561 non-null int64
capital.loss      32561 non-null int64
hours.per.week    32561 non-null int64
native.country    31978 non-null object
target            32561 non-null object
dtypes: int64(6), object(9)
memory usage: 3.7+ MB
We see that, the train data has 32561 rows and 15 columns. Out of these 15 columns, 6 have integers classes and the rest have object (or character) classes. Similarly, we can check for test data. An alternative way of quickly checking rows and columns is

print ("The train data has",train.shape)
print ("The test data has",test.shape)
('The train data has', (32561, 15))
('The test data has', (16281, 15))


#Let have a glimpse of the data set
train.head()
age	workclass	fnlwgt	education	education.num	marital.status	occupation	relationship	race	sex	capital.gain	capital.loss	hours.per.week	native.country	target
0	39	State-gov	77516	Bachelors	13	Never-married	Adm-clerical	Not-in-family	White	Male	2174	0	40	United-States	<=50K
1	50	Self-emp-not-inc	83311	Bachelors	13	Married-civ-spouse	Exec-managerial	Husband	White	Male	0	0	13	United-States	<=50K
2	38	Private	215646	HS-grad	9	Divorced	Handlers-cleaners	Not-in-family	White	Male	0	0	40	United-States	<=50K
3	53	Private	234721	11th	7	Married-civ-spouse	Handlers-cleaners	Husband	Black	Male	0	0	40	United-States	<=50K
4	28	Private	338409	Bachelors	13	Married-civ-spouse	Prof-specialty	Wife	Black	Female	0	0	40	Cuba	<=50K
Now, let's check the missing values (if present) in this data.

nans = train.shape[0] - train.dropna().shape[0]
print ("%d rows have missing values in the train data" %nans)

nand = test.shape[0] - test.dropna().shape[0]
print ("%d rows have missing values in the test data" %nand)

2399 rows have missing values in the train data
1221 rows have missing values in the test data
We should be more curious to know which columns have missing values.

#only 3 columns have missing values
train.isnull().sum()
age	0
workclass	1836
fnlwgt	0
education	0
education.num	0
marital.status	0
occupation	1843
relationship	0
race	0
sex	0
capital.gain	0
capital.loss	0
hours.per.week	0
native.country	583
target	0
dtype: int64	
Let's count the number of unique values from character variables.

cat = train.select_dtypes(include=['O'])
cat.apply(pd.Series.nunique)
workclass	8
education	16
marital.status	7
occupation	14
relationship	6
race	5
sex	2
native.country	41
target	2
dtype: int64	
Since missing values are found in all 3 character variables, let's impute these missing values with their respective modes.

#Education
train.workclass.value_counts(sort=True)
train.workclass.fillna('Private',inplace=True)


#Occupation
train.occupation.value_counts(sort=True)
train.occupation.fillna('Prof-specialty',inplace=True)


#Native Country
train['native.country'].value_counts(sort=True)
train['native.country'].fillna('United-States',inplace=True)
Let's check again if there are any missing values left.

train.isnull().sum()
age	0
workclass	0
fnlwgt	0
education	0
education.num	0
marital.status	0
occupation	0
relationship	0
race	0
sex	0
capital.gain	0
capital.loss	0
hours.per.week	0
native.country	0
target	0
dtype: int64	
Now, we'll check the target variable to investigate if this data is imbalanced or not.

#check proportion of target variable
train.target.value_counts()/train.shape[0]
<=50K    0.75919
>50K     0.24081
Name: target, dtype: float64
We see that 75% of the data set belongs to <=50K class. This means that even if we take a rough guess of target prediction as <=50K, we'll get 75% accuracy. Isn't that amazing? Let's create a cross tab of the target variable with education. With this, we'll try to understand the influence of education on the target variable.

pd.crosstab(train.education, train.target,margins=True)/train.shape[0]
target	<=50K	>50K	All
education			
10th	0.026750	0.001904	0.028654
11th	0.034243	0.001843	0.036086
12th	0.012285	0.001013	0.013298
1st-4th	0.004975	0.000184	0.005160
5th-6th	0.009736	0.000491	0.010227
7th-8th	0.018611	0.001228	0.019840
9th	0.014957	0.000829	0.015786
Assoc-acdm	0.024631	0.008139	0.032769
Assoc-voc	0.031357	0.011087	0.042443
Bachelors	0.096250	0.068210	0.164461
Doctorate	0.003286	0.009398	0.012684
HS-grad	0.271060	0.051442	0.322502
Masters	0.023464	0.029452	0.052916
Preschool	0.001566	0.000000	0.001566
Prof-school	0.004699	0.012991	0.017690
Some-college	0.181321	0.042597	0.223918
All	0.759190	0.240810	1.000000
We see that out of 75% people with <=50K salary, 27% people are high school graduates, which is correct as people with lower levels of education are expected to earn less. On the other hand, out of 25% people with >=50K salary, 6% are bachelors and 5% are high-school grads. Now, this pattern seems to be a matter of concern. That's why we'll have to consider more variables before coming to a conclusion.

If you've come this far, you might be curious to get a taste of building your first machine learning model. In the coming week we'll share an exclusive tutorial on machine learning in python. However, let's get a taste of it here.

We'll use the famous and formidable scikit learn library. Scikit learn accepts data in numeric format. Now, we'll have to convert the character variable into numeric. We'll use the labelencoder function.

In label encoding, each unique value of a variable gets assigned a number, i.e., let's say a variable color has four values ['red','green','blue','pink'].

Label encoding this variable will return output as: red = 2 green = 0 blue = 1 pink = 3

#load sklearn and encode all object type variables
from sklearn import preprocessing

for x in train.columns:
    if train[x].dtype == 'object':
        lbl = preprocessing.LabelEncoder()
        lbl.fit(list(train[x].values))
        train[x] = lbl.transform(list(train[x].values))
Let's check the changes applied to the data set.

train.head()
age	workclass	fnlwgt	education	education.num	marital.status	occupation	relationship	race	sex	capital.gain	capital.loss	hours.per.week	native.country	target
0	39	6	77516	9	13	4	0	1	4	1	2174	0	40	38	0
1	50	5	83311	9	13	2	3	0	4	1	0	0	13	38	0
2	38	3	215646	11	9	0	5	1	4	1	0	0	40	38	0
3	53	3	234721	1	7	2	5	0	2	1	0	0	40	38	0
4	28	3	338409	9	13	2	9	5	2	0	0	0	40	4	0
As we can see, all the variables have been converted to numeric, including the target variable.

#<50K = 0 and >50K = 1
train.target.value_counts()
0    24720
1     7841
Name: target, dtype: int64
Building a Random Forest Model

Let's create a random forest model and check the model's accuracy.

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import accuracy_score

y = train['target']
del train['target']

X = train
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)

#train the RF classifier
clf = RandomForestClassifier(n_estimators = 500, max_depth = 6)
clf.fit(X_train,y_train)

    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=6, max_features='auto', max_leaf_nodes=None,
                min_impurity_split=1e-07, min_samples_leaf=1,
                min_samples_split=2, min_weight_fraction_leaf=0.0,
                n_estimators=500, n_jobs=1, oob_score=False, random_state=None,
                verbose=0, warm_start=False)

clf.predict(X_test)
Now, let's make prediction on the test set and check the model's accuracy.

#make prediction and check model's accuracy
prediction = clf.predict(X_test)
acc =  accuracy_score(np.array(y_test),prediction)
print ('The accuracy of Random Forest is {}'.format(acc))
The accuracy of Random Forest is 0.85198075545.
Hurrah! Our learning algorithm gave 85% accuracy. Well, we can do tons of things on this data and improve the accuracy. We'll learn about it in future articles. What's next?

In this tutorial, we divided the train data into two halves and made prediction on the test data. As your exercise, you should use this model and make prediction on the test data we loaded initially. You can perform same set of steps we did on the train data to complete this exercise. In case you face any difficulty, feel free to share it in Comments below.

Summary

This tutorial is meant to help python developers or anyone who's starting with python to get a taste of data manipulation and a little bit of machine learning using python. I'm sure, by now you would be convinced that python is actually very powerful in handling and processing data sets. But, what we learned here is just the tip of the iceberg. Don't get complacent with this knowledge.

To dive deeper in pandas, check its documentation and start exploring. If you get stuck anywhere, you can drop your questions or suggestions in Comments below. Hope you found this tutorial useful.

--
From ml4t:

Pandas: This library was created by Wes McKinney at a hedge fund call AQR. It's used at many hedge funds and by many people in the finance industry. One of the key components of Pandas is something called the dataframe.

==
import pandas as pd
import numpy as np

==
Use pandas in my Thinkpad computer:

export PATH=~/anaconda2/bin:$PATH
source activate homework1
python file_name.py

==
(findreadcsv)
Read csv file to a dataframe:

df = pd.read_csv("data/AAPL.csv")

Practice shows it will read the data as appropriate data types automatically.

--
Only read in selected columns (col1, col2):

dfSPY = pd.read_csv("data.csv", usecols = ['col1', 'col2'])

--
Read the string 'nan' as not-a-number:

dfSPY = pd.read_csv("data.csv", na_values = ['nan'])

Let's understand that csv 'nan' as string, so we need to tell the read_csv that 'nan' should be interpreted as not a number. Tao: otherwise it will read 'nan' as a string, instead of not-a-number.

--
Make a column to be the index:

dfSPY = pd.read_csv("data.csv", index_col = "date", parse_dates = True)

We make the date column in the csv file as index. We do this by using the index_col parameter. We also want the dates present in the DataFrame to be converted into date time index objects. This can be done by setting the value for the parse_dates parameter to True.

==
(finddropnan)
Drop the rows with NaN:

df2 = df1.dropna()
df2 = df1.dropna(subset = ["col1"]) # Drop only the rows which have col1 equals NaN.

==
(finddataframe)
(findcreatedataframe)
# Create DataFrame:

# Tao: practice shows that d below is just a normal dictionary. The key is string type, and value is a list.

d = {'col1': [1, 2], 'col2': [3, 4]}

df = pd.DataFrame(data=d)

df

   col1  col2
0     1     3
1     2     4

You will also observe there is a column that is not named and has values 0, 1, 2, 3. And this is not from the .csv. These are called index for the data frame, which help you to access rows.

--
Add a new column from computation:

df['col_c'] = 2 * df['col_a'] + df['col_a'] * df['col_b']

==
(findhead)
(findtail)
head & tail

df.head() # Top 5 rows 
df.tail() # Last 5 rows 

df.head(5)
df.tail(3)

--
(findaddcolumn)
Add a column

In the same way as adding to dictionary:

# Tao: the new df contains all the old columns and the new column
df['new_column'] = 2 * df['col_a'] + df['col_b']

--
(findrank)
Rank

df:

           coverage	 name	reports	 year
Cochice	    25	     Jason	4	     2012
Pima	    94	     Molly	24	     2012
Santa Cruz	57	     Tina	31	     2013
Maricopa	62	     Jake	2	     2014
Yuma	    70	     Amy	3	     2014

# Create a new column that is the rank of the value of coverage in ascending order
# Tao: the new df contains all the old columns and the new column

df['coverageRanked'] = df['coverage'].rank(ascending = True)

df:

           coverage	 name	reports	 year  coverageRanked
Cochice	    25	     Jason	4	     2012  1
Pima	    94	     Molly	24	     2012  5
Santa Cruz	57	     Tina	31	     2013  2
Maricopa	62	     Jake	2	     2014  3
Yuma	    70	     Amy	3	     2014  4

--
(findcorrelation)
Correlation

df['A'].corr(df['B'])

--
(findiloc)
(findloc)
(findix)
iloc, loc, ix:

Abstract from https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/

There’s three main options to achieve the selection and indexing activities in Pandas, which can be confusing. The three selection cases and methods covered in this post are:

1. Selecting data by row numbers (.iloc)
2. Selecting data by label or by a conditional statment (.loc)
3. Selecting in a hybrid approach (.ix) (now Deprecated in Pandas 0.20.1). The ix[] indexer is a hybrid of .loc and .iloc.

.iloc selections: position based selection:

data.iloc[<row selection], <column selectoin>]
 
row selectoin and column selection: 
iteger list of rows:[0,1,2]  
integer list of columns: [0,1,2]
slice of rows: [4:7]
slice of columns:[4:7]
single values: 1
single column selections: 1

Examples of iloc:

# Single selections using iloc and DataFrame
# Rows:
data.iloc[0] # first row of data frame. Note a Series data type output.
data.iloc[1] # second row of data frame
data.iloc[-1] # last row of data frame
# Columns:
data.iloc[:,0] # first column of data frame 
data.iloc[:,1] # second column of data frame 
data.iloc[:,-1] # last column of data frame

# Multiple row and column selections using iloc and DataFrame
data.iloc[0:5] # first five rows of dataframe
data.iloc[:, 0:2] # first two columns of data frame with all rows
data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.
data.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1).

Note that .iloc returns a Pandas Series when one row is selected, and a Pandas DataFrame when multiple rows are selected, or if any column in full is selected. To counter this, pass a single-valued list if you require DataFrame output.

# Get value from one row:
# From online: If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:

sub_df

          A         B
2 -0.133653 -0.030854

sub_df.iloc[0]

A   -0.133653
B   -0.030854

sub_df.iloc[0]['A']
-0.13365288513107493

--
loc selectoins: position based selection:

data.loc[<row selection], <column selection>]

row selectoin and column selection: 
index/label value: 'john'
named column: 'first_name'
list of labels: ['john', 'sarah']
list of column names: ['first_name', 'age']
logical/boolean index: data['age'] == 10
slice of columns: 'first_name': 'address'

Examples of loc:

# Select rows with index values 'Andrade' and 'Veness', with all columns between 'city' and 'email'
data.loc[['Andrade', 'Veness'], 'city':'email']

# Select same rows, with just 'first_name', 'address' and 'city' columns
data.loc['Andrade':'Veness', ['first_name', 'address', 'city']]
 
# Change the index to be based on the 'id' column
data.set_index('id', inplace=True)

# select the row with 'id' = 487
data.loc[487]

# Select rows with first name Antonio, # and all columns between 'city' and 'email'
data.loc[data['first_name'] == 'Antonio', 'city':'email']
 
# Select rows where the email column ends with 'hotmail.com', include all columns
data.loc[data['email'].str.endswith("hotmail.com")]   
 
# Select rows with last_name equal to some values, all columns
data.loc[data['first_name'].isin(['France', 'Tyisha', 'Eric'])]   
       
# Select rows with first name Antonio AND hotmail email addresses
data.loc[data['email'].str.endswith("gmail.com") & (data['first_name'] == 'Antonio')] 
 
# select rows with id column between 100 and 200, and just return 'postal' and 'web' columns
data.loc[(data['id'] > 100) & (data['id'] <= 200), ['postal', 'web']] 
 
# A lambda function that yields True/False values can also be used.
# Select rows where the company name has 4 words in it.
data.loc[data['company_name'].apply(lambda x: len(x.split(' ')) == 4)] 
 
# Selections can be achieved outside of the main .loc for clarity:
# Form a separate variable with your selections:
idx = data['company_name'].apply(lambda x: len(x.split(' ')) == 4)
# Select only the True values in 'idx' and only the 3 columns specified:
data.loc[idx, ['email', 'first_name', 'company']]

==
(findslice)
(findselect)
Slice dataframe
Select rows and columns

Select rows:

df[10:21] # Data from index 10 to 20, because 21 is not inclusive in the range. This operation is called slicing and it is a very important operation in Python pandas

df.ix[10:21] # Equivalent as df[10:21], just looks more Pythonic and robust.

Select colums:
df['col1'] <- the returned type is not dataframe!
df[['A', 'B', 'C']]: returns a dataframe with columns A, B, C

print df['col1']

--
Select rows and columns:
df[1:5, ['A', 'B']] # Selects rows 1 to 4, columns A and B.
df.ix[1:5, ['A', 'B']] # Equivalent as above

--
df[df.A > 0]

                   A         B         C         D
2013-01-01  0.469112 -0.282863 -1.509059 -1.135632
2013-01-02  1.212112 -0.173215  0.119209 -1.044236
2013-01-04  0.721555 -0.706771 -1.039575  0.271860

--
# select * from a_df
# where label = 100

a_df[a_df.label == 100]

==
(findmax)
(findstatistics)
Max
Statistics functions

df['col1'].max() 

df.mean() # Return the mean of each column
df.median()
df.std() # Standard deviation

df.kurtosis() 

Kurtosis:

If we’ve got a positive kurtosis, that means we’ve got fat tails, there are more occurrences outside in the tails than would normally happen
with a Gaussian distribution. If negative.... less...

Correlation:
df.corr(method = 'pearson') # df only has two columns, this returns the correlation of the two columns.

--
sqrt, square, power

import numpy as np
df_sqrt = np.sqrt(df['col_a'])
df_square = np.power(df['col_a'], 2)

--
(findlinearregression)
(findpolyfit)
Linear Regression
Polyfit

import pandas as pd
import numpy as np

beta, alpha = np.polyfit(df['col1'], df['col2'], 1) 

1 means polynomial degree is 1 (linear).
So the line is y = beta * x + alpha

--
(findsort)
Sort

df.sort_index(axis=1, ascending=False)

                   D         C         B         A
2013-01-01 -1.135632 -1.509059 -0.282863  0.469112
2013-01-02 -1.044236  0.119209 -0.173215  1.212112
2013-01-03  1.071804 -0.494929 -2.104569 -0.861849
2013-01-04  0.271860 -1.039575 -0.706771  0.721555
2013-01-05 -1.087401  0.276232  0.567020 -0.424972
2013-01-06  0.524988 -1.478427  0.113648 -0.673690

--
df.sort_values(by='B')

                   A         B         C         D
2013-01-03 -0.861849 -2.104569 -0.494929  1.071804
2013-01-04  0.721555 -0.706771 -1.039575  0.271860
2013-01-01  0.469112 -0.282863 -1.509059 -1.135632
2013-01-02  1.212112 -0.173215  0.119209 -1.044236
2013-01-06 -0.673690  0.113648 -1.478427  0.524988
2013-01-05 -0.424972  0.567020  0.276232 -1.087401


--
(findisnull)
(findmissingvalues)
isnull or missing values

df['A'].isnull().sum(): returns number of missing values in column A.

Fill the missing data:

df.fillna()

df.fillna(method = 'ffill') # Forwad fill, ie, fill in the last, previous known value. <- Think of stock curve (price vs time).

df.fillna(method = 'bfill') # Backward fill.

df.fillna(method="ffill", inplace=True)

--
(findlen)
(findsize)
Number of rows in dataframe df:

len(df)

--
(findgroupby)
group by

# Now a new df:

df

     A      B         C         D
0  foo    one -1.202872 -0.055224
1  bar    one -1.814470  2.395985
2  foo    two  1.018601  1.552825
3  bar  three -0.595447  0.166599
4  foo    two  1.395433  0.047609
5  bar    two -0.392670 -0.136473
6  foo    one  0.007207 -0.561757
7  foo  three  1.928123 -1.623033

--
df.groupby('A').sum()

            C        D
A                     
bar -2.802588  2.42611
foo  3.146492 -0.63958

--
df.groupby(['A','B']).sum()

                  C         D
A   B                        
bar one   -1.814470  2.395985
    three -0.595447  0.166599
    two   -0.392670 -0.136473
foo one   -1.195665 -0.616981
    three  1.928123 -1.623033
    two    2.414034  1.600434

--
# group by, count

df

  col1 col2  col3  col4  col5  col6
0    A    B  0.20 -0.61 -0.49  1.49
1    A    B -1.53 -1.01 -0.39  1.82
2    A    B -0.44  0.27  0.72  0.11
3    A    B  0.28 -1.32  0.38  0.18
4    C    D  0.12  0.59  0.81  0.66
5    C    D -0.13 -1.65 -1.64  0.50
6    C    D -1.42 -0.11 -0.18 -0.44
7    E    F -0.00  1.42 -0.26  1.17
8    E    F  0.91 -0.47  1.35 -0.34
9    G    H  1.48 -0.63 -1.14  0.17

df.groupby(['col1', 'col2']).size()

col1  col2
A     B       4
C     D       3
E     F       2
G     H       1

--
# Sort within each group:

Sort the PVE score within each job_id:

score_df['rank'] = score_df.groupby(['job_id'])['score'].rank(ascending = True)

==
(findplot)
Plot in Python, using matplotlib

Tao's standard way:

tao_df is a DataFrame.

tao_df:

col_a  col_b  col_c
 1     0.1    0.3
 2     0.2    0.2
 3     0.3    0.1

The following code will make two curves in the same plot.
The first curve uses the values of col_a as x-axis, the values of col_b as y-axis.
The second curve uses the values of col_a as x-axis, the values of col_c as y-axis.

import pandas as pd
import matplotlib.pyplot as plt

# Plot curves:
plt.plot(tao_df['col_a'], tao_df['col_b'], label = 'Values of b', color = 'blue') # label sets the legend text
plt.plot(tao_df['col_a'], tao_df['col_c'], label = 'Values of c', color = 'red') # label sets the legend text
plt.xlabel('Values of a')
plt.ylabel('Magnitudes')
plt.title('Tao plot')
plt.legend()
plt.show()

# Plot scatters:
dot_size = 3
plt.scatter(tao_df['col_a'], tao_df['col_b'], label = 'Values of b', color = 'blue', s = dot_size) # label sets the legend text
plt.plot(tao_df['col_a'], tao_df['col_c'], label = 'Values of c', color = 'red', s = dot_size) # label sets the legend text
plt.xlabel('Values of a')
plt.ylabel('Magnitudes')
plt.title('Tao plot')
plt.legend()
plt.show()

--
Make multiple plots side-by-side:

Should use Python 3:

# One row, two columns:
from matplotlib import pyplot as plt
fig, axes = plt.subplots(1, 2)

# Sets size of each subplot. If axis text is hidden, the increase the height.
fig.set_figheight(15)
fig.set_figwidth(15)

axes[0].plot(data['row_num'], data['score_pve'], label = 'score_pve', color = 'blue')
axes[1].plot(data['row_num'], data['score_pve'], label = 'score_pve', color = 'blue')

# Two rows, two columns:
fig, axes = plt.subplots(2, 2)
axes[0, 0].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[0, 0].set_xlabel('user_id')
axes[0, 0].set_ylabel('score')
axes[0, 0].set_title('Hello')
axes[0, 0].text(1000, 0.5, "Helo", dict(size=10, color='black')) # Add text in each subplot
axes[0, 1].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[1, 0].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[1, 1].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')

--
Using dataframe.plot():

import pandas as pd
import matplotlib.pyplot as plot

# Make the plot and show it in a window:
df['col1'].plot()
plot.show() # Must be called to show plots

# Make the plot and save it to file:
df['col1'].plot()
plot.savefig('fig_1.png')

# Set the y axis range:
df1 = pd.DataFrame(data = dic1)
df1.plot()
plot.ylim(-256, 100)

# Scatter plot:
df.plot(kind = 'scatter', x = 'compay_1', y = 'compay_2')
plot.show()

# Plot histogram, set number of bins to 20:
df.hist(bins = 20)
plot.show()

# Add verticle line at the position x coordinate = 5, with white color:
plot.axvline(5, color = 'w', linestyle = 'dashed', linewidth = 2)
plot.show()

Add title and labels:

ax = df.plot(title = 'prices', fontsize = 2)
ax.set_xlabel("date")
ax.set_ylabel("price")
plot.show()

==
(finddates)
Dates in pandas

import pandas as pd

def test_run():
	start_date = '2010-01-22'
	end_date = '2010-01-26'
	dates = pd.date_range(start_date, end_date)
	print dates[0] # Output: 2010-01-22 00:00:00
	df1 = pd.DataFrame(index = dates)

We used pandas date range method which takes two parameters, that is start and end date. The output you see is not the list of strings, but the list of date time index objects. 

The output above: 2010-01-22 00:00:00:
This is the first element of the list which a date/time index object. The trailing zero zeros for each object is the default time stamp.

Next we define an empty dataframe df1 with these dates as index. We use the parameter index to supply the dates. Note that without this parameter the dataframe will have an index of integers 0,1,2 as seen before. So here's your DataFrame, DF1. It's an empty DataFrame with no columns. However, as we pass the index parameter, we have an index as dates. And you can see that it's a date time index object.

==
(findjoin)
Join

df_join = df1.join(df2) <- Left join, avadoles!
df_join = df1.join(df2, how = 'inner') <- Inner join

Avadoles! Different from Hive, Spark, SQL:
DataFrame.join does a left join by default. So if we write a.join b, it will read in all the rows from a, but only those rows from b whose index values are present in a's index.

--
(findmerge)
(findconcat)

Join, merge, concat:

From online:

Use merge, which is inner join by default:

pd.merge(df1, df2, left_index=True, right_index=True)
Or join, which is left join by default:

df1.join(df2)
Or concat, which is outer join by default:

pd.concat([df1, df2], axis=1)
Samples:

df1 = pd.DataFrame({'a':range(6),
                    'b':[5,3,6,9,2,4]}, index=list('abcdef'))

print (df1)
   a  b
a  0  5
b  1  3
c  2  6
d  3  9
e  4  2
f  5  4

df2 = pd.DataFrame({'c':range(4),
                    'd':[10,20,30, 40]}, index=list('abhi'))

print (df2)
   c   d
a  0  10
b  1  20
h  2  30
i  3  40

#default inner join
df3 = pd.merge(df1, df2, left_index=True, right_index=True)

print (df3)
   a  b  c   d
a  0  5  0  10
b  1  3  1  20

#default left join
df4 = df1.join(df2)

print (df4)
   a  b    c     d
a  0  5  0.0  10.0
b  1  3  1.0  20.0
c  2  6  NaN   NaN
d  3  9  NaN   NaN
e  4  2  NaN   NaN
f  5  4  NaN   NaN

#default outer join
df5 = pd.concat([df1, df2], axis=1)

print (df5)
     a    b    c     d
a  0.0  5.0  0.0  10.0
b  1.0  3.0  1.0  20.0
c  2.0  6.0  NaN   NaN
d  3.0  9.0  NaN   NaN
e  4.0  2.0  NaN   NaN
f  5.0  4.0  NaN   NaN
h  NaN  NaN  2.0  30.0
i  NaN  NaN  3.0  40.0

==
(findrenamecolumn)
Rename colum

df_new = df.rename(columns = {'old_name' : 'new_name'})

==
(findnormalize)
Normalize

df1 = df1 / df1[0] # Divide the entire dataframe by its first row. It is equivalent as below, but it is more elegant and much FASTER.

df1 = df1 / df1.ix[0,:] # Should be the same as above.

Does the same thing as above:

for date in df1.index:
    for s in symbols:
        df1[date, s] = df1[date, s] / df1[0, s]

==
(findsumcolumns)
(findmeancolumns)
Sum (or mean) of all columns into a new column

df2 = pd.DataFrame(data = dic2)
df2['sum'] = df2.iloc[:,-num_columns:].sum(axis=1) # Sum all columns
df2['mean'] = df2['sum'] / num_columns
df2['std'] = df2.iloc[:,-num_columns:].std(axis=1)

print df2.head(5)

   run_0  run_1  run_2  sum      mean       std
0      0      0      0    0  0.000000  0.000000
1      1      1     -1    1  0.333333  1.018350
2      2      0      1    3  1.000000  1.154701
3      1      2      2    5  1.666667  1.835857
4     -1      3      1    3  1.000000  1.154701



