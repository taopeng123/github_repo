
The book "A Byte of Python" has been all recroded in this note.

==
findgeneral                        Python general
findcomment                        Comment in Python
finddatatypes                      Data types
findescape                         Escape Sequences
findoperators                      Operators
findmath                           Math functions
findand                            and
findor                             or
findnot                            not
findprint                          print
findreadfrominput                  Read from input in terminal
findreadfromfile                   Read from file 
findwritetofile                    Write to file
findreadfromwebpage                Read from webpage
findwhile                          while
findif                             if
findcast                           cast
findround                          Round to nearest integer
findprecision                      Display a float with two decimal places:
findbreak                          break
findcontinue                       continue
findfor                            for
findrange                          range
findfunction                       function
findparameter                      Function parameters
findreturn                         The return Statement
findpass                           Pass statement
finddocstring                      DocStrings
findlocalvariables                 Local variables
findglobal                         Global statement
findnonlocal                       Nonlocal statement
finddefaultargument                Default Argument Values
findkeywordargument                Keyword Arguments
findvararg                         VarArgs parameters
findkeywordonly                    Keyword- only Parameters
findmodule                         Modules
findtuple                          tuple
findlist                           list
finddictionary                     dictionary
findset                            set
findsequence                       sequences 
findnull                           null
findnone                           None
findstring                         string
findsubstring                      Return a substring, Determine whether a string contains a substring
findsplit                          Split a string
findprefix                         Whether a string start with a prefix
findstrip                          Strip a string (remove charaters at beginning and end)
findtrim                           Strip a string (remove charaters at beginning and end)
findjoinpandaslisttostring               Join list to string
findargs                           Parse command line arguments  
findargv                           Parse command line arguments 
findclass                          class
findinheritance                    inheritance
finddestructor                     destructor
findgetter                         Getters and setters
findsetter                         Getters and setters
findexception                      Exceptions
findsleep                          sleep
findtime                           Output current local time

** numpy **

findnumpy                          NumPy
findcreatendarraynumpy             Create numpy array / ndarray
findcreatenumpyarraynumpy          Create numpy array / ndarray
findonesnumpy                      Create numpy array of ones
findzerosnumpy                     Create numpy array of zeros
findrandomnumpy                    Create numpy array of random numbers
findrandnumpy                      Create numpy array of random numbers
findemptynumpy                     Create an empty numpy array
findemptyarraynumpy                Create an empty numpy array
findsamevaluesnumpy                Create numpy array of same values
findrangenumpy                     Create an array of evenly-spaced values (with step)
findarangenumpy                    Create an array of evenly-spaced values (with step)
findlinspacenumpy                  Create an array of evenly-spaced values (with number of values)
findeyenumpy                       np.eye(), ie, identity matrix
findidentitynumpy                  np.identity(), ie, identity matrix
findloaddatanumpy                  Load data from file
findloadnumpy                      Load data from file
findreaddatanumpy                  Load data from file
findimportdatanumpy                Load data from file
findsavenumpy                      Save NumPy Arrays
(indsavedatanumpy                  Save NumPy Arrays
findsavearraynumpy                 Save NumPy Arrays
findsizepandasnumpy                      Size (number of elements) of any array
findmathsnumpy                     Maths in numpy add, substract, etc
findmathnumpy                      Maths in numpy add, substract, etc
findindexingnumpy                  Indexing (select an element) of array
findindexnumpy                     Indexing (select an element) of array
findslicepandasnumpy                     slice
findtransposenumpy                 transpose
findresizenumpy                    resize
findreshapepandasnumpy                   reshape
findflattennumpy                   flatten
findravelnumpy                     flatten
findappendnumpy                    append
findinsertnumpy                    insert
findeletenumpy                     delete
findconcatpandasenatenumpy               concatenate
findconcatpandasnumpy                    concatenate
findvstacknumpy                    vstack
findhstacknumpy                    hstack
findvsplitnumpy                    vsplit
findhsplitnumpy                    hsplit
findhistogramnumpy                 histogram
findplotpandasnumpy                      Plot in numpy
findmatplotlibnumpy                matplotlib
findmeshgridnumpy                  meshgrid


** scipy **

findscipy                          Scipy
findvectorizescipy                 vectorize
findmatrixscipy                    Matrix

** pandas **

findpandas                         Pandas
findcreatedataframefromnumpyarray  Create DataFrame from NumPy array
findcreatedataframefromdictionary  Create DataFrame from dictionary
findshapepandas                    shape
findlenpandas                      Number of rows in dataframe
findcountpandas                    count
findindexpandas                    index
findsetindexpandas                 set_index
findresetindexpandas               reset_index
findaddrowpandas                   Add a row
findaddcolumnpandas                Add a column
finddroppandas                     Drop column or row
finddropcolumnpandas               Drop column
finddroprowpandas                  Drop row
finddropduplicatespandas           Drop duplicate rows
findaxispandas                     axis
findrenamecolumnpandas             Rename column
findreplacepandas                  Replace all occurrences of string 
findremovecolumnpandas             Delete column
finddeletecolumnpandas             Delete column
findlambdapandas                   map each element using lambda function
findudfpandas                      udf
findapplypandas                    Apply udf on a column
findmappandas                      Apply udf on a all elements
findapplymappandas                 Apply udf on a all elements
finddatecsvpandas                  Read dates from csv file
findpivotpandas                    pivot
findreshapepandas                  reshape
findlongtowidepandas               Long to wide
findmeltpandas                     melt
findwidetolongpandas               Wide to long
finditeraterowspandas              Iterate rows
finditerrowspandas                 Iterate rows
findsavecsvpandas                  Save DataFrame to csv file
findtocsvpandas                    Save DataFrame to csv file
findreadcsvpandas                  Read csv file to DataFrame
findcreatedataframepandas          Create DataFrame
findrankpandas                     Rank
findcorrelationpandas              Correlation
findplotpandas                     Plot in Python, using matplotlib
finddatespandas                    Dates in pandas
findjoinpandas                     Join
findmergepandas                    Join, merge, concat
findconcatpandas                   Join, merge, concat
finddropnanpandas                  Drop the rows with NaN
findheadpandas                     head
findtailpandas                     tail
findslicepandas                    Slice dataframe             
findselectpandas                   Select rows and columns
findilocpandas                     iloc, loc, ix
findlocpandas                      iloc, loc, ix
findixpandas                       iloc, loc, ix
findmaxpandas                      Max
findstatisticspandas               Statistics functions
findlinearregressionpandas         Linear Regression
findpolyfitpandas                  Polyfit
findsortpandas                     Sort
findgroupbypandas                  group by
findisnullpandas                   isnull or missing values
findmissingvaluespandas            isnull or missing values
findnormalizepandas                Normalize
findsumcolumnspandas               Sum of all columns 
findmeancolumnspandas              Mean of all columns 

** scikitlearn **

findscikitlearn                    Scikit-Learn
findsklearn                        Scikit-Learn
findpcasklearn                     PCA
findscalesklearn                   Scale data
findstandardizesklearn             Standardize data
findsplitsklearn                   Train test split
findtraintestsplitsklearn          Train test split
findkmeanssklearn                  K-means clustering
findclustersklearn                 K-means clustering
findconfusionmatrixsklearn         Confusion matrix
finsvmsklearn                      SVM
findcrossvalidationsklearn         Cross validation

(endfind)

==
# Read the book "A Byte of Python" in full again in May 2018, and recorded all my notes in this file.

Features of Python (from Tao):

1. There is no ; at the end of each statement.
2. We can not randomly add spaces in front of a statement.
3. In "i = 5", there can be spaces between =
4. No need to declare data type for a variable like: int i = 5.

==
(findgeneral)                        
Python general

Portable:

Due to its open-source nature, Python has been ported to (i.e. changed to make it work on) many platforms. All your Python programs can work on any of these platforms without requiring any changes at all if you are careful enough to avoid any system-dependent features.

--
Interpreted:

A program written in a compiled language like C or C++ is converted from the source language i.e. C or C++ into a language that is spoken by your computer (binary code i.e. 0s and 1s) using a compiler with various flags and options. When you run the program, the linker/loader software copies the program from hard disk to memory and starts running it.

Python, on the other hand, does not need compilation to binary. You just run the program directly from the source code. Internally, Python converts the source code into an intermediate form called bytecodes and then translates this into the native language of your computer and then runs it. All this, actually, makes using Python much easier since you don't have to worry about compiling the program, making sure that the proper libraries are linked and loaded, etc, etc. This also makes your Python programs much more portable, since you can just copy your Python program onto another computer and it just works!

--
Python is strongly object-oriented in the sense that everything is an object including numbers, strings and functions.

--
To test if you have Python already installed on your Linux box:

$ python -V

If you have Python 2.x already installed, then try python3 -V

--
There are two ways of using Python to run your program - using the interactive interpreter
prompt or using a source file.

Using The Interpreter Prompt:

$ python
>>> print('Hello World')

$ python3
>>> print('Hello World')

To exit the prompt, press ctrl-d 

--
Like C++, unlike Java:
A file name can be different from the class name. A file can even have no class in it.

--
Using A Source File:

I follow the convention of having all Python programs saved with the extension .py

-- File helloworld.py starts --

#!/usr/bin/python
#Filename: helloworld.py
print('Hello World')

-- File helloworld.py ends --

Run the above program:

$ python helloworld.py

See below for the benefit of specifying the interpreter at the beginning of the file

--
Executable Python Programs:

First, we have to give the program executable permission using the chmod command then run the source program.

$ chmod a+x helloworld.py
$ ./helloworld.py

The chmod command is used here to change the mode of the file by giving execute permission to all users of the system.

--
Tao: the benefit of specifying the interpreter at the beginning of the file:

You can rename the file to just helloworld and run it as ./helloworld and it will still work since the system knows that it has to run the program using the interpreter whose location is specified in the first line in the source file.

What if you don't know where Python is located? Then, you can use the special env program on Linux/Unix systems. Just change the first line of the program to the following:

#!/usr/bin/env python

The env program will in turn look for the Python interpreter which will run the program.

--
Python does not use comments except for the special case of the first line here (#!/usr/bin/python). It is called the shebang line - whenever the first two characters of the source file are #! followed by the location of a program, this tells your Linux/Unix system that this program should be run with this interpreter when you execute the program.

W.r.t. Python, a program or a script or software all mean the same thing.

--
If you want to specify more than one logical line on a single physical line, then you have to
explicitly specify this using a semicolon (;) which indicates the end of a logical
line/statement. For example:

i = 5; print(i);

An example of writing a logical line spanning many physical lines follows. This is referred to
as explicit line joining.
s = 'This is a string. \
This continues the string.'
print(s)

This gives the output:
This is a string. This continues the string.
Similarly,
print\
(i)
is the same as
print(i)

--
Indentation

Statements which go together must have the same indentation. Each such set of statements is called a block

I strongly recommend that you use a single tab or four spaces for each indentation level.

Python will always use indentation for blocks and will never use braces.

In the above file (helloworld.py), ensure there are no spaces or tabs before the first character in each line

--
What if we wanted to be able to run the program from anywhere? 

You can do this by storing the program in one of the directories listed in the PATH environment variable. Whenever you run any program, the system looks for that program in each of the directories listed in the PATH environment variable and then runs that program. We can make this program available everywhere by simply copying this source file to one of the directories listed in PATH.

$ echo $PATH
/usr/local/bin:/usr/bin:/bin:/usr/X11R6/bin:/home/swaroop/bin

$ cp helloworld.py /home/swaroop/bin/helloworld

$ helloworld
Hello World

We see that /home/swaroop/bin is one of the directories in the PATH variable

Alternatively, you can add a directory of your choice to the PATH variable - this can be done by running PATH=$PATH:/home/swaroop/mydir

This method is very useful if you want to write useful scripts that you want to run the program anytime, anywhere. It is like creating your own commands just like cd or any other commands that you use in the Linux terminal or DOS prompt.

--
Python is case-sensitive

--
(findcomment)
Comment in Python:

Anything to the right of the # symbol is a comment

--
If you need quick information about any function or statement in Python, then you can use the built-in help functionality. This is very useful especially when using the interpreter prompt. For example, run help(print) - this displays the help for the print function which is used to print things to the screen. Use help() to learn
more about using help itself! In case you need to get help for operators like return, then you need to put those inside quotes such as help('return') so that Python doesn't get confused on what we're trying to do.

Press q to exit the help.

==
(finddatatypes)
Data types:

# Show data type:
i = 123
type(i)
# Displays: <type 'int'>

# Convert string to int:
x = '123456'
i = int(x)

# Convert int to string:
str(10) # Returns: '10'

Numbers:

Numbers in Python are of three types - integers, floating point (or floats for short) and complex numbers.

Examples:

Integer: 2
Float: 3.23, 52.3E-4.
Complex number: (-5+4j), (2.3 - 4.6j)

The default integer type can be any large value.

Boolean:

The True and False are called Boolean types and you can consider them to be equivalent to the value 1 and 0 respectively.

if True:
    print('Yes, it is true')

--
Print the type of a variable:

>>> i = 123
>>> type(i)
<type 'int'>
>>> type(i) is int
True

==
(findnull)
(findnone)
None

与C不同的是，在python中是没有NULL的，取而代之的是None，它的含义是为空，但要注意和空列表与空字符串的区别，None的类型是Nonetype

>>>a=None
>>>type(a)
<class 'Nonetype'>

另外，None是没有像len,size等属性的，要判断一个变量是否为None，直接使用
if a == None. 

Same as C++:
"if a" is equivalent to "if a != None"
"if not a" is equivalent to "if a == None"

再者，注意None与布尔类型的区别，布尔类型只包括两个：True和False（注意它的大小写）
但python是把0，空字符串‘ ’和None都看作False，把其他数值和非空字符串都看作True

==
(findstring)
String

name = 'john'

len(str) # Length of str

--
(findsubstring)

** Return substring:

To return a substring from a string, search for "find sequence",
this is also what people did online.

** Whether contains substring:

Determine whether a string contains a substring:
substr in str # Returns True if str contains substr, otherwise false.

** Position of substring:

str1 = "this is string example....wow!!!";
str2 = "exam";

print str1.find(str2) # Returns: 15 <- the index of 'e' in str1

** Starts with **

'helo'.startswith('he') # Returns: true

** Ends with **

'helo'.endswith('lo') # Returns: true

** Contains substring **

'el' in 'helo' # Returns: true


--
(findsplit)
Split a string

split() method returns a list of strings after breaking the given string by the specified separator.

# Splits at space
text = 'geeks for geeks'
text.split() # Returns: ['geeks', 'for', 'geeks']
 
# Splits at ',' 
word = 'geeks, for, geeks'
word.split(', ') # Returns: ['geeks', 'for', 'geeks']

--
(findstrip)
(findtrim)
Strip a string (remove charaters at beginning and end)

str.strip() removes all whitespace at the start and end, including spaces, tabs, newlines and carriage returns. Leaving it in doesn't do any harm, and allows your program to deal with unexpected extra whitespace inserted into the file.

# str.strip([chars]): 相當於Java中的trim(), returns a copy of the string in which all chars have been stripped(即removed) from the beginning and the end of the string (default whitespace characters).

str = "0000000this is string example....wow!!!0000000";
str.strip('0') # Returns: "this is string example....wow!!!"

--
(findprefix)
Whether a string start with a prefix

The method startswith() checks whether string starts with str

str = "this is string example"
str.startswith('this') # Returns True

--
(findjoinpandaslisttostring)
Join list to string

delimiter = '_*_'
mylist = ['Brazil', 'Russia', 'India', 'China']
delimiter.join(mylist) # Returns 'Brazil_*_Russia_*_India_*_China'

--
(findargs)   
(findargv)    
Parse command line arguments   

** Simple way **

import sys

for i in sys.argv:
    print(i)

$ python using_sys.py we are arguments

using_sys.py
we
are
arguments

** Complicated way **

File prog.py:

import optparse
parser = optparse.OptionParser()
parser.add_option('-q', '--query', action="store", dest="query") # Tao: dest="query" means the input argument is stored in options.query.
options, args = parser.parse_args()
print 'Query string=', options.query

Using the file prog.py:

python prog.py -q helo
# Output: Query string= helo

python prog.py --query helo 
# Output: Query string= helo

--
Single Quotes:
You can specify strings using single quotes such as 'Quote me on this'.

Double Quotes:
Strings in double quotes work exactly the same way as strings in single quotes. An example
is "What's your name?"

Triple Quotes:
You can specify multi-line strings using triple quotes - (""" or '''). You can use single quotes
and double quotes freely within the triple quotes. An example is:
'''This is a multi-line string. This is the first line.
This is the second line.
"What's your name?," I asked.
He said "Bond, James Bond."
'''

--
Reverse a string: nameStr[::-1]

--
# The format function does not only applies to print, but applies to all strings:

name = "Tao"
s = "My name is {}".format(name) 

--
(findescape)
Escape Sequences:

You specify the single quote as \'
Now, you can specify the string as 'What\'s your name?'

What if you wanted to specify a two-line string? One way is to use a triple-quoted string as shown previously or you can use an escape sequence for the newline character - \n to indicate the start of a new line.

One thing to note is that in a string, a single backslash at the end of the line indicates that the string is continued in the next line, but no newline is added. For example:
"This is the first sentence.\
This is the second sentence."
is equivalent to "This is the first sentence. This is the second sentence.".

--
Raw Strings
If you need to specify some strings where no special processing such as escape sequences are handled, then what you need is to specify a raw string by prefixing r or R to the string. An example is r"Newlines are indicated by \n".

Always use raw strings when dealing with regular expressions. Otherwise, a lot of backwhacking may be required. For example, backreferences can be referred to as '\\1' or r'\1'.

--
Strings Are Immutable
This means that once you have created a string, you cannot change it.

--
String Literal Concatenation
If you place two string literals side by side, they are automatically concatenated by Python.
For example, 'What\'s ' 'your name?' is automatically converted in to "What's your name?".

--
There is no separate char data type in Python. There is no real need for it and I am sure you won't miss it.

--
The format Method

Sometimes we may want to construct strings from other information. This is where the format() method is useful. The format method can be called to substitute those specifications with corresponding arguments to the format method.

age = 25
name = 'Swaroop'
print('{0} is {1} years old'.format(name, age)) # Output: Swaroop is 25 years old
print('Why is {0} playing with that python?'.format(name)) # Output: Why is Swaroop playing with that python?

Notice that we could achieved the same using string concatenation: name + ' is ' + str(age) + ' years old' but notice how much uglier and error-prone this is.

More examples of format method:

>>> '{0:.3}'.format(1/3) # decimal (.) precision of 3 for float
'0.333'

>>> '{0:_^11}'.format('hello') # fill with underscores (_) with the text centered (^) to 11 width
'___hello___'

>>> '{name} wrote {book}'.format(name='Swaroop', book='A Byte of Python') # keyword-based
'Swaroop wrote A Byte of Python'

==
(findoperators)
Operators

Evaluation order:
Remember (same as Java and C++):
->||

There are no ++ and -- operators in Python.
x++ can be written as x += 1 and x-- can be written as x -= 1 <- tao: so there are += and -= operators in Python.

Shortcut:
a *= 3
is equivalent as
a = a * 3

Power:
3 ** 4 gives 3 * 3 * 3 * 3

Divide:
4 / 3 gives 1.3333333333333333, not 1!

Floor Division:
4 // 3 gives 1

Modulus:
5 % 2 gives 1

(findand)
(findor)
(findnot)
Bitwise AND: &
Bit-wise OR: |

Boolean AND: and
Boolean OR:  or 
Boolean NOT: not

Less Than: <
Less Than or Equal To: <=

Equal To: ==
x = 'str'; 
y = 'str'; 
x == y returns True.

Not Equal To: !=

Right Shift: >>
11 >> 1 gives 5. 11 is represented in bits by 1011 which when right shifted by 1 bit gives 101 which is the decimal 5.

Left Shift: <<

==
(findmath)
Math functions

import math 
math.exp(-45.17) # The method exp() returns returns exponential of x: e^x.
math.sqrt(100)

# No need to import anything for abs
abs(-23.56)
abs(-46)


==
(findprint)
print

A better way to print without using format (from online and tao's experiment):

a = 1
b = 2
print('a =', a) # Output: a = 1
print('a = ', a, ', b = ', b, sep="") # Output: a = 1, b = 2

--
age = 25
age *= 2
name = 'John'
print(age)
print('{0} is speaking'.format(name))
print('{0} is {1} years old'.format(name, age))
print('{name} is {age} years old'.format(name = 'Kevin', age = 40))
print('Age is', age) # Output: Age is 25, note a space is added between "Age is" and "25".

By deafult, the print() function prints the text as well as an automatic newline to the screen. Tao: to override it, use this:
print(line, end = 'a')
this makes the newline character replaced by 'a'.

--
From online:

print in Python 3 vs Python 2:

Old: print "The answer is", 2*2
New: print("The answer is", 2*2)

Old: print x,           # Trailing comma suppresses newline
New: print(x, end=" ")  # Appends a space instead of a newline

Old: print              # Prints a newline
New: print()            # You must call the function!

==
(findreadfrominput)
Read from input in terminal

# something = input('Enter text: ') //Prints "Enter text: " to the screen and waits for input from the user.
# Enter text: sir // sir是用户從鍵盤輸入的, 然後something就等於sir了

==
(findreadfromfile)
Read from file 

file = open('poem.txt') # if no mode is specified, 'r'ead mode is assumed by default

while True:
    line = file.readline()
    if len(line) == 0: # Zero length indicates EOF
        break
    print(line, end = '') # Suppress the newline at end of each output line, see more below.
file.close() # close the file

By deafult, the print() function prints the text as well as an automatic newline to the screen. We are suppressing the newline by specifying end='' because the line that is read from the file already ends with a newline character.

==
(findwritetofile)
Write to file

Write to file:

f = open('poem.txt', 'w') # w: write mode. r: read mode. a: append mode.
f = open('poem.txt', 'w+') # w+: the + sign that means it will create a file if it does not exist
f.write('helo') # write text to file
f.close() # close the file

Determine whether a file exists:

import os
os.path.exists('/this/is/a/dir') # Returns true for directories, not just files.

==
(findreadfromwebpage)
Read from webpage:

link = "https://stackoverflow.com/questions/15138614/how-can-i-read-the-contents-of-an-url-with-python"
f = urllib.urlopen(link)
myfile = f.read()
print myfile

==
(findwhile)
(findif)
(findbreak)
(findcontinue)
(findtrue)

while, if, cast, break, continue, True

number = 1
running = True

while running:
    #guess = int(input("Input a number: ")) //由此可知, int(str)可以將 string類型的str 轉化為int
    guess = 1

    if guess == number:
        print("That's right.")
        running = False
    elif guess > number:
        print('Too big.')
        break
    else:
        print('Too small.')
        continue
else:
    print('The while loop is over.') // A while statement can have an optional else clause. The else block is executed when the while loop condition becomes False

There is no switch statement in Python.

The continue statement is used to tell Python to skip the rest of the statements in the current loop block and to continue to the next iteration of the loop.

The break statement is used to break out of a loop statement i.e. stop the execution of a looping statement, even if the loop condition has not become False or the sequence of items has been completely iterated over.

An important note is that if you break out of a for or while loop, any corresponding loop else block is not executed.

==
(findcast)
int(x)
float(x)

--
(findround)
Round to nearest integer:

int(round(x))

--
(findprecision)
Display a float with two decimal places:

>>> '%.2f' % 1.234
'1.23'

>>> '%.2f' % 5.0
'5.00'

==
(findfor)
(findrange)
for, range

for i in range(1, 5): # range(1, 5)是一個sequence: [1, 2, 3, 4]. 它包括1, 但不包括5! range(1,5,2) = [1,3].
    print(i)

Remember that the for..in loop works for any sequence. Here, we have a list of numbers generated by the built-in range function, but in general we can use any kind of sequence of any kind of objects!

--
fruits = ['banana', 'apple',  'mango']

for fruit in fruits:        
   print fruit

--
Question:

Is there a way to step between 0 and 1 by 0.1?

I thought I could do it like the following, but it failed:

for i in range(0, 1, 0.1):
    print i
Instead, it says that the step argument cannot be zero, which I did not expect.

Answer:

Rather than using a decimal step directly, it's much safer to express this in terms of how many points you want. Otherwise, floating-point rounding error is likely to give you a wrong result.

You can use the linspace function from the NumPy library (which isn't part of the standard library but is relatively easy to obtain). linspace takes a number of points to return, and also lets you specify whether or not to include the right endpoint:

>>> np.linspace(0,1,11)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1. ])
>>> np.linspace(0,1,10,endpoint=False)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])
If you really want to use a floating-point step value, you can, with numpy.arange.

>>> import numpy as np
>>> np.arange(0.0, 1.0, 0.1)
array([ 0. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9])
Floating-point rounding error will cause problems, though. Here's a simple case where rounding error causes arange to produce a length-4 array when it should only produce 3 numbers:

>>> numpy.arange(1, 1.3, 0.1)
array([1. , 1.1, 1.2, 1.3])

==
(findfunction)
function

def sayHello():
    print('Hello World!')

sayHello() # Call the function

def fahrenheit(value):
    return (value * 9 / 5) + 32

Tao: from above and below, Python also uses naming conventions like sayHello (Java style), rather than say_hello (C++ style).

--
(findparameter)
Function parameters:

Note the terminology used - the names given in the function definition are called parameters whereas the values you supply in the function call are called arguments.

def printMax(a, b):
    if a > b:
        print(a, 'is maximum')
    elif a == b:
        print(a, 'is equal to', b)
    else:
        print(b, 'is maximum')

printMax(3, 4)

x = 5
y = 7
printMax(x, y)

--
(findreturn)
The return Statement

def maximum(x, y):
    if x > y:
        return x
    else:
        return y

maximum(2, 3) //Returns 3

Note that a return statement without a value is equivalent to return None. None is a special type in Python that represents nothingness. For example, it is used to indicate that a variable has no value if it has a value of None.

Every function implicitly contains a return None statement at the end unless you have written your own return statement. 

--
(findpass)
Pass statement

def someFunction():
    pass

The pass statement is used in Python to indicate an empty block of statements.

--
(finddocstring)
DocStrings

Python has a nifty feature called documentation strings, usually referred to by its shorter name docstrings. DocStrings are an important tool that you should make use of since it helps to document the program better and makes it easier to understand.

A string on the first logical line of a function is the docstring for that function. Note that DocStrings also apply to modules and classes.

The convention followed for a docstring is a multi-line string where the first line starts with a capital letter and ends with a dot. Then the second line is blank followed by any detailed explanation starting from the third line. You are strongly advised to follow this convention for all your docstrings for all your non-trivial functions.

We can access the docstring of the printMax function using the __doc__ (notice the double underscores) attribute (name belonging to) of the function.

If you have used help() in Python, then you have already seen the usage of docstrings! What it does is just fetch the __doc__ attribute of that function and displays it in a neat manner for you. You can try it out on the function above - just include help(printMax) in your program. Remember to press the q key to exit help.

def printMax(x, y):
    '''Prints the maximum of two numbers.

    The two values must be integers.'''
    x = int(x) # convert to integers, if possible
    y = int(y)

    if x > y:
        print(x, 'is maximum')
    else:
        print(y, 'is maximum')

printMax(3, 5) # Output: 5 is maximum

print(printMax.__doc__)
# Output:
Prints the maximum of two numbers.

    The two values must be integers.

We can use docstrings for classes as well as methods. We can access the class docstring at runtime using Robot.__doc__ and the method docstring as Robot.sayHi.__doc__

--
(findlocalvariables)
Local variables

When you declare variables inside a function definition, they are not related in any way to other variables with the same names used outside the function i.e. variable names are local to the function. This is called the scope of the variable. All variables have the scope of the block they are declared in starting from the point of definition of the name.

x = 50

def func(x):
    print('x is', x) # Output: x is 50
    x = 2
    print('Changed local x to', x) # Output: Changed local x to 2

func(x)
print('x is still', x) # Output: x is still 50

--
(findglobal)
Global statement

If you want to assign a value to "a name defined at the top level of the program" (i.e. not inside any kind of scope such as functions or classes), then you have to tell Python that the name is not local, but it is global. We do this using the global statement. It is impossible to assign a value to a variable defined outside a function without the global statement.

x = 50

def func():
    global x
    print('x is', x) # Output: x is 50
    x = 2
    print('Changed global x to', x) # Output: Changed global x to 2

func()
print('Value of x is', x) # Output: Value of x is 2

--
(findnonlocal)
Nonlocal statement

We have seen how to access variables in the local and global scope above. There is another kind of scope called "nonlocal" scope which is in-between these two types of scopes. Nonlocal scopes are observed when you define functions inside functions. Since everything in Python is just executable code, you can define functions anywhere.

def func_outer():
    x = 2
    print('x is', x) # Output: x is 2

    def func_inner():
        nonlocal x
        x = 5

    func_inner()

    print('Changed local x to', x) # Output: Changed local x to 5

func_outer()

--
(finddefaultargument)
Default Argument Values

For some functions, you may want to make some of its parameters as optional and use default values if the user does not want to provide values for such parameters. This is done with the help of default argument values.

Only those parameters which are at the end of the parameter list can be given default argument values. This is because the values are assigned to the parameters by position.

def say(message, times = 1):
    print(message * times)

say('Hello') # Output: Hello
say('World', 5) # Output: WorldWorldWorldWorldWorld

--
(findkeywordargument)
Keyword Arguments

If you have some functions with many parameters and you want to specify only some of them, then you can give values for such parameters by naming them - this is called keyword arguments - we use the name (keyword) instead of the position (which we have been using all along) to specify the arguments to the function.

We can give values to only those parameters which we want, provided that the other parameters have default argument values.

def func(a, b=5, c=10):
    print('a is', a, 'and b is', b, 'and c is', c)

func(3, 7) # Output: a is 3 and b is 7 and c is 10
func(25, c=24) # Output: a is 25 and b is 5 and c is 24
func(c=50, a=100) # Output: a is 100 and b is 5 and c is 50

--
(findvararg)
VarArgs parameters

Sometimes you might want to define a function that can take any number of parameters, this can be achieved by using the stars:

def total(initial=5, *numbers, **keywords):
    count = initial
    for number in numbers:
        count += number
    for key in keywords:
        count += keywords[key]
    return count

total(10, 1, 2, 3, vegetables=50, fruits=100)
      --  -------  -------------------------
  initial numbers              keywords
          (numbers is a list)  (keywords is a dictionary)


When we declare a starred parameter such as *param, then all the positional arguments from that point till the end are collected as a list called 'param'.

Similarly, when we declare a double-starred parameter such as **param, then all the keyword arguments from that point till the end are collected as a dictionary called 'param'.

--
(findkeywordonly)
Keyword- only Parameters

If we want to specify certain keyword parameters to be available as keyword-only and not as positional arguments, they can be declared after a starred parameter:

def total(initial=5, *numbers, vegetables):
    count = initial
    for number in numbers:
        count += number
    count += vegetables
    return count

total(10, 1, 2, 3, vegetables=50)
total(10, 1, 2, 3) # Raises error because we have not supplied a default argument value for 'vegetables'

Declaring parameters after a starred parameter results in keyword-only arguments. If these arguments are not supplied a default value (Tao: in the function definition), then calls to the function will raise an error if the keyword argument is not supplied, as seen above.

If you want to have keyword-only arguments but have no need for a starred parameter, then simply use an empty star without using any name such as def total(initial=5, *, vegetables).

==
(findmodule)
Modules

Tao: modules in Python is like packages in Java and Scala.

What if you wanted to reuse a number of functions in other programs that you write? The answer is modules.

There are various methods of writing modules, but the simplest way is to create a file with a .py extension that contains functions and variables.

Another method is to write the modules in the native language in which the Python interpreter itself was written. For example, you can write modules in the C programming language and when compiled, they can be used from your Python code when using the standard Python interpreter.

A module can be imported by another program to make use of its functionality. This is how we can use the Python standard library as well. First, we will see how to use the standard library modules.

Example:

#!/usr/bin/python
# Filename: using_sys.py

import sys

print('The command line arguments are:')
for i in sys.argv:
    print(i)

print('\n\nThe PYTHONPATH is', sys.path, '\n')

$ python using_sys.py we are arguments

The command line arguments are:
using_sys.py
we
are
arguments
The PYTHONPATH is ['', 'C:\\Windows\\system32\\python30.zip',
'C:\\Python30\\DLLs', 'C:\\Python30\\lib',
'C:\\Python30\\lib\\plat-win', 'C:\\Python30',
'C:\\Python30\\lib\\site-packages']

When Python executes the import sys statement, it looks for the sys module. In this case, it is one of the built-in modules, and hence Python knows where to find it.

If it was not a compiled module i.e. a module written in Python, then the Python interpreter will search for it in the directories listed in its sys.path variable. If the module is found, then the statements in the body of that module is run and then the module is made available for you to use. Note that the initialization is done only the first time that we import a module.

The sys.argv variable is a list of strings. Python stores the command line arguments in the sys.argv variable for us to use.

Remember, the name of the script running is always the first argument in the sys.argv list. Notice that Python starts counting from 0 and not 1.

The sys.path contains the list of directory names where modules are imported from. Observe that the first string in sys.path is empty - this empty string indicates that the current directory is also part of the sys.path which is same as the PYTHONPATH environment variable. This means that you can directly import modules located in the current directory. Otherwise, you will have to place your module in one of the directories listed in sys.path. 

--
from . . . import . . .

Tao: in the above, even you imported sys, you still need to type sys.argv rather than only argv each time.

If you want to directly import the argv variable into your program (to avoid typing the
sys. everytime for it), then you can use the 
from sys import argv 
statement. 

If you want to import all the names used in the sys module, then you can use the 
from sys import *
statement. This works for any module. In general, you should avoid using this statement and use the import statement instead since your program will avoid name clashes and will be more readable.

--
Making Your Own Modules

Creating your own modules is easy, you've been doing it all along! This is because every
Python program is also a module. You just have to make sure it has a .py extension.

Example:

#!/usr/bin/python
# Filename: mymodule.py

def sayhi():
    print('Hi, this is mymodule speaking.')

__version__ = '0.1'

# End of mymodule.py

The above was a sample module. As you can see, there is nothing particularly special about
compared to our usual Python program.

Remember that the module should be placed in the same directory as the program that we
import it in, or the module should be in one of the directories listed in sys.path.

#!/usr/bin/python
# Filename: mymodule_demo.py

import mymodule
mymodule.sayhi()

print ('Version', mymodule.__version__)

$ python mymodule_demo.py
Hi, this is mymodule speaking.
Version 0.1

Here is a version utilising the from..import syntax:
#!/usr/bin/python
# Filename: mymodule_demo2.py

from mymodule import sayhi, __version__

sayhi()
print('Version', __version__)

You could also use:
from mymodule import *
This will import all public names such as sayhi but would not import __version__
because it starts with double underscores.

--
How to import module from different folder (from online):

By default, you can't. When importing a file, Python only searches the current directory, the directory that the entry-point script is running from, and sys.path which includes locations such as the package installation directory (it's actually a little more complex than this, but this covers most cases).

However, you can add to the Python path at runtime:

# some_file.py
import sys
sys.path.append('/path/to/application/app/folder')

import file

==
Byte- compiled .pyc files

Importing a module is a relatively costly affair, so Python does some tricks to make it faster. One way is to create byte-compiled files with the extension .pyc which is an intermediate form that Python transforms the program into. This .pyc file is useful when you import the module the next time from a different program - it will be much faster since a portion of the processing required in importing a module is already done. Also, these byte-compiled files are platform-independent.

--
_ _ name_ _

Every module has a name. This is handy in the particular situation of figuring out if the module is being run standalone or being imported. As mentioned previously, when a module is imported for the first time, the code in that module is executed. We can use this concept to alter the behavior of the module if the program was used by itself and not when it was imported from another module. This can be achieved using the __name__ attribute of the module.

Example:
#!/usr/bin/python
# Filename: using_name.py
if __name__ == '__main__':
    print('This program is being run by itself')
else:
    print('I am being imported from another module')

$ python using_name.py
This program is being run by itself

$ python
>>> import using_name
I am being imported from another module

--
dir

When you supply a module name to the dir() function, it returns the list of the names
defined in that module. When no argument is applied to it, it returns the list of names
defined in the current module.

==
Packages

What if you wanted to organize modules? That's where packages come into the
picture.

Packages are just folders of modules with a special __init__.py file that indicates to
Python that this folder is special because it contains Python modules.

==
Data Structures

There are four built-in data structures in Python - list, tuple, dictionary and set.

tuple用(), list用[], dictionary用{}, 它們依次是 小中大 括號

Python uses 0-based indexing
R uses 1-based indexing

==
(findtuple)
tuple

Tao: tuple is like array in C++

tuples are immutable.

The pair of parentheses in tuples is optional.

zoo = ('monkey', 'tiger', 'cat')
a = len(zoo)
b = zoo[0]
print(zoo)

newZoo = ('pig', 'dog', zoo) # newZoo = ('monkey', 'camel', ('python', 'elephant', 'penguin'))
newZoo[2] # Returns ('python', 'elephant', 'penguin')
newZoo[2][2] # Returns 'penguin'
len(newZoo)
len(newZoo[2])

emptyTuple = ()
singleton = (2, ) # A tuple containing the item 2 (only one item). Must define like this. (2) means a pair of parentheses surrounding the object in an expression (tao: like an arithmetic expression (2 + 1)).

# passing tuples around:
def func():
    return (3, 'helo')

num, s = func()

Can define a tuple within a tuple, or a tuple within a list, or a list within a tuple.

==
(findlist)
list

Tao: list is like vector in C++

Lists are mutatble. 
You can add, remove or search for items in the list. we say that a list is a mutable data type

The list of items should be enclosed in square brackets.

odd = [1, 3, 5]
odd.append(7)
print(odd) # Output: [1, 3, 5, 7]

shoplist = ['apple', 'mango', 'orange']
a = shoplist[0]
b = len(shoplist)

Check if a list is empty (this is also what all people do online, it looks like there is no isEmpty function for list):

if len(my_list) == 0:
    pass

shoplist.append('banana')
shoplist.sort()
print(shoplist)

for item in shoplist:
    print(item, end=' ') #加end=' '是因為print()函數會自動在 print出的東西的 末尾加一個newline character, end=' '的作用就是將這個newline character換成空格.

del shoplist[0] #將shoplist的第0個元素 從shoplist中 刪掉

# using lists as stacks (from online):
stack = [3, 4, 6]
stack.append(6) #相當於push <- It adds 6 to the original list "stack", and it returns nothing.
stack.pop()

stackNew = stack + [40] <- This returns a new list

# using lists as queues (from online, many people do this):
from collections import deque
queue = deque(["Eric", "John", "Michael"])
queue.append("Terry") #相當於offer
left = queue.popleft() #相當於poll

list1 = [3, 4, 6]
a = sum(list1)
b = max(list1)
c = min(list1)

# empty list
my_list = []

==
(finddictionary)
dictionary

Tao: dictionary is like map in C++

ages = {'John' : 30, 'Mary' : 40}
a = ages['John']
b = len(ages)
del ages['John']
ages['Mary'] = 20 # update
ages['Kate'] = 40 # add an item
new_dict = {} # Create an empty dictionary

# Traverse a dictionary:
for name, age in ages.items(): #不是 for name : age ...
    print('{0} is {1} years old'.format(name, age))

# Existence of key in dictionary:
if 'Mary' in ages: # if ages contains key 'Mary'
    print('Has Mary.')

Remember that key-value pairs in a dictionary are not ordered in any manner. If you want a particular order, then you will have to sort them yourself before using it.

==
list and dictionary as function parameter

def total(count, *numbers, **keywords):
    for i in numbers:
        count += i
    for key in keywords:
        count += keywords[key]
    return count

print(total(0, 1, 2, 3, John=4, Jack=5))

==
(findset)
set

bric = set(['brazil', 'russia', 'india']) # A set is initialized from a list
'india' in bric # 相當於Java中的bri.contains('india')
bric.add('china')
bric.remove('china')
bric.issuperset(bri) # bri is also a set
bri & bric # OR bri.intersection(bric)
bricNew = bric.copy()

Sets are unordered.

==
(findsequence)
sequences 

Lists, tuples and strings are examples of sequences. The major features of sequences is that they have membership tests (i.e. the in and not in expressions) and indexing operations. 

Sequences also have a slicing operation which allows us to retrieve a slice of the sequence i.e. a part of the sequence.

name = 'jan'
name[-1] # Returns 'n'
name[4] # 報錯: IndexError: string index out of range
name[1:3]  #包括name[1], 但不包括name[3]
name[2:]
name[:] # a copy of the whole sequence
name[::2] # 2 is the step
name[:-1] # Returns a slice of the sequence which excludes the last item of the sequence but contains everything else.
name[::-1] # Reverse the sequence


==
(findclass)
class

Tao: about self:
1. In a class, all member methods should have a self parameter: def funcName(self, x, y). When calling this method, no need to insantiate this self parameter: funcName(2, 3).
2. In a class, when using its own member variables or methods, should add self: self.variableName, self.func(). This is true even when a member function in a class calls itself recursively, it still needs to add self.

Tao: in a class, no need to define its member variables. Any variables can jump out suddenly like the self.name below.

Tao: there is no way to overload __init__ method in Python (confirmed from online). We need to use tricks to overload it or avoid having the desire to overload it.

# Example class 1:

class Person:
    # 這是constructor. self相當於Java中的this:
    def __init__(self, name): 
        self.name = name 
    
    def sayHi(self):
        print('Hi', self.name) 
    
    #def sayHello(): //Avadoles!!! 報錯, 因為所有class method必加self參數. 實踐表明, static methd不用加self參數
    #    print('Hello')

p = Person('John')
p.sayHi()

# Example class 2:

class Animal():
    pass # An empty block

anim = Animal()

Self: although, you can give any name for this parameter, it is strongly recommended that you
use the name self.

All class memembers (including the data memebers) are public. If you use data members with names using the double underscore prefix such as __privatevar, Python uses name-mangling to effectively make it a private variable.

fields vs variables, methods vs functions:

Variables that belong to an object or class are referred to as fields. Functions that belong to a class: such functions are called methods of the class. This terminology is important because it helps us to differentiate between functions and variables which are independent and those which belong to a class or object.

Tao: Functions can be out of class in Python.

--
Class And Object Variables

There are two types of fields - class variables and object variables:

Class variables are shared - they can be accessed by all instances of that class. There is only one copy of the class variable and when any one object makes a change to a class variable, that change will be seen by all the other instances. Tao: this is like static variable in Java.

Object variables are owned by each individual object/instance of the class. In this case, each object has its own copy of the field i.e. they are not shared and are not related in any way to the field by the same name in a different instance.

Example:

class Robot:
    # A class variable, counting the number of robots
    # Tao: therefore, do not explicitly define variables in a class unless you want to make it a class variable
    population = 0

    def __init__(self, name):
        self.name = name # Tao: self.name is an object variable
        print('Initializing {0}'.format(self.name))

        # When this person is created, the robot adds to the population
        Robot.population += 1

    def __del__(self):
        print('{0} is being destroyed!'.format(self.name))

        Robot.population -= 1

        if Robot.population == 0:
            print('{0} was the last one.'.format(self.name))
        else:
            print('There are still {0:d} robots working.'.format(Robot.population))

    def howMany():
        print('We have {0:d} robots.'.format(Robot.population))

    howMany = staticmethod(howMany)

droid1 = Robot('R2-D2') #Output: Initializing R2-D2)
Robot.howMany() #Output: We have 1 robots.

droid2 = Robot('C-3PO') #Output: Initializing C-3PO)
Robot.howMany() #Output: We have 2 robots.

del droid1 #Output: R2-D2 is being destroyed! (newline) There are still 1 robots working.
del droid2 #Output: C-3PO is being destroyed! (newline) C-3PO was the last one.
Robot.howMany() #Output: We have 0 robots.

Here, population belongs to the Robot class and hence is a class variable. The name variable belongs to the object (it is assigned using self) and hence is an object variable.

Thus, we refer to the population class variable as Robot.population and not as self.population. We refer to the object variable name using self.name notation in the methods of that object.

The howMany is actually a method that belongs to the class and not to the object. This means we can define it as either a classmethod or a staticmethod depending on whether we need to know which class we are part of. Since we don't need such information, we will go for staticmethod.

We could have also achieved the same using decorators. Decorators can be imagined to be a shortcut to calling an explicit statement:

@staticmethod
def howMany():
    print('We have {0:d} robots.'.format(Robot.population))

(finddestructor)
The __del__ method (see example above) is run when the object is no longer in use and there is no guarantee when that method will be run. If you want to explicitly see it in action, we have to use the del statement which is what we have done here.

==
(findinheritance)
Inheritance

       SchoolMember 
            |
     ------------------        
    |                  |
 Teacher             Student

class SchoolMember:
    def __init__(self, name, age):
        self.name = name
        self.age = age

    def tell(self):
        print('Name:"{0}" Age:"{1}"'.format(self.name, self.age), end=" ")

class Teacher(SchoolMember):
    def __init__(self, name, age, salary):
        SchoolMember.__init__(self, name, age)
        self.salary = salary

    def tell(self):
        SchoolMember.tell(self)
        print('Salary: "{0:d}"'.format(self.salary))

class Student(SchoolMember):
    def __init__(self, name, age, marks):
        SchoolMember.__init__(self, name, age)
        self.marks = marks

    def tell(self):
        SchoolMember.tell(self)
        print('Marks: "{0:d}"'.format(self.marks))

t = Teacher('Mrs. Shrividya', 40, 30000)
s = Student('Swaroop', 25, 75)

members = [t, s]

for member in members:
    member.tell() # works for both Teachers and Students

# Output:
Name:"Mrs. Shrividya" Age:"40" Salary: "30000"
Name:"Swaroop" Age:"25" Marks: "75"

Inherit from multiply classes: class Teacher(SchoolMember, Buyer, UncleFucker)

This is very important to remember - Python does not automatically call the constructor of the base class, you have to explicitly call it yourself.

Best illurstration of polymorphism:
You can refer to a Teacher or Student object as a SchoolMember object which could be useful in some situations such as counting of the number of school members. This is called polymorphism where a sub-type can be substituted in any situation where a parent type is expected i.e. the object can be treated as an instance of the parent class.

==
(findgetter)
(findsetter)
Getters and setters:

What's the pythonic way to use getters and setters?

The "Pythonic" way is not to use "getters" and "setters", but to use plain attributes, like the question demonstrates, and del for dereferencing (but the names are changed to protect the innocent... builtins).

The sample code is:

class C(object):
    def __init__(self):
        self._x = None

    @property
    def x(self):
        """I'm the 'x' property."""
        print("getter of x called")
        return self._x

    @x.setter
    def x(self, value):
        print("setter of x called")
        self._x = value

    @x.deleter
    def x(self):
        print("deleter of x called")
        del self._x


c = C()
c.x = 'foo'  # setter called
foo = c.x    # getter called
del c.x      # deleter called

==
References

When you create an object and assign it to a variable, the variable only refers to the object and does not represent the object itself. Tao: same as Java.

When you create an object and assign it to a variable, the variable only refers to the object and does not represent the object itself! That is, the variable name points to that part of your computer's memory where the object is stored. This is called as binding of the name to the object.

Remember that an assignment statement for lists does not create a copy. You have to use slicing operation to make a copy of the sequence.

The time.strftime() function takes a specification such as the one we have used in the
above program. The %Y specification will be replaced by the year without the century. The
%m specification will be replaced by the month as a decimal number between 01 and 12 and
so on.

==
(findexception)
Exceptions

We put all the statements that might raise exceptions/errors inside the try block and then put handlers for the appropriate errors/exceptions in the except clause/block. The except clause can handle a single specified error or exception, or a parenthesized list of errors/exceptions. If no names of errors or exceptions are supplied, it will handle all errors and exceptions.

Note that there has to be at least one except clause associated with every try clause. Otherwise, what's the point of having a try block?

If any error or exception is not handled, then the default Python handler is called which just
stops the execution of the program and prints an error message.

You can also have an else clause associated with a try..except block. The else clause is executed if no exception occurs.

try:
    text = input('Enter something --> ')
except EOFError:
    print('Why did you do an EOF on me?')
except KeyboardInterrupt:
    print('You cancelled the operation.')
else:
    print('You entered {0}'.format(text))

--
Raising Exceptions

You can raise exceptions using the raise statement by providing the name of the error/exception and the exception object that is to be thrown.

The error or exception that you can arise should be class which directly or indirectly must be a derived class of the Exception class.


# A user-defined exception class.'''
class ShortInputException(Exception):
    def __init__(self, length, atleast):
        Exception.__init__(self)
        self.length = length
        self.atleast = atleast

    try:
        text = input('Enter something --> ')
        if len(text) < 3:
            raise ShortInputException(len(text), 3)
        # Other work can continue as usual here

    except EOFError:
        print('Why did you do an EOF on me?')

    except ShortInputException as ex:
        print('ShortInputException: The input was {0} long, expected at least {1}'.format(ex.length, ex.atleast))

    else:
        print('No exception was raised.')

--
Finally

Suppose you are reading a file in your program. How do you ensure that the file object is closed properly whether or not an exception was raised? This can be done using the finally block.

In the following, observe that the KeyboardInterrupt exception is thrown and the program quits. However, before the program exits, the finally clause is executed and the file object is always closed.

import time

try:
    f = open('poem.txt')
    while True: # our usual file-reading idiom
        line = f.readline()
        if len(line) == 0:
            break
        print(line, end='')
        time.sleep(2) # To make sure it runs for a while

except KeyboardInterrupt:
    print('!! You cancelled the reading from the file.')

finally:
    f.close()
    print('(Cleaning up: Closed the file)')

--
The with statement

Acquiring a resource in the try block and subsequently releasing the resource in the finally block is a common pattern. Hence, there is also a with statement that enables this to be done in a clean manner:

with open("poem.txt") as f:
    for line in f:
        print(line, end='')

The output should be same as the previous example (the example of finally block). The difference here is that we are using the open function with the with statement - we leave the closing of the file to be done automatically by with open.

==
Standard Library

sys module

The sys module contains system-specific functionality. We have already seen that the sys.argv list contains the command-line arguments.

Suppose we want to check the version of the Python command being used. The first entry is the major version.

>>> import sys
>>> sys.version_info
(3, 0, 0, 'beta', 2)
>>> sys.version_info[0] >= 3
True

Ensure the program runs only under Python 3.0. We use another module from the standard library called warnings that is used to display warnings to the end-user:

import sys, warnings

if sys.version_info[0] < 3:
    warnings.warn("Need Python 3.0 for this program to run",
        RuntimeWarning)
else:
    print('Proceed as normal')

Output:

$ python2.5 versioncheck.py
versioncheck.py:6: RuntimeWarning: Need Python 3.0 for this program to run RuntimeWarning)

$ python3 versioncheck.py
Proceed as normal

--
logging module
(Tao: not important, can skip)

What if you wanted to have some debugging messages or important messages to be stored somewhere so that you can check whether your program has been running as you would expect it? How do you "store somewhere" these messages? This can be achieved using the logging module.

import os, platform, logging

if platform.platform().startswith('Windows'):
    logging_file = os.path.join(os.getenv('HOMEDRIVE'), os.getenv('HOMEPATH'), 'test.log')
else:
    logging_file = os.path.join(os.getenv('HOME'), 'test.log')

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s : %(levelname)s : %(message)s',
    filename = logging_file,
    filemode = 'w',
)

logging.debug("Start of the program")
logging.info("Doing something")
logging.warning("Dying now")

Output:
$python use_logging.py
Logging to C:\Users\swaroop\test.log

If we check the contents of test.log, it will look something like this:
2008-09-03 13:18:16,233 : DEBUG : Start of the program
2008-09-03 13:18:16,233 : INFO : Doing something
2008-09-03 13:18:16,233 : WARNING : Dying now

We use three modules from the standard library - the os module for interacting with the operating system, the platform module for information about the platform i.e. the operating system and the logging module to log information.

We use the os.path.join() function to put these three parts of the location together. The reason to use a special function rather than just adding the strings together is because this function will ensure the full location matches the format expected by the operating system.

Once the program has run, we can check this file and we will know what happened in the program, even though no information was displayed to the user running the program.

--
urllib and json modules
(Tao: not important, can skip)

How much fun would it be if we could write our own program that will get search results from the web? Let us explore that now.

This can be achieved using a few modules. First is the urllib module that we can use to fetch any webpage from the internet. We will make use of Yahoo! Search to get the search results and luckily they can give us the results in a format called JSON which is easy for us to parse because of the built-in json module in the standard library.

import sys

if sys.version_info[0] != 3:
    sys.exit('This program needs Python 3.0')

import json
import urllib, urllib.parse, urllib.request, urllib.response

# Get your own APP ID at http://developer.yahoo.com/wsregapp/

YAHOO_APP_ID =
'jl22psvV34HELWhdfUJbfDQzlJ2B57KFS_qs4I8D0Wz5U5_yCI1Awv8.lBSfPhwr'

SEARCH_BASE =
'http://search.yahooapis.com/WebSearchService/V1/webSearch'

class YahooSearchError(Exception):
    pass

# Taken from http://developer.yahoo.com/python/python-json.html
def search(query, results=20, start=1, **kwargs):
    kwargs.update({
        'appid': YAHOO_APP_ID,
        'query': query,
        'results': results,
        'start': start,
        'output': 'json'
})

url = SEARCH_BASE + '?' + urllib.parse.urlencode(kwargs)
result = json.load(urllib.request.urlopen(url))

if 'Error' in result:
    raise YahooSearchError(result['Error'])
return result['ResultSet']

query = input('What do you want to search for? ')

for result in search(query)['Result']:
    print("{0} : {1}".format(result['Title'], result['Url']))


==
More

Passing tuples around

Ever wished you could return two different values from a function? You can. All you have to do is use a tuple.

>>> def get_error_details():
... return (2, 'second error details')
...

>>> errnum, errstr = get_error_details()
>>> errnum
2

>>> errstr
'second error details'

Notice that the usage of a, b = <some expression> interprets the result of the expression as a tuple with two values.

If you want to interpret the results as (a, <everything else>), then you just need to star it just like you would in function parameters:

>>> a, *b = [1, 2, 3, 4]
>>> a
1
>>> b
[2, 3, 4]

This also means the fastest way to swap two variables in Python is:
>>> a = 5; b = 8
>>> a, b = b, a
>>> a, b
(8, 5)

--
Special Methods

There are certain methods such as the __init__ and __del__ methods which have special significance in classes.

Special methods are used to mimic certain behaviors of built-in types. For example, if you want to use the x[key] indexing operation for your class (just like you use it for lists and tuples), then all you have to do is implement the __getitem__() method and your job is done. If you think about it, this is what Python does for the list class itself! Some useful special methods are listed in the following table:

__init__(self, ...): This method is called just before the newly created object is returned for usage.

__del__(self): Called just before the object is destroyed

__str__(self): Called when we use the print function or when str() is used.

__lt__(self, other): Called when the less than operator (<) is used. Similarly, there are special
methods for all the operators (+, >, etc.)

__getitem__(self, key): Called when x[key] indexing operation is used.

__len__(self): Called when the built-in len() function is used for the sequence object.

--
Single Statement Blocks

In an if or loop, if the body has only one line, then can put this line in the same line as if or for:

>>> flag = True
>>> if flag: print 'Yes'

I strongly recommend avoiding this short-cut method, except for error checking.

--
Lambda Forms

A lambda statement is used to create new function objects and then return them at runtime.

Tao: in the below:
1. make_repeater returns a function object.
2. This function object is created by the lambda statement. The s in the lambda s is the parameter of this function object. s * n is the function body.
3. twice is such a function object, so it can be called as other functions: twice(5).

def make_repeater(n):
    return lambda s: s * n

twice = make_repeater(2)

print(twice('word')) # Output: wordword
print(twice(5)) # Output: 10

--
List Comprehension

List comprehensions are used to derive a new list from an existing list. Suppose you have a list of numbers and you want to get a corresponding list with all the numbers multiplied by 2 only when the number itself is greater than 2. List comprehensions are ideal for such situations.

listone = [2, 3, 4]
listtwo = [2*i for i in listone if i > 2]
print(listtwo) # Output: [6, 8]

--
exec and eval

The exec function is used to execute Python statements which are stored in a string or file, as opposed to written in the program itself. For example, we can generate a string containing Python code at runtime and then execute these statements using the exec statement:

>>> exec('print("Hello World")')
Hello World

Similarly, the eval function is used to evaluate valid Python expressions which are stored in a string. A simple example is shown below.

>>> eval('2*3')
6

--
The assert statement

The assert statement is used to assert that something is true. For example, if you are very sure that you will have at least one element in a list you are using and want to check this, and raise an error if it is not true, then assert statement is ideal in this situation. When the assert statement fails, an AssertionError is raised.

>>> mylist = ['item']
>>> assert len(mylist) >= 1
>>> mylist.pop()
'item'
>>> mylist
[]
>>> assert len(mylist) >= 1
Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
AssertionError

The assert statement should be used judiciously. Most of the time, it is better to catch exceptions, either handle the problem or display an error message to the user and then quit.

--
The repr function

The repr function is used to obtain a canonical string representation of the object. The    interesting part is that you will have eval(repr(object)) == object most of the time.

>>> i = []
>>> i.append('item')
>>> repr(i)
"['item']"
>>> eval(repr(i))
['item']
>>> eval(repr(i)) == i
True

Basically, the repr function is used to obtain a printable representation of the object. You can control what your classes return for the repr function by defining the __repr__ method in your class.

--
(findsleep)
sleep

The method sleep() suspends execution for the given number of seconds. The argument may be a floating point number to indicate a more precise sleep time.

import time
time.sleep(5) # Sleep for 5 seconds.

--
(findtime)
Output current local time

import datetime
time_str = (datetime.datetime.now() - datetime.timedelta(hours = 5)).strftime("%H:%M, %Y-%m-%d") # Minus 5 hours, may not be necessary.
print('Chicago time: {0}\n'.format(time_str)) # Output: Chicago time: 16:33, 2018-08-01

--
t1 = time.time()
print "ml4t"
t2 = time.time()
print "The time taken by print statement is ", t2 - t1, "seconds"


==
others:

# Python程序的文件名不用跟class名一樣, 甚至程序裡可以不含class.

# When you create an objet and assign it to a variable, the variable only refers to the object and does not represent the object itself.

# Python is strongly object-oriented in the sense that everything is an object including numbers, strings and functions.

# Python中的函數是可以在class之外的

# swap two variables:
# a = 5; b = 8
# a, b = b, a

# Read a list of numbers (in string form) from input and convert them into a list of int (from HackerRank):
# arr = [int(arr_temp) for arr_temp in input().strip().split(' ')]  # 此句中strip()還可以刪掉輸入末尾的newline, 當然同時也刪空格(from Haddop課).
# now arr is a list of int

# abs(-45), abs(100.12)

# 實踐表明, Python中連注釋都要正式indent, 否則報錯

==
The current date and time which we find out using the time.strftime() function. (import time).

Notice the use of os.sep variable (import os) - this gives the directory separator according to your
operating system i.e. it will be '/' in Linux, Unix, it will be '\\' in Windows and ':' in
Mac OS. Using os.sep instead of these characters directly will make our program portable
and work across these systems.

==
The zip command that we are using has some options and parameters passed. The -q
option is used to indicate that the zip command should work quietly. The -r option
specifies that the zip command should work recursively for directories i.e. it should include
all the subdirectories and files. The two options are combined and specified in a shortcut as
-qr. The options are followed by the name of the zip archive to create followed by the list of
files and directories to backup. We convert the source list into a string using the join
method of strings which we have already seen how to use.

Then, we finally run the command using the os.system function which runs the command
as if it was run from the system i.e. in the shell - it returns 0 if the command was
successfully, else it returns an error number.

==
target = today + os.sep + now + '_' + \ comment.replace(' ', '_') + '.zip'

==
# Create the subdirectory if it isn't already there
if not os.path.exists(today): #t oday is a string defined earlier.
    os.mkdir(today) # make directory

==
(findnumpy)
NumPy

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/python-numpy-tutorial?utm_source=adwords_ppc&utm_campaignid=1565261270&utm_adgroupid=67750485268&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=1t1&utm_creative=332661264371&utm_targetid=aud-299261629574:dsa-473406585115&utm_loc_interest_ms=&utm_loc_physical_ms=1016367&gclid=Cj0KCQiAn8nuBRCzARIsAJcdIfNaldds0HRvuMY0tIWi-Cx2HTJNxH6C9Isjr1criuRR-7s0TIGKfDIaAuOcEALw_wcB

Python Numpy Array Tutorial

A NumPy tutorial for beginners in which you'll learn how to create a NumPy array, use broadcasting, access values, manipulate arrays, and much more.

NumPy is, just like SciPy, Scikit-Learn, Pandas, etc. one of the packages that you just can’t miss when you’re learning data science, mainly because this library provides you with an array data structure that holds some benefits over Python lists, such as: being more compact, faster access in reading and writing items, being more convenient and more efficient.

Today’s post will focus precisely on this. This NumPy tutorial will not only show you what NumPy arrays actually are and how you can install Python, but you’ll also learn how to make arrays (even when your data comes from files!), how broadcasting works, how you can ask for help, how to manipulate your arrays and how to visualize them.

Content
What Is A Python Numpy Array?
How To Install Numpy
How To Make NumPy Arrays
How NumPy Broadcasting Works
How Do Array Mathematics Work?
How To Subset, Slice, And Index Arrays
How To Ask For Help
How To Manipulate Arrays
How To Visualize NumPy Arrays
Beyond Data Analysis with NumPy

What Is A Python Numpy Array?

You already read in the introduction that NumPy arrays are a bit like Python lists, but still very much different at the same time. For those of you who are new to the topic, let’s clarify what it exactly is and what it’s good for.

As the name gives away, a NumPy array is a central data structure of the numpy library. The library’s name is short for “Numeric Python” or “Numerical Python”.

This already gives an idea of what you’re dealing with, right?

In other words, NumPy is a Python library that is the core library for scientific computing in Python. It contains a collection of tools and techniques that can be used to solve on a computer mathematical models of problems in Science and Engineering. One of these tools is a high-performance multidimensional array object that is a powerful data structure for efficient computation of arrays and matrices. To work with these arrays, there’s a vast amount of high-level mathematical functions operate on these matrices and arrays.

Then, what is an array?

When you look at the print of a couple of arrays, you could see it as a grid that contains values of the same type:

# Print the array
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 2d array
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print the 3d array
print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

You see that, in the example above, the data are integers. The array holds and represents any regular data in a structured way.

However, you should know that, on a structural level, an array is basically nothing but pointers. It’s a combination of a memory address, a data type, a shape, and strides:

- The data pointer indicates the memory address of the first byte in the array,

- The data type or dtype pointer describes the kind of elements that are contained within the array,

- The shape indicates the shape of the array, and

- The strides are the number of bytes that should be skipped in memory to go to the next element. If your strides are (10,1), you need to proceed one byte to get to the next column and 10 bytes to locate the next row.

Or, in other words, an array contains information about the raw data, how to locate an element and how to interpret an element.

Enough of the theory. Let’s check this out ourselves:

You can easily test this by exploring the numpy array attributes:

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Print out memory address
print(my_2d_array.data)

<memory at 0x7f58cb57fa68>

# Print out the shape of `my_array`
print(my_2d_array.shape)

(2, 4)

# Print out the data type of `my_array`
print(my_2d_array.dtype)

int64

# Print out the stride of `my_array`
print(my_2d_array.strides)

(32, 8)

You see that now, you get a lot more information: for example, the data type that is printed out is ‘int64’ or signed 32-bit integer type; This is a lot more detailed! That also means that the array is stored in memory as 64 bytes (as each integer takes up 8 bytes and you have an array of 8 integers). The strides of the array tell us that you have to skip 8 bytes (one value) to move to the next column, but 32 bytes (4 values) to get to the same position in the next row. As such, the strides for the array will be (32,8).

Note that if you set the data type to int32, the strides tuple that you get back will be (16, 4), as you will still need to move one value to the next column and 4 values to get the same position. The only thing that will have changed is the fact that each integer will take up 4 bytes instead of 8.

(A picture)

The array that you see above is, as its name already suggested, a 2-dimensional array: you have rows and columns. The rows are indicated as the “axis 0”, while the columns are the “axis 1”. The number of the axis goes up accordingly with the number of the dimensions: in 3-D arrays, of which you have also seen an example in the previous code chunk, you’ll have an additional “axis 2”. Note that these axes are only valid for arrays that have at least 2 dimensions, as there is no point in having this for 1-D arrays;

These axes will come in handy later when you’re manipulating the shape of your NumPy arrays.

How To Install Numpy

Before you can start to try out these NumPy arrays for yourself, you first have to make sure that you have it installed locally (assuming that you’re working on your pc). If you have the Python library already available, go ahead and skip this section :)

If you still need to set up your environment, you must be aware that there are two major ways of installing NumPy on your pc: with the help of Python wheels or the Anaconda Python distribution.

… With Python Wheels

Make sure firstly that you have Python installed. You can go here if you still need to do this :)

If you’re working on Windows, make sure that you have added Python to the PATH environment variable. Then, don’t forget to install a package manager, such as pip, which will ensure that you’re able to use Python’s open-source libraries.

Note that recent versions of Python 3 come with pip, so double check if you have it and if you do, upgrade it before you install NumPy:

pip install pip --upgrade
pip --version

Next, you can go here or here to get your NumPy wheel. After you have downloaded it, navigate to the folder on your pc that stores it through the terminal and install it:

install "numpy-1.9.2rc1+mkl-cp34-none-win_amd64.whl"
import numpy
numpy.__version__

The two last lines allow you to verify that you have installed NumPy and check the version of the package.

After these steps, you’re ready to start using NumPy!

… With The Anaconda Python Distribution

To get NumPy, you could also download the Anaconda Python distribution. This is easy and will allow you to get started quickly! If you haven’t downloaded it already, go here to get it. Follow the instructions to install, and you're ready to start!

Do you wonder why this might actually be easier?

The good thing about getting this Python distribution is the fact that you don’t need to worry too much about separately installing NumPy or any of the major packages that you’ll be using for your data analyses, such as pandas, scikit-learn, etc.

Because, especially if you’re very new to Python, programming or terminals, it can really come as a relief that Anaconda already includes 100 of the most popular Python, R and Scala packages for data science. But also for more seasoned data scientists, Anaconda is the way to go if you want to get started quickly on tackling data science problems.

What’s more, Anaconda also includes several open source development environments such as Jupyter and Spyder. If you’d like to start working with Jupyter Notebook after this tutorial, go to this page.

In short, consider downloading Anaconda to get started on working with numpy and other packages that are relevant to data science!

How To Make NumPy Arrays

So, now that you have set up your environment, it’s time for the real work. Admittedly, you have already tried out some stuff with arrays in the above DataCamp Light chunks. However, you haven’t really gotten any real hands-on practice with them, because you first needed to install NumPy on your own pc. Now that you have done this, it’s time to see what you need to do in order to run the above code chunks on your own.

Some exercises have been included below so that you can already practice how it’s done before you start on your own!

To make a numpy array, you can just use the np.array() function (see example below, tao). All you need to do is pass a list to it, and optionally, you can also specify the data type of the data. If you want to know more about the possible data types that you can pick, go here or consider taking a brief look at DataCamp’s NumPy cheat sheet.

There’s no need to go and memorize these NumPy data types if you’re a new user; But you do have to know and care what data you’re dealing with. The data types are there when you need more control over how your data is stored in memory and on disk. Especially in cases where you’re working with extensive data, it’s good that you know to control the storage type.

Don’t forget that, in order to work with the np.array() function, you need to make sure that the numpy library is present in your environment. The NumPy library follows an import convention: when you import this library, you have to make sure that you import it as np. By doing this, you’ll make sure that other Pythonistas understand your code more easily.

In the following example you’ll create the my_array array that you have already played around with above:

# Import `numpy` as `np`
import numpy as np

(findcreatendarraynumpy)
(findcreatenumpyarraynumpy)
# Make the array `my_array`
# Tao: the parameter dtype is optinal 
my_array = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.int64)

# Print `my_array`
print(my_array)

[[1 2 3 4]
 [5 6 7 8]]

If you would like to know more about how to make lists, go here.

However, sometimes you don’t know what data you want to put in your array, or you want to import data into a numpy array from another source. In those cases, you’ll make use of initial placeholders or functions to load data from text into arrays, respectively.

The following sections will show you how to do this.

How To Make An “Empty” NumPy Array

What people often mean when they say that they are creating “empty” arrays is that they want to make use of initial placeholders, which you can fill up afterward. You can initialize arrays with ones or zeros, but you can also create arrays that get filled up with evenly spaced values, constant or random values.

However, you can still make a totally empty array, too.

Luckily for us, there are quite a lot of functions to make

Try it all out below!

(findonesnumpy)
# Create an array of ones
print(np.ones((3,4)))

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

(findzerosnumpy)
# Create an array of zeros
print(np.zeros((2,3,4),dtype=np.int16))

[[[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]

 [[0 0 0 0]
  [0 0 0 0]
  [0 0 0 0]]]

(findrandomnumpy)
(findrandnumpy)
# Create an array with random values
print(np.random.random((2,2)))

[[ 0.4809319   0.39211752]
 [ 0.34317802  0.72904971]]

(findemptynumpy)
(findemptyarraynumpy)
# Create an empty array
print(np.empty((3,2)))

[[ 0.  0.]
 [ 0.  0.]
 [ 0.  0.]]

(findsamevaluesnumpy)
# Create a full array
print(np.full((2,2),7))

[[ 7.  7.]
 [ 7.  7.]]

(findrangenumpy)
(findarangenumpy)
# Create an array of evenly-spaced values (with step, tao)
print(np.arange(10,25,5))

[10 15 20]

(findlinspacenumpy)
# Create an array of evenly-spaced values (with number of values, tao)
print(np.linspace(0,2,9))

[ 0.    0.25  0.5   0.75  1.    1.25  1.5   1.75  2.  ]

Tip: play around with the above functions so that you understand how they work!

- For some, such as np.ones(), np.random.random(), np.empty(), np.full() or np.zeros() the only thing that you need to do in order to make arrays with ones or zeros is pass the shape of the array that you want to make. As an option to np.ones() and np.zeros(), you can also specify the data type. In the case of np.full(), you also have to specify the constant value that you want to insert into the array.

- With np.linspace() and np.arange() you can make arrays of evenly spaced values. The difference between these two functions is that the last value of the three that are passed in the code chunk above designates either the step value for np.linspace() or a number of samples for np.arange() (tao: swapped). What happens in the first is that you want, for example, an array of 9 values that lie between 0 and 2. For the latter, you specify that you want an array to start at 10 and per steps of 5, generate values for the array that you’re creating.

(findeyenumpy)
(findidentitynumpy)
Remember that NumPy also allows you to create an identity array or matrix with np.eye() and np.identity(). An identity matrix is a square matrix of which all elements in the principal diagonal are ones, and all other elements are zeros. When you multiply a matrix with an identity matrix, the given matrix is left unchanged.

In other words, if you multiply a matrix by an identity matrix, the resulting product will be the same matrix again by the standard conventions of matrix multiplication.

Even though the focus of this tutorial is not on demonstrating how identity matrices work, it suffices to say that identity matrices are useful when you’re starting to do matrix calculations: they can simplify mathematical equations, which makes your computations more efficient and robust.

How To Load NumPy Arrays From Text

Creating arrays with the help of initial placeholders or with some example data is an excellent way of getting started with numpy. But when you want to get started with data analysis, you’ll need to load data from text files.

With that what you have seen up until now, you won’t really be able to do much. Make use of some specific functions to load data from your files, such as loadtxt() or genfromtxt().

Let’s say you have the following text files with data:

# This is your data in the text file
# Value1  Value2  Value3
# 0.2536  0.1008  0.3857
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  0.5929
# 0.1781  0.3049  0.8928
# 0.6253  0.3486  0.8791

(findloaddatanumpy)
(findloadnumpy)
(findreaddatanumpy)
(findimportdatanumpy)

# Import your data
x, y, z = np.loadtxt('data.txt', skiprows=1, unpack=True)

In the code above, you use loadtxt() to load the data in your environment. You see that the first argument that both functions take is the text file data.txt. Next, there are some specific arguments for each: in the first statement, you skip the first row, and you return the columns as separate arrays with unpack=TRUE. This means that the values in column Value1 will be put in x, and so on.

Note that, in case you have comma-delimited data or if you want to specify the data type, there are also the arguments delimiter and dtype that you can add to the loadtxt() arguments.

That’s easy and straightforward, right?

Let’s take a look at your second file with data:

# Your data in the text file
# Value1  Value2  Value3
# 0.4839  0.4536  0.3561
# 0.1292  0.6875  MISSING
# 0.1781  0.3049  0.8928
# MISSING 0.5801  0.2038
# 0.5993  0.4357  0.7410

my_array2 = np.genfromtxt('data2.txt', skip_header=1, filling_values=-999)

You see that here, you resort to genfromtxt() to load the data. In this case, you have to handle some missing values that are indicated by the 'MISSING' strings. Since the genfromtxt() function converts character strings in numeric columns to nan, you can convert these values to other ones by specifying the filling_values argument. In this case, you choose to set the value of these missing values to -999.

If by any chance, you have values that don’t get converted to nan by genfromtxt(), there’s always the missing_values argument that allows you to specify what the missing values of your data exactly are.

But this is not all.

Tip: check out this page to see what other arguments you can add to import your data successfully.

You now might wonder what the difference between these two functions really is.

The examples indicated this maybe implicitly, but, in general, genfromtxt() gives you a little bit more flexibility; It’s more robust than loadtxt().

Let’s make this difference a little bit more practical: the latter, loadtxt(), only works when each row in the text file has the same number of values; So when you want to handle missing values easily, you’ll typically find it easier to use genfromtxt().

But this is definitely not the only reason.

A brief look on the number of arguments that genfromtxt() has to offer will teach you that there is really a lot more things that you can specify in your import, such as the maximum number of rows to read or the option to automatically strip white spaces from variables.


(findsavenumpy)
(findsavedatanumpy)
(findsavearraynumpy)
How To Save NumPy Arrays

Once you have done everything that you need to do with your arrays, you can also save them to a file. If you want to save the array to a text file, you can use the savetxt() function to do this:

import numpy as np
x = np.arange(0.0,5.0,1.0)
np.savetxt('test.out', x, delimiter=',')

Remember that np.arange() creates a NumPy array of evenly-spaced values. The third value that you pass to this function is the step value.

There are, of course, other ways to save your NumPy arrays to text files. Check out the functions in the table below if you want to get your data to binary files or archives:

save(): Save an array to a binary file in NumPy .npy format

savez(): Save several arrays into an uncompressed .npz archive

savez_compressed(): Save several arrays into a compressed .npz archive

For more information or examples of how you can use the above functions to save your data, go here or make use of one of the help functions that NumPy has to offer to get to know more instantly!

Are you not sure what these NumPy help functions are?

No worries! You’ll learn more about them in one of the next sections!

How To Inspect Your NumPy Arrays

Besides the array attributes that have been mentioned above, namely, data, shape, dtype and strides, there are some more that you can use to easily get to know more about your arrays. The ones that you might find interesting to use when you’re just starting out are the following:

# Print the number of `my_array`'s dimensions
print(my_array.ndim)

2

(findsizepandasnumpy)
# Print the number of `my_array`'s elements
print(my_array.size)

8

# Print information about `my_array`'s memory layout
print(my_array.flags)

C_CONTIGUOUS : True
F_CONTIGUOUS : False
OWNDATA : True
WRITEABLE : True
ALIGNED : True
UPDATEIFCOPY : False

# Print the length of one array element in bytes
print(my_array.itemsize)

8

# Print the total consumed bytes by `my_array`'s elements
print(my_array.nbytes)

64

These are almost all the attributes that an array can have.

Don’t worry if you don’t feel that all of them are useful for you at this point; This is fairly normal, because, just like you read in the previous section, you’ll only get to worry about memory when you’re working with large data sets.

Also note that, besides the attributes, you also have some other ways of gaining more information on and even tweaking your array slightly:

# Print the length of `my_array`
print(len(my_array))

2

# Change the data type of `my_array`
print(my_array.astype(float))

[[ 1.  2.  3.  4.]
 [ 5.  6.  7.  8.]]

Now that you have made your array, either by making one yourself with the np.array() or one of the initial placeholder functions, or by loading in your data through the loadtxt() or genfromtxt() functions, it’s time to look more closely into the second key element that really defines the NumPy library: scientific computing.

How NumPy Broadcasting Works

Before you go deeper into scientific computing, it might be a good idea to first go over what broadcasting exactly is: it’s a mechanism that allows NumPy to work with arrays of different shapes when you’re performing arithmetic operations.

To put it in a more practical context, you often have an array that’s somewhat larger and another one that’s slightly smaller. Ideally, you want to use the smaller array multiple times to perform an operation (such as a sum, multiplication, etc.) on the larger array.

To do this, you use the broadcasting mechanism.

However, there are some rules if you want to use it. And, before you already sigh, you’ll see that these “rules” are very simple and kind of straightforward!

- First off, to make sure that the broadcasting is successful, the dimensions of your arrays need to be compatible. Two dimensions are compatible when they are equal. Consider the following example:

# Initialize `x`
x = np.ones((3,4))

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.random.random((3,4))

print(y)

[[ 0.69646919  0.28613933  0.22685145  0.55131477]
 [ 0.71946897  0.42310646  0.9807642   0.68482974]
 [ 0.4809319   0.39211752  0.34317802  0.72904971]]

# Check shape of `y`
print(y.shape)

(3, 4)

# Add `x` and `y`
print(x + y)

[[ 1.69646919  1.28613933  1.22685145  1.55131477]
 [ 1.71946897  1.42310646  1.9807642   1.68482974]
 [ 1.4809319   1.39211752  1.34317802  1.72904971]]

- Two dimensions are also compatible when one of them is 1:

# Import `numpy` as `np`
import numpy as np

# Initialize `x`
x = np.ones((3,4))

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Check shape of `x`
print(x.shape)

(3, 4)

# Initialize `y`
y = np.arange(4)

print(y)

[0 1 2 3]

# Check shape of `y`
print(y.shape)

(4,)

# Subtract `x` and `y`
print(x - y)

[[ 1.  0. -1. -2.]
 [ 1.  0. -1. -2.]
 [ 1.  0. -1. -2.]]

Note that if the dimensions are not compatible, you will get a ValueError.

Tip: also test what the size of the resulting array is after you have done the computations! You’ll see that the size is actually the maximum size along each dimension of the input arrays.

In other words, you see that the result of x-y gives an array with shape (3,4): y had a shape of (4,) and x had a shape of (3,4). The maximum size along each dimension of x and y is taken to make up the shape of the new, resulting array.

- Lastly, the arrays can only be broadcast together if they are compatible in all dimensions. Consider the following example:

# Import `numpy` as `np`
import numpy as np

# Initialize `x` and `y`
x = np.ones((3,4))
print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

y = np.random.random((5,1,4))
print(y)

[[[ 0.63440096  0.84943179  0.72445532  0.61102351]]

 [[ 0.72244338  0.32295891  0.36178866  0.22826323]]

 [[ 0.29371405  0.63097612  0.09210494  0.43370117]]

 [[ 0.43086276  0.4936851   0.42583029  0.31226122]]

 [[ 0.42635131  0.89338916  0.94416002  0.50183668]]]

# Add `x` and `y`
print(x + y)

[[[ 1.63440096  1.84943179  1.72445532  1.61102351]
  [ 1.63440096  1.84943179  1.72445532  1.61102351]
  [ 1.63440096  1.84943179  1.72445532  1.61102351]]

 [[ 1.72244338  1.32295891  1.36178866  1.22826323]
  [ 1.72244338  1.32295891  1.36178866  1.22826323]
  [ 1.72244338  1.32295891  1.36178866  1.22826323]]

 [[ 1.29371405  1.63097612  1.09210494  1.43370117]
  [ 1.29371405  1.63097612  1.09210494  1.43370117]
  [ 1.29371405  1.63097612  1.09210494  1.43370117]]

 [[ 1.43086276  1.4936851   1.42583029  1.31226122]
  [ 1.43086276  1.4936851   1.42583029  1.31226122]
  [ 1.43086276  1.4936851   1.42583029  1.31226122]]

 [[ 1.42635131  1.89338916  1.94416002  1.50183668]
  [ 1.42635131  1.89338916  1.94416002  1.50183668]
  [ 1.42635131  1.89338916  1.94416002  1.50183668]]]

You see that, even though x and y seem to have somewhat different dimensions, the two can be added together.

That is because they are compatible in all dimensions:

- Array x has dimensions 3 X 4,
- Array y has dimensions 5 X 1 X 4

Since you have seen above that dimensions are also compatible if one of them is equal to 1, you see that these two arrays are indeed a good candidate for broadcasting!

What you will notice is that in the dimension where y has size 1, and the other array has a size greater than 1 (that is, 3), the first array behaves as if it were copied along that dimension.

Note that the shape of the resulting array will again be the maximum size along each dimension of x and y: the dimension of the result will be (5,3,4)

In short, if you want to make use of broadcasting, you will rely a lot on the shape and dimensions of the arrays with which you’re working.

But what if the dimensions are not compatible?

What if they are not equal or if one of them is not equal to 1?

You’ll have to fix this by manipulating your array! You’ll see how to do this in one of the next sections.

How Do Array Mathematics Work?

You’ve seen that broadcasting is handy when you’re doing arithmetic operations. In this section, you’ll discover some of the functions that you can use to do mathematics with arrays.

(findmathsnumpy)
(findmathnumpy)
As such, it probably won’t surprise you that you can just use +, -, *, / or % to add, subtract, multiply, divide or calculate the remainder of two (or more) arrays. However, a big part of why NumPy is so handy, is because it also has functions to do this. The equivalent functions of the operations that you have seen just now are, respectively, np.add(), np.subtract(), np.multiply(), np.divide() and np.remainder().

You can also easily do exponentiation and taking the square root of your arrays with np.exp() and np.sqrt(), or calculate the sines or cosines of your array with np.sin() and np.cos(). Lastly, its’ also useful to mention that there’s also a way for you to calculate the natural logarithm with np.log() or calculate the dot product by applying the dot() to your array.

Try it all out in the DataCamp Light chunk below.

Just a tip: make sure to check out first the arrays that have been loaded for this exercise!

print(x)

[[1 2 3]
 [3 4 5]]

print(y)

[6 7 8]

# Add `x` and `y`
print(np.add(x,y))

[[ 7  9 11]
 [ 9 11 13]]

# Subtract `x` and `y`
print(np.subtract(x,y))

[[-5 -5 -5]
 [-3 -3 -3]]

# Multiply `x` and `y`
print(np.multiply(x,y))

[[ 6 14 24]
 [18 28 40]]

# Divide `x` and `y`
print(np.divide(x,y))

[[ 0.16666667  0.28571429  0.375     ]
 [ 0.5         0.57142857  0.625     ]]

# Calculate the remainder of `x` and `y`
print(np.remainder(x,y))

[[1 2 3]
 [3 4 5]]

Remember how broadcasting works? Check out the dimensions and the shapes of both x and y in your IPython shell. Are the rules of broadcasting respected?

But there is more.

Check out this small list of aggregate functions:

a.sum(): Array-wise sum
a.min(): Array-wise minimum value
b.max(axis=0): Maximum value of an array row
b.cumsum(axis=1): Cumulative sum of the elements
a.mean(): Mean
b.median(): Median
a.corrcoef(): Correlation coefficient
np.std(b): Standard deviation

Besides all of these functions, you might also find it useful to know that there are mechanisms that allow you to compare array elements. For example, if you want to check whether the elements of two arrays are the same, you might use the == operator. To check whether the array elements are smaller or bigger, you use the < or > operators.

This all seems quite straightforward, yes?

However, you can also compare entire arrays with each other! In this case, you use the np.array_equal() function. Just pass in the two arrays that you want to compare with each other, and you’re done.

Note that, besides comparing, you can also perform logical operations on your arrays. You can start with np.logical_or(), np.logical_not() and np.logical_and(). This basically works like your typical OR, NOT and AND logical operations;

In the simplest example, you use OR to see whether your elements are the same (for example, 1), or if one of the two array elements is 1. If both of them are 0, you’ll return FALSE. You would use AND to see whether your second element is also 1 and NOT to see if the second element differs from 1.

Test this out in the code chunk below:

print(a)

[ True  True False False]

print(b)

[ True False  True False]

# `a` AND `b` 
print(np.logical_and(a, b))

[ True False False False]

# `a` OR `b`
print(np.logical_or(a, b))

[ True  True  True False]

# `a` NOT `b`
print(np.logical_not(a,b))

[False False  True  True]

How To Subset, Slice, And Index Arrays

Besides mathematical operations, you might also consider taking just a part of the original array (or the resulting array) or just some array elements to use in further analysis or other operations. In such case, you will need to subset, slice and/or index your arrays.

These operations are very similar to when you perform them on Python lists. If you want to check out the similarities for yourself, or if you want a more elaborate explanation, you might consider checking out DataCamp’s Python list tutorial.

If you have no clue at all on how these operations work, it suffices for now to know these two basic things:

- You use square brackets [] as the index operator, and

- Generally, you pass integers to these square brackets, but you can also put a colon : or a combination of the colon with integers in it to designate the elements/rows/columns you want to select.

Besides from these two points, the easiest way to see how this all fits together is by looking at some examples of subsetting:

(findindexingnumpy)
(findindexnumpy)

print(my_array)

[1 2 3 4]

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Select the element at the 1st index
print(my_array[1])

2

# Select the element at row 1 column 2
print(my_2d_array[1][2])

7

# Select the element at row 1 column 2
print(my_2d_array[1,2])

7

print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

# Select the element at row 1, column 2 and
print(my_3d_array[1,1,2])

11

(findslicepandasnumpy)
Something a little bit more advanced than subsetting, if you will, is slicing. Here, you consider not just particular values of your arrays, but you go to the level of rows and columns. You’re basically working with “regions” of data instead of pure “locations”.

You can see what is meant with this analogy in these code examples:

# Select items at index 0 and 1
print(my_array[0:2])

[1 2]

# Select items at row 0 and 1, column 1
print(my_2d_array[0:2,1])

[2 6]

# Select items at row 1
# This is the same as saying `my_3d_array[1,:,:]
print(my_3d_array[1,...])

[[ 1  2  3  4]
 [ 9 10 11 12]]

You’ll see that, in essence, the following holds:

a[start:end] # items start through the end (but the end is not included!)
a[start:]    # items start through the rest of the array
a[:end]      # items from the beginning through the end (but the end is not included!)

Lastly, there’s also indexing. When it comes to NumPy, there are boolean indexing and advanced or “fancy” indexing.

(In case you’re wondering, this is true NumPy jargon, I didn’t make the last one up!)

First up is boolean indexing. Here, instead of selecting elements, rows or columns based on index number, you select those values from your array that fulfill a certain condition.

Putting this into code can be pretty easy:

print(my_array)

[1 2 3 4]

# Try out a simple example
print(my_array[my_array<2])

[1]

print(my_3d_array)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

# Specify a condition
bigger_than_3 = (my_3d_array >= 3)

# Use the condition to index our 3d array
print(my_3d_array[bigger_than_3])

[ 3  4  5  6  7  8  3  4  9 10 11 12]

Note that, to specify a condition, you can also make use of the logical operators | (OR) and & (AND). If you would want to rewrite the condition above in such a way (which would be inefficient, but I demonstrate it here for educational purposes :)), you would get bigger_than_3 = (my_3d_array > 3) | (my_3d_array == 3).

With the arrays that have been loaded in, there aren’t too many possibilities, but with arrays that contain for example, names or capitals, the possibilities could be endless!

When it comes to fancy indexing, that what you basically do with it is the following: you pass a list or an array of integers to specify the order of the subset of rows you want to select out of the original array.

Does this sound a little bit abstract to you?

No worries, just try it out in the code chunk below:

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Select elements at (1,0), (0,1), (1,2) and (0,0)
print(my_2d_array[[1, 0, 1, 0],[0, 1, 2, 0]])

[5 2 7 1]

# Select a subset of the rows and columns
print(my_2d_array[[1, 0, 1, 0]][:,[0,1,2,0]])

[[5 6 7 5]
 [1 2 3 1]
 [5 6 7 5]
 [1 2 3 1]]

Now, the second statement might seem to make less sense to you at first sight. This is normal. It might make more sense if you break it down:

- If you just execute my_2d_array[[1,0,1,0]], the result is the following:

array([[5, 6, 7, 8],
   [1, 2, 3, 4],
   [5, 6, 7, 8],
   [1, 2, 3, 4]])

- What the second part, namely, [:,[0,1,2,0]], is tell you that you want to keep all the rows of this result, but that you want to change the order of the columns around a bit. You want to display the columns 0, 1, and 2 as they are right now, but you want to repeat column 0 as the last column instead of displaying column number 3. This will give you the following result:

array([[5, 6, 7, 5],
   [1, 2, 3, 1],
   [5, 6, 7, 5],
   [1, 2, 3, 1]])

Advanced indexing clearly holds no secrets for you any more!

How To Ask For Help

As a short intermezzo, you should know that you can always ask for more information about the modules, functions or classes that you’re working with, especially because NumPy can be quite something when you first get started on working with it.

Asking for help is fairly easy.

You just make use of the specific help functions that numpy offers to set you on your way:

- Use lookfor() to do a keyword search on docstrings. This is specifically handy if you’re just starting out, as the ‘theory’ behind it all might fade in your memory. The one downside is that you have to go through all of the search results if your query is not that specific, as is the case in the code example below. This might make it even less overviewable for you.

- Use info() for quick explanations and code examples of functions, classes, or modules. If you’re a person that learns by doing, this is the way to go! The only downside about using this function is probably that you need to be aware of the module in which certain attributes or functions are in. If you don’t know immediately what is meant by that, check out the code example below.

You see, both functions have their advantages and disadvantages, but you’ll see for yourself why both of them can be useful: try them out for yourself in the DataCamp Light code chunk below!

# Look up info on `mean` with `np.lookfor()`
print(np.lookfor("mean"))

Search results for 'mean'
-------------------------
numpy.mean
    Compute the arithmetic mean along the specified axis.
numpy.nanmean
    Compute the arithmetic mean along the specified axis, ignoring NaNs.
numpy.ma.mean
    Returns the average of the array elements along given axis.
numpy.matrix.mean
    Returns the average of the matrix elements along the given axis.
numpy.array_equiv
    Returns True if input arrays are shape consistent and all elements equal.
numpy.ma.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.chararray.mean
    Returns the average of the array elements along given axis.
numpy.ma.fix_invalid
    Return input with invalid data masked and replaced by a fill value.
numpy.ma.MaskedArray.anom
    Compute the anomalies (deviations from the arithmetic mean)
numpy.polynomial.polyutils.trimcoef
    Remove "small" "trailing" coefficients from a polynomial.
numpy.exp
    Calculate the exponential of all elements in the input array.
numpy.pad
    Pads an array.
numpy.put
    Replaces specified elements of an array with given values.
numpy.std
    Compute the standard deviation along the specified axis.
numpy.sum
    Sum of array elements over a given axis.
numpy.var
    Compute the variance along the specified axis.
numpy.copy
    Return an array copy of the given object.
numpy.prod
    Return the product of array elements over a given axis.
numpy.take
    Take elements from an array along an axis.
numpy.isnan
    Test element-wise for NaN and return result as a boolean array.
numpy.ravel
    Return a contiguous flattened array.
numpy.copyto
    Copies values from one array to another, broadcasting as necessary.
numpy.einsum
    Evaluates the Einstein summation convention on the operands.
numpy.kaiser
    Return the Kaiser window.
numpy.median
    Compute the median along the specified axis.
numpy.nanmax
    Return the maximum of an array or maximum along an axis, ignoring any
numpy.nanmin
    Return minimum of an array or minimum along an axis, ignoring any NaNs.
numpy.nanstd
    Compute the standard deviation along the specified axis, while
numpy.nansum
    Return the sum of array elements over a given axis treating Not a
numpy.nanvar
    Compute the variance along the specified axis, while ignoring NaNs.
numpy.nditer
    Efficient multi-dimensional iterator object to iterate over arrays.
numpy.average
    Compute the weighted average along the specified axis.
numpy.hamming
    Return the Hamming window.
numpy.hanning
    Return the Hanning window.
numpy.polyfit
    Least squares polynomial fit.
numpy.reshape
    Gives a new shape to an array without changing its data.
numpy.bartlett
    Return the Bartlett window.
numpy.blackman
    Return the Blackman window.
numpy.can_cast
    Returns True if cast between data types can occur according to the
numpy.digitize
    Return the indices of the bins to which each value in input array belongs.
numpy.fromfile
    Construct an array from data in a text or binary file.
numpy.fromiter
    Create a new 1-dimensional array from an iterable object.
numpy.isfinite
    Test element-wise for finiteness (not infinity or not Not a Number).
numpy.full_like
    Return a full array with the same shape and type as a given array.
numpy.histogram
    Compute the histogram of a set of data.
numpy.nanmedian
    Compute the median along the specified axis, while ignoring NaNs.
numpy.ones_like
    Return an array of ones with the same shape and type as a given array.
numpy.empty_like
    Return a new array with the same shape and type as a given array.
numpy.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.nan_to_num
    Replace nan with zero and inf with finite numbers.
numpy.percentile
    Compute the qth percentile of the data along the specified axis.
numpy.recfromcsv
    Load ASCII data stored in a comma-separated file.
numpy.recfromtxt
    Load ASCII data from a file and return it in a record array.
numpy.trim_zeros
    Trim the leading and/or trailing zeros from a 1-D array or sequence.
numpy.zeros_like
    Return an array of zeros with the same shape and type as a given array.
numpy.ma.dot
    Return the dot product of two arrays.
numpy.ma.exp
    Calculate the exponential of all elements in the input array.
numpy.ma.var
    Compute the variance along the specified axis.
numpy.ma.copy
    a.copy(order='C')
numpy.fft.fft2
    Compute the 2-dimensional discrete Fourier Transform
numpy.fft.fftn
    Compute the N-dimensional discrete Fourier Transform.
numpy.fft.rfft
    Compute the one-dimensional discrete Fourier Transform for real input.
numpy.busday_offset
    First adjusts the date to fall on a valid day according to
numpy.ma.ravel
    Returns a 1D version of self, as a view.
numpy.fft.ifft2
    Compute the 2-dimensional inverse discrete Fourier Transform.
numpy.fft.ifftn
    Compute the N-dimensional inverse discrete Fourier Transform.
numpy.fft.rfftn
    Compute the N-dimensional discrete Fourier Transform for real input.
numpy.nanpercentile
    Compute the qth percentile of the data along the specified axis,
numpy.ma.median
    Compute the median along the specified axis.
numpy.fft.irfftn
    Compute the inverse of the N-dimensional FFT of real input.
numpy.linalg.svd
    Singular Value Decomposition.
numpy.ma.polyfit
    Least squares polynomial fit.
numpy.bytes0.split
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.linalg.cond
    Compute the condition number of a matrix.
numpy.linalg.norm
    Matrix or vector norm.
numpy.may_share_memory
    Determine if two arrays might share memory
numpy.bytes0.decode
    Decode the bytes using the codec registered for encoding.
numpy.bytes0.rsplit
    Return a list of the sections in the bytes, using sep as the delimiter.
numpy.str0.encode
    Encode S using the codec registered for encoding. Default encoding
numpy.matlib.randn
    Return a random matrix with data from the "standard normal" distribution.
numpy.matrix.ravel
    Return a flattened matrix.
numpy.bytes0.replace
    Return a copy with all occurrences of substring old replaced by new.
numpy.chararray.copy
    Return a copy of the array.
numpy.chararray.sort
    Sort an array, in-place.
numpy.chararray.view
    New view of array with the same data.
numpy.ma.empty_like
    Return a new array with the same shape and type as a given array.
numpy.ma.frombuffer
    Interpret a buffer as a 1-dimensional array.
numpy.matrix.flatten
    Return a flattened copy of the matrix.
numpy.chararray.astype
    Copy of the array, cast to a specified type.
numpy.testing.Tester
    Nose test runner.
numpy.chararray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.chararray.tobytes
    Construct Python bytes containing the raw data bytes in the array.
numpy.matrix.partition
    Rearranges the elements in the array in such a way that value of the
numpy.chararray.tostring
    Construct Python bytes containing the raw data bytes in the array.
numpy.chararray.transpose
    a.transpose(*axes)
numpy.ma.MaskedArray.mean
    Returns the average of the array elements along given axis.
numpy.testing.noseclasses.NumpyDocTestCase._formatMessage
    Honour the longMessage attribute when generating failure messages.
numpy.ma.MaskedArray.dot
    Masked dot product of two arrays. Note that `out` and `strict` are
numpy.ma.MaskedArray.var
    Compute the variance along the specified axis.
numpy.ma.MaskedArray.copy
    Return a copy of the array.
numpy.ma.MaskedArray.view
    New view of array with the same data.
numpy.core.multiarray.scalar
    Return a new scalar array of the given type initialized with obj.
numpy.ma.MaskedArray.ravel
    Returns a 1D version of self, as a view.
numpy.ma.MaskedArray.filled
    Return a copy of self, with masked values filled with a given value.
numpy.lib.format.open_memmap
    Open a .npy file as a memory-mapped array.
numpy.lib.format.write_array
    Write an array to an NPY file, including a header.
numpy.ma.MaskedArray.flatten
    Return a copy of the array collapsed into one dimension.
numpy.polynomial.Hermite._fit
    Least squares fit of Hermite series to data.
numpy.ma.MaskedArray.transpose
    a.transpose(*axes)
numpy.polynomial.HermiteE._fit
    Least squares fit of Hermite series to data.
numpy.polynomial.Laguerre._fit
    Least squares fit of Laguerre series to data.
numpy.polynomial.Legendre._fit
    Least squares fit of Legendre series to data.
numpy.polynomial.Chebyshev._fit
    Least squares fit of Chebyshev series to data.
numpy.polynomial.Polynomial._fit
    Least-squares fit of a polynomial to data.
numpy.distutils.command.bdist_rpm.bdist_rpm.set_undefined_options
    Set the values of any "undefined" options from corresponding
numpy.lib.format._write_array_header
    Write the header for an array and returns the version used
numpy.distutils.numpy_distribution.NumpyDistribution._set_command_options
    Set the options for 'command_obj' from 'option_dict'.  Basically
numpy.testing.Tester._get_custom_doctester
    Return instantiated plugin for doctests
numpy.distutils.fcompiler.FCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.msvccompiler.MSVCCompiler.link
    Link a bunch of stuff together to create an executable or
numpy.distutils.intelccompiler.IntelCCompiler.link
    Link a bunch of stuff together to create an executable orNone

# Get info on data types with `np.info()`
print(np.info(np.ndarray.dtype))

None

np.info(np.ndarray.dtype)

(Did not print anything)

Note that you indeed need to know that dtype is an attribute of ndarray. Also, make sure that you don’t forget to put np in front of the modules, classes or terms you’re asking information about, otherwise you will get an error message like this:

Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'ndarray' is not defined

You now know how to ask for help, and that’s a good thing. The next topic that this NumPy tutorial covers is array manipulation.

Not that you can not overcome this topic on your own, quite the contrary!

But some of the functions might raise questions, because, what is the difference between resizing and reshaping?

And what is the difference between stacking your arrays horizontally and vertically?

The next section is all about answering these questions, but if you ever feel in doubt, feel free to use the help functions that you have just seen to quickly get up to speed.

How To Manipulate Arrays

Performing mathematical operations on your arrays is one of the things that you’ll be doing, but probably most importantly to make this and the broadcasting work is to know how to manipulate your arrays.

Below are some of the most common manipulations that you’ll be doing.

How To Transpose Your Arrays

What transposing your arrays actually does is permuting the dimensions of it. Or, in other words, you switch around the shape of the array. Let’s take a small example to show you the effect of transposition:

# Print `my_2d_array`
print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

(findtransposenumpy)
# Transpose `my_2d_array`
print(np.transpose(my_2d_array))

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

# Or use `T` to transpose `my_2d_array`
print(my_2d_array.T)

[[1 5]
 [2 6]
 [3 7]
 [4 8]]

Tip: if the visual comparison between the array and its transposed version is not entirely clear, inspect the shape of the two arrays to make sure that you understand why the dimensions are permuted.

Note that there are two transpose functions. Both do the same; There isn’t too much difference. You do have to take into account that T seems more of a convenience function and that you have a lot more flexibility with np.transpose(). That’s why it’s recommended to make use of this function if you want to more arguments.

All is well when you transpose arrays that are bigger than one dimension, but what happens when you just have a 1-D array? Will there be any effect, you think?

Try it out for yourself in the code chunk below. Your 1-D array has already been loaded in:

# Print `my_2d_array`
print(my_array)

[1 2 3 4]

# Transpose `my_2d_array`
print(np.transpose(my_array))

[1 2 3 4]

# Or use `T` to transpose `my_2d_array`
print(my_array.T)

[1 2 3 4]

You’re absolutely right! There is no effect when you transpose a 1-D array!

Reshaping Versus Resizing Your Arrays

You might have read in the broadcasting section that the dimensions of your arrays need to be compatible if you want them to be good candidates for arithmetic operations. But the question of what you should do when that is not the case, was not answered yet.

Well, this is where you get the answer!

What you can do if the arrays don’t have the same dimensions, is resize your array. You will then return a new array that has the shape that you passed to the np.resize() function. If you pass your original array together with the new dimensions, and if that new array is larger than the one that you originally had, the new array will be filled with copies of the original array that are repeated as many times as is needed.

(findresizenumpy)
However, if you just apply np.resize() to the array and you pass the new shape to it, the new array will be filled with zeros.

Let’s try this out with an example:

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Print the shape of `x`
print(x.shape)

(3, 4)

# Resize `x` to ((6,4))
print(np.resize(x, (6,4)))

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Try out this as well
print(x.resize((6,4)))

None

# Print out `x`
print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]
 [ 0.  0.  0.  0.]]

(findreshapepandasnumpy)
Besides resizing, you can also reshape your array. This means that you give a new shape to an array without changing its data. The key to reshaping is to make sure that the total size of the new array is unchanged. If you take the example of array x that was used above, which has a size of 3 X 4 or 12, you have to make sure that the new array also has a size of 12.

Psst… If you want to calculate the size of an array with code, make sure to use the size attribute: x.size or x.reshape((2,6)).size:

print(x)

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

# Print the size of `x` to see what's possible
print(x.size)

12

# Reshape `x` to (2,6)
print(x.reshape((2,6)))

[[ 1.  1.  1.  1.  1.  1.]
 [ 1.  1.  1.  1.  1.  1.]]

print(x) # tao: x did not change

[[ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]
 [ 1.  1.  1.  1.]]

(findflattennumpy)
(findravelnumpy)
# Flatten `x`
z = x.ravel()

# Print `z`
print(z)

[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]

If all else fails, you can also append an array to your original one or insert or delete array elements to make sure that your dimensions fit with the other array that you want to use for your computations.

Another operation that you might keep handy when you’re changing the shape of arrays is ravel(). This function allows you to flatten your arrays. This means that if you ever have 2D, 3D or n-D arrays, you can just use this function to flatten it all out to a 1-D array.

Pretty handy, isn’t it?

How To Append Arrays

When you append arrays to your original array, they are “glued” to the end of that original array. If you want to make sure that what you append does not come at the end of the array, you might consider inserting it. Go to the next section if you want to know more.

Appending is a pretty easy thing to do thanks to the NumPy library; You can just make use of the np.append().

Check how it’s done in the code chunk below. Don’t forget that you can always check which arrays are loaded in by typing, for example, my_array in the IPython shell and pressing ENTER.

(findappendnumpy)
print(my_array)

[1 2 3 4]

# Append a 1D array to your `my_array`
new_array = np.append(my_array, [7, 8, 9, 10])

# Print `new_array`
print(new_array)

[ 1  2  3  4  7  8  9 10]

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

# Append an extra column to your `my_2d_array`
new_2d_array = np.append(my_2d_array, [[7], [8]], axis=1)

# Print `new_2d_array`
print(new_2d_array)

[[1 2 3 4 7]
 [5 6 7 8 8]]

Note how, when you append an extra column to my_2d_array, the axis is specified. Remember that axis 1 indicates the columns, while axis 0 indicates the rows in 2-D arrays.

How To Insert And Delete Array Elements

Next to appending, you can also insert and delete array elements. As you might have guessed by now, the functions that will allow you to do these operations are np.insert() and np.delete():

(findinsertnumpy)
print(my_array)

[1 2 3 4]

# Insert `5` at index 1
print(np.insert(my_array, 1, 5))

[1 5 2 3 4]

print(my_array) # tao: my_array did not change

[1 2 3 4]

(findeletenumpy)
# Delete the value at index 1
print(np.delete(my_array,[1]))

[1 3 4]

print(my_array) # tao: my_array did not change

[1 2 3 4]

How To Join And Split Arrays

You can also ‘merge’ or join your arrays. There are a bunch of functions that you can use for that purpose and most of them are listed below.

Try them out, but also make sure to test out what the shape of the arrays is in the IPython shell. The arrays that have been loaded are x, my_array, my_resized_array and my_2d_array.

print(my_array)
[1 2 3 4]

print(my_2d_array)

[[1 2 3 4]
 [5 6 7 8]]

print(x)

[ 1.  1.  1.  1.]

(findconcatpandasenatenumpy)
(findconcatpandasnumpy)
# Concatentate `my_array` and `x`
print(np.concatenate((my_array,x)))

[ 1.  2.  3.  4.  1.  1.  1.  1.]

(findvstacknumpy)
# Stack arrays row-wise
print(np.vstack((my_array, my_2d_array)))

[[1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

print(my_resized_array)

[[1 2 3 4]
 [1 2 3 4]]

# Stack arrays row-wise
print(np.r_[my_resized_array, my_2d_array])

[[1 2 3 4]
 [1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

(findhstacknumpy)
# Stack arrays horizontally
print(np.hstack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.column_stack((my_resized_array, my_2d_array)))

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

# Stack arrays column-wise
print(np.c_[my_resized_array, my_2d_array])

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

You’ll note a few things as you go through the functions:

- The number of dimensions needs to be the same if you want to concatenate two arrays with np.concatenate(). As such, if you want to concatenate an array with my_array, which is 1-D, you’ll need to make sure that the second array that you have, is also 1-D.

- With np.vstack(), you effortlessly combine my_array with my_2d_array. You just have to make sure that, as you’re stacking the arrays row-wise, that the number of columns in both arrays is the same. As such, you could also add an array with shape (2,4) or (3,4) to my_2d_array, as long as the number of columns matches. Stated differently, the arrays must have the same shape along all but the first axis. The same holds also for when you want to use np.r[].

- For np.hstack(), you have to make sure that the number of dimensions is the same and that the number of rows in both arrays is the same. That means that you could stack arrays such as (2,3) or (2,4) to my_2d_array, which itself as a shape of (2,4). Anything is possible as long as you make sure that the number of rows matches. This function is still supported by NumPy, but you should prefer np.concatenate() or np.stack().

- With np.column_stack(), you have to make sure that the arrays that you input have the same first dimension. In this case, both shapes are the same, but if my_resized_array were to be (2,1) or (2,), the arrays still would have been stacked.

- np.c_[] is another way to concatenate. Here also, the first dimension of both arrays needs to match.

When you have joined arrays, you might also want to split them at some point. Just like you can stack them horizontally, you can also do the same but then vertically. You use np.hsplit() and np.vsplit(), respectively:

print(my_stacked_array)

[[1 2 3 4 1 2 3 4]
 [1 2 3 4 5 6 7 8]]

(findhsplitnumpy)
# Split `my_stacked_array` horizontally at the 2nd index
print(np.hsplit(my_stacked_array, 2))

[array([[1, 2, 3, 4],
       [1, 2, 3, 4]]), array([[1, 2, 3, 4],
       [5, 6, 7, 8]])]

(findvsplitnumpy)
# Split `my_stacked_array` vertically at the 2nd index
print(np.vsplit(my_stacked_array, 2))

[array([[1, 2, 3, 4, 1, 2, 3, 4]]), array([[1, 2, 3, 4, 5, 6, 7, 8]])]

What you need to keep in mind when you’re using both of these split functions is probably the shape of your array. Let’s take the above case as an example: my_stacked_array has a shape of (2,8). If you want to select the index at which you want the split to occur, you have to keep the shape in mind.

How To Visualize NumPy Arrays

Lastly, something that will definitely come in handy is to know how you can plot your arrays. This can especially be handy in data exploration, but also in later stages of the data science workflow, when you want to visualize your arrays.

(findhistogramnumpy)
With np.histogram()

Contrary to what the function might suggest, the np.histogram() function doesn’t draw the histogram but it does compute the occurrences of the array that fall within each bin; This will determine the area that each bar of your histogram takes up.

What you pass to the np.histogram() function then is first the input data or the array that you’re working with. The array will be flattened when the histogram is computed.

# Import `numpy` as `np`
import numpy as np

# Initialize your array
my_3d_array = np.array([[[1,2,3,4], [5,6,7,8]], [[1,2,3,4], [9,10,11,12]]], dtype=np.int64)

# Pass the array to `np.histogram()`
print(np.histogram(my_3d_array))

(array([4, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([  1. ,   2.1,   3.2,   4.3,   5.4,   6.5,   7.6,   8.7,   9.8,
        10.9,  12. ]))

# Specify the number of bins
print(np.histogram(my_3d_array, bins=range(0,13)))

(array([0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]))

You’ll see that as a result, the histogram will be computed: the first array lists the frequencies for all the elements of your array, while the second array lists the bins that would be used if you don’t specify any bins.

If you do specify a number of bins, the result of the computation will be different: the floats will be gone and you’ll see all integers for the bins.

There are still some other arguments that you can specify that can influence the histogram that is computed. You can find all of them here.

But what is the point of computing such a histogram if you can’t visualize it?

Visualization is a piece of cake with the help of Matplotlib, but you don’t need np.histogram() to compute the histogram. plt.hist() does this for itself when you pass it the (flattened) data and the bins:

(findplotpandasnumpy)
(findmatplotlibnumpy)
# Import numpy and matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Construct the histogram with a flattened 3d array and a range of bins
plt.hist(my_3d_array.ravel(), bins=range(0,13))

# Add a title to the plot
plt.title('Frequency of My 3D Array Elements')

# Show the plot
plt.show()

The above code will then give you the following (basic) histogram:

(A picture)

(findmeshgridnumpy)
Using np.meshgrid()

Another way to (indirectly) visualize your array is by using np.meshgrid(). The problem that you face with arrays is that you need 2-D arrays of x and y coordinate values. With the above function, you can create a rectangular grid out of an array of x values and an array of y values: the np.meshgrid() function takes two 1D arrays and produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays. Then, you can use these matrices to make all sorts of plots.

np.meshgrid() is particularly useful if you want to evaluate functions on a grid, as the code below demonstrates:

# Import NumPy and Matplotlib
import numpy as np
import matplotlib.pyplot as plt

# Create an array
points = np.arange(-5, 5, 0.01)

# Make a meshgrid
xs, ys = np.meshgrid(points, points)
z = np.sqrt(xs ** 2 + ys ** 2)

# Display the image on the axes
plt.imshow(z, cmap=plt.cm.gray)

# Draw a color bar
plt.colorbar()

# Show the plot
plt.show()
The code above gives the following result:

(A picture)

Beyond Data Analysis with NumPy

Congratulations, you have reached the end of the NumPy tutorial!

You have covered a lot of ground, so now you have to make sure to retain the knowledge that you have gained. Don’t forget to get your copy of DataCamp’s NumPy cheat sheet to support you in doing this!

After all this theory, it’s also time to get some more practice with the concepts and techniques that you have learned in this tutorial. One way to do this is to go back to the scikit-learn tutorial and start experimenting with further with the data arrays that are used to build machine learning models.

If this is not your cup of tea, check again whether you have downloaded Anaconda. Then, get started with NumPy arrays in Jupyter with this Definitive Guide to Jupyter Notebook. Also make sure to check out this Jupyter Notebook, which also guides you through data analysis in Python with NumPy and some other libraries in the interactive data science environment of the Jupyter Notebook.

Lastly, consider checking out DataCamp’s courses on data manipulation and visualization. Especially our latest courses in collaboration with Continuum Analytics will definitely interest you! Take a look at the Manipulating DataFrames with Pandas or the Pandas Foundations courses.

--
More notes about numpy:

From https://www.hackerearth.com/practice/machine-learning/data-manipulation-visualisation-r-python/tutorial-data-manipulation-numpy-pandas-python/tutorial/

Practical Tutorial on Data Manipulation with Numpy and Pandas in Python

TUTORIAL
Introduction

The pandas library has emerged into a power house of data manipulation tasks in python since it was developed in 2008. With its intuitive syntax and flexible data structure, it's easy to learn and enables faster data computation. The development of numpy and pandas libraries has extended python's multi-purpose nature to solve machine learning problems as well. The acceptance of python language in machine learning has been phenomenal since then.

This is just one more reason underlining the need for you to learn these libraries now. Published in early 2017, this blog claimed that python jobs outnumbered R jobs.

In this tutorial, we'll learn about using numpy and pandas libraries for data manipulation from scratch. Instead of going into theory, we'll take a practical approach.

First, we'll understand the syntax and commonly used functions of the respective libraries. Later, we'll work on a real-life data set.

Note: This tutorial is best suited for people who know the basics of python. No further knowledge is expected. Make sure you have python installed on your laptop.

Table of Contents

6 Important things you should know about Numpy and Pandas
Starting with Numpy
Starting with Pandas
Exploring an ML Data Set
Building a Random Forest Model
6 Important things you should know about Numpy and Pandas

The data manipulation capabilities of pandas are built on top of the numpy library. In a way, numpy is a dependency of the pandas library.
Pandas is best at handling tabular data sets comprising different variable types (integer, float, double, etc.). In addition, the pandas library can also be used to perform even the most naive of tasks such as loading data or doing feature engineering on time series data.
Numpy is most suitable for performing basic numerical computations such as mean, median, range, etc. Alongside, it also supports the creation of multi-dimensional arrays.
Numpy library can also be used to integrate C/C++ and Fortran code.
Remember, python is a zero indexing language unlike R where indexing starts at one.
The best part of learning pandas and numpy is the strong active community support you'll get from around the world.
Just to give you a flavor of the numpy library, we'll quickly go through its syntax structures and some important commands such as slicing, indexing, concatenation, etc. All these commands will come in handy when using pandas as well. Let's get started!

Starting with Numpy

#load the library and check its version, just to make sure we aren't using an older version
import numpy as np
np.__version__
'1.12.1'

#create a list comprising numbers from 0 to 9
L = list(range(10))

#converting integers to string - this style of handling lists is known as list comprehension.
#List comprehension offers a versatile way to handle list manipulations tasks easily. We'll learn about them in future tutorials. Here's an example.  

[str(c) for c in L]
['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']

[type(item) for item in L]
[int, int, int, int, int, int, int, int, int, int]

Creating Arrays

Numpy arrays are homogeneous in nature, i.e., they comprise one data type (integer, float, double, etc.) unlike lists.

#creating arrays
np.zeros(10, dtype='int')
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

#creating a 3 row x 5 column matrix
np.ones((3,5), dtype=float)
array([[ 1.,  1.,  1.,  1.,  1.],
      [ 1.,  1.,  1.,  1.,  1.],
      [ 1.,  1.,  1.,  1.,  1.]])

#creating a matrix with a predefined value
np.full((3,5),1.23)
array([[ 1.23,  1.23,  1.23,  1.23,  1.23],
      [ 1.23,  1.23,  1.23,  1.23,  1.23],
      [ 1.23,  1.23,  1.23,  1.23,  1.23]])

#create an array with a set sequence
np.arange(0, 20, 2)
array([0, 2, 4, 6, 8,10,12,14,16,18])

#create an array of even space between the given range of values
np.linspace(0, 1, 5)
array([ 0., 0.25, 0.5 , 0.75, 1.])

#create a 3x3 array with mean 0 and standard deviation 1 in a given dimension
np.random.normal(0, 1, (3,3))
array([[ 0.72432142, -0.90024075,  0.27363808],
      [ 0.88426129,  1.45096856, -1.03547109],
      [-0.42930994, -1.02284441, -1.59753603]])

#create an identity matrix
np.eye(3)
array([[ 1.,  0.,  0.],
      [ 0.,  1.,  0.],
      [ 0.,  0.,  1.]])

#set a random seed
np.random.seed(0)

x1 = np.random.randint(10, size=6) #one dimension
x2 = np.random.randint(10, size=(3,4)) #two dimension
x3 = np.random.randint(10, size=(3,4,5)) #three dimension

print("x3 ndim:", x3.ndim)
print("x3 shape:", x3.shape)
print("x3 size: ", x3.size)
('x3 ndim:', 3)
('x3 shape:', (3, 4, 5))
('x3 size: ', 60)

Array Indexing

The important thing to remember is that indexing in python starts at zero.

x1 = np.array([4, 3, 4, 4, 8, 4])
x1
array([4, 3, 4, 4, 8, 4])

#assess value to index zero
x1[0]
4

#assess fifth value
x1[4]
8

#get the last value
x1[-1]
4

#get the second last value
x1[-2]
8

#in a multidimensional array, we need to specify row and column index
x2
array([[3, 7, 5, 5],
      [0, 1, 5, 9],
      [3, 0, 5, 0]])

#1st row and 2nd column value
x2[2,3]
0

#3rd row and last value from the 3rd column
x2[2,-1]
0

#replace value at 0,0 index
x2[0,0] = 12
x2
array([[12,  7,  5,  5],
      [ 0,  1,  5,  9],
      [ 3,  0,  5,  0]])

Array Slicing

Now, we'll learn to access multiple or a range of elements from an array.

x = np.arange(10)
x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

#from start to 4th position
x[:5]
array([0, 1, 2, 3, 4])

#from 4th position to end
x[4:]
array([4, 5, 6, 7, 8, 9])

#from 4th to 6th position
x[4:7]
array([4, 5, 6])

#return elements at even place
x[ : : 2]
array([0, 2, 4, 6, 8])

#return elements from first position step by two
x[1::2]
array([1, 3, 5, 7, 9])

#reverse the array
x[::-1]
array([9, 8, 7, 6, 5, 4, 3, 2, 1, 0])

Array Concatenation

Many a time, we are required to combine different arrays. So, instead of typing each of their elements manually, you can use array concatenation to handle such tasks easily.

#You can concatenate two or more arrays at once.
x = np.array([1, 2, 3])
y = np.array([3, 2, 1])
z = [21,21,21]
np.concatenate([x, y,z])
array([ 1,  2,  3,  3,  2,  1, 21, 21, 21])

#You can also use this function to create 2-dimensional arrays.
grid = np.array([[1,2,3],[4,5,6]])
np.concatenate([grid,grid])
array([[1, 2, 3],
      [4, 5, 6],
      [1, 2, 3],
      [4, 5, 6]])

#Using its axis parameter, you can define row-wise or column-wise matrix
np.concatenate([grid,grid],axis=1)
array([[1, 2, 3, 1, 2, 3],
      [4, 5, 6, 4, 5, 6]])

Until now, we used the concatenation function of arrays of equal dimension. But, what if you are required to combine a 2D array with 1D array? In such situations, np.concatenate might not be the best option to use. Instead, you can use np.vstack or np.hstack to do the task. Let's see how!
x = np.array([3,4,5])
grid = np.array([[1,2,3],[17,18,19]])
np.vstack([x,grid])
array([[ 3,  4,  5],
      [ 1,  2,  3],
      [17, 18, 19]])


#Similarly, you can add an array using np.hstack
z = np.array([[9],[9]])
np.hstack([grid,z])
array([[ 1,  2,  3,  9],
      [17, 18, 19,  9]])

Also, we can split the arrays based on pre-defined positions. Let's see how!
x = np.arange(10)
x
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

x1,x2,x3 = np.split(x,[3,6])
print x1,x2,x3
[0 1 2] [3 4 5] [6 7 8 9]

grid = np.arange(16).reshape((4,4))
grid
upper,lower = np.vsplit(grid,[2])
print (upper, lower)
(array([[0, 1, 2, 3],
       [4, 5, 6, 7]]), array([[ 8,  9, 10, 11],
       [12, 13, 14, 15]]))

In addition to the functions we learned above, there are several other mathematical functions available in the numpy library such as sum, divide, multiple, abs, power, mod, sin, cos, tan, log, var, min, mean, max, etc. which you can be used to perform basic arithmetic calculations. Feel free to refer to numpy documentation for more information on such functions.

--
From ml4t:

--
Any python can run numpy, no need to use some annaconda environment (confirmed from tao's practice).

--
To use NumPy:
import numpy as np

The NumPy numerical library: NumPy is a Python library that acts as a wrapper around underlying C and Fortran code. Because of that, it's very, very fast. NumPy focuses on matrices which are called ndarrays. NumPy is one of the important reasons people use Python for financial research.

Now, how does NumPy relate to Pandas? Well, I said just a moment ago that NumPy is a wrapper for numerical libraries, well it turns out that Pandas is a kind of wrapper for NumPy. So remember our traditional data frame here, with our columns being symbols and our rows being dates. This data frame is just a wrapper around this ndarray, access the columns with symbols and the rows by dates. But you can, in fact, just treat this inside part (tao: the part of the dataframe without the header line and the index column) as an ndarray directly. If you use this syntax (nd1 = df1.values) in Python, that pulls these values out and lets you access it directly and then ndarray. You don't really need to do that though, you can, if you like, ** treat a data frame just like a NumPy ndarray **. And so we're going to assume in the rest of this lesson that we're just working with an ndarray

--
NumPy access ndarray cells:

nd[0, 0] # The element at row 0, column 0
nd[3, 2] # The element at row 3, column 2
nd[0:3, 1:3] # The block from row 0 to row 2, column 1 to column 2. Notices that 0:3 means 0,1,2.
nd[:, 3] # All the rows, column 3.
nd[-1, 1:3] # -1 means last row. Similary, -2 means second to last row.
nd[0, 1:3] # For the 0 row, get values from column 1 to column 2

nd[0, 0:3:2] # Slice n:m:t sepcifies a range that starts at n, and stops before m, in steps of size t.

nd1[0:2, 0:2] = nd2[-2:, 2:4] # Replace some of the values in nd1, with these values from nd2. "-2:" means from "the second to last row" to "the last row". 

--
Assign values:

nd[0, 0] = 1 # Assign value
nd[0, :] = 2 # Assign a single value to an entire row
nd[:, 3] = [1, 2, 3, 4, 5] # Assign a list to a column in an array.

--
Create NumPy arrays from scratch:

np.array() can take as input a list, a template, or other sequence.

import numpy as np

a = np.array([2, 3, 4]) # List to 1D array

b = np.array([(2, 3, 4), (5, 6, 7)]) # List of tuples to 2D array. Each tuple serves as one row. We could also have passed a list of lists.

print b

Output:
[[2 3 4]
  5 6 7]]

b.shape # Returns [2, 3]  
b.shape[0] # Number of rows
b.shape[1] # Number of columns
b.size # Number of elements in the array

print b.dtype # Data type of each element

--
Create empty ndarray:

np.empty(5)
np.empty((5, 4))

The empty function takes the shape of the array as input. The shape can be defined as a single integer, as we did over here, for creating a one dimensional array, or a sequence of integers denoting the size in each dimension. For a two dimensional array, a sequence of two integers is needed. That is the number of rows and the number of columns.

print np.empty((5, 4))

Now let's check the output. Hm, strange. The empty array is not actually empty. What happens is that when we call numpy.empty to create an array, the elements of the array read in whatever values were present in the corresponding memory location.

--
Create ndarray with ones or zeroes:

np.ones((5, 4)) # Create an array full of ones.
np.zeroes((5, 4)) # Create an array full of zeroes.

We notice that the default data type of all the values in the array is float. Fortunately, you can change this when creating the array:

np.ones((5, 4), dtype = np.int_) # Here we defined the values to be integers

--
Create ndarray with random values:

Generate an array full of random numbers, uniformly sampled from [0.0, 1.0). Pass in a size tuple:

np.random.random((5, 4)) # 5 rows, 4 columns

A slightly variation of this function is rand. We directly pass the
values of the rows and columns through the function and did not define a tuple:

np.random.rand(5, 4) # 5 rows, 4 columns

--
Create ndarray with normal (Gaussian) distribution.

Standard normal (mean = 0, s.d. = 1):

np.random.normal(size = (2, 3)) # 2 rows, 3 columns

Mean = 50, s.d. = 10:

np.random.normal(50, 10, size = (2, 3))

--
Create ndarray with random integers:

np.random.randint(10) # A single integer in [0, 10)
np.random.randint(0, 10) # Same as above, specifying [low, high) explicit
np.random.randint(0, 10, size = 5) # 5 random integers as a 1D array 
np.random.randint(0, 10, size = (2, 3)) # 2*3 array of random integers 

--
np.random.seed(693)

a = np.random.randint(0, 10, size = (5, 4))

Note how we used seed, the random number generator with the constant, to get the same sequence of numbers every time.

--
We can also sum in a specific direction of the array.  What I mean by direction is along rows or columns. NumPy gives this direction a special name.  It is called axis.  Axis = 0 signifies rows, and axis =  1 indicates columns. 

a:

[[2 0 5 1
  1 3 4 4
  9 2 9 1
  9 3 7 5 
  4 7 0 3]]

# Iterate over rows, to compute sum of each column:
a.sum(axis = 0) # Returns sum of each column: [25 15 25 14]

# Iterate over columns, to compute sum of each row:
a.sum(axis = 1) # Returns sum of each row: [8 12 21 24 14]

# Returns min across rows, to compute min of each column:
a.min(axis = 0) # Returns min of each column: [1 0 0 1]

# Returns max across columns, to compute max of each row:
a.max(axis = 1) # Returns max of each row: [5 4 9 9 7]

a.mean() # Returns the mean all elements: 3.95. Of course we can get mean along each axis as we did for max and min.  

--
Find the position of some element in an ndarray:

a.argmx() # Returns the index of the maximum value in given 1D array

For multidimensional arrays, finding and representing indices is a little tricky. 

--
We want to get all the values from the array, which is less than mean of the entire array.  

mean = a.mean()
a[a < mean] # Returns the wanted values above
a[a < mean] = mean # all the values previously less than mean have been replaced by the mean

--
Arithmetic operations on arrays are always applied element wise (tao: ie, element by elment).

2 * a # Every element multiplied by 2.
a + b # Add every element from a and b.
a * b # Normal matrix multiplication (as in Linear Algebra).

What about matrix multiplication?  How do you achieve that?  Like, for everything, Num Pi has a function.  It has function called dot, which performs matrix multiplication.  

==
(findscipy)
Scipy

Tao: before reading this scipy note, no need to review numpy because the beginning of this scipy note will provide a review on numpy.

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/python-scipy-tutorial

Scipy Tutorial: Vectors and Arrays (Linear Algebra)

A SciPy tutorial in which you'll learn the basics of linear algebra that you need for machine learning in Python, with a focus how to with NumPy.

Much of what you need to know to really dive into machine learning is linear algebra, and that is exactly what this tutorial tackles. Today’s post goes over the linear algebra topics that you need to know and understand to improve your intuition for how and when machine learning methods work by looking at the level of vectors and matrices.

By the end of the tutorial, you’ll hopefully feel more confident to take a closer look at an algorithm!

Contents
Introduction
SciPy versus NumPy
Interact with NumPy and SciPy
The Essentials of NumPy ndarray Objects
Array Creation
Linear Algebra With SciPy
Install Scipy
Vectors and Matrices: The Basics
Eigenvalues and Eigenvectors
Singular Value Decomposition (SVD)

Introduction

One of the three basic topics that you need to master if you want to learn data science is machine learning. And when you’re really getting into machine learning, it’s important to dive deeper and to gather an intuition and understanding of how the algorithms work.

And I think that you, (aspiring) data scientist, will probably agree with me when I say that it can be a tough job to get to this point.

But it doesn’t need to be.

SciPy versus NumPy

From DataCamp’s NumPy tutorial, you will have gathered that this library is one of the core libraries for scientific computing in Python. This library contains a collection of tools and techniques that can be used to solve on a computer mathematical models of problems in Science and Engineering. But the one tool that you’ll most often use is a high-performance multidimensional array object: it’s a powerful data structure that allows you to efficiently compute arrays and matrices.

Now, SciPy is basically NumPy.

It’s also one of the core packages for scientific computing that provides mathematical algorithms and convenience functions, but it’s built on the NumPy extension of Python. This means that SciPy and NumPy are often used together!

Later on in this tutorial, it will become clear to you how the collaboration between these two libraries has become self-evident.

Interact with NumPy and SciPy

To interact efficiently with both packages, you first need to know some of the basics of this library and its powerful data structure. To work with these arrays, there’s a huge amount of high-level mathematical functions operate on these matrices and arrays.

In what follows, you’ll get a quick overview of the topics that you need to have mastered in order to work efficiently with SciPy. In essence, you have to know how about the array structure and how you can handle data types and how you can manipulate the shape of arrays. If this sounds alien to you, consider taking DataCamp’s Python NumPy Tutorial and while you’re learning, don’t forget to take a look at the NumPy cheat sheet.

If you already know all of this, skip through the following section and go to “Linear Algebra With SciPy”, but keep your SciPy cheat sheet for linear algebra ready!

The Essentials of NumPy ndarray Objects

An array is, structurally speaking, nothing but pointers. It’s a combination of a memory address, a data type, a shape and strides. It contains information about the raw data, how to locate an element and how to interpret an element.

The memory address and strides are important when you dive deeper into the lower-level details of arrays, while the data type and shape are things that beginners should surely know and understand. Two other attributes that you might want to consider are the data and size, which allow you to gather even more information on your array.

Refresh the usage of the ndarray attributes in the following DataCamp Light chunk. The array myArray has already been loaded in. You can inspect it by typing it in into the IPython shell and by pressing ENTER.

print(myArray)

[[1 2 3]
 [4 5 6]]

# Inspect the data of `myArray`
print(myArray.data)

<memory at 0x7fda4dd82990>

# Inspect the data type of `myArray`
print(myArray.dtype)

int64

# Inspect the shape of `myArray`
print(myArray.shape)

(2, 3)

# Inspect the size of `myArray`
print(myArray.size)

6

You’ll see in the results of the code that is included in the code chunk above that the data type of myArray is int64. When you’re intensively working with arrays, you will definitely remember that there are ways to convert your arrays from one data type to another with the astype() method.

Nevertheless, when you’re using SciPy and NumPy together, you might also find the following type handling NumPy functions very useful, especially when you’re working with complex numbers:

print(myArray)

[[ 1.+5.j  2.+0.j  3.+6.j]
 [ 4.+1.j  5.+0.j  6.+2.j]]

# Return the real part of `myArray` elements
print(np.real(myArray))

[[ 1.  2.  3.]
 [ 4.  5.  6.]]

# Return the imaginary part of `myArray` elements
print(np.imag(myArray))

[[ 5.  0.  6.]
 [ 1.  0.  2.]]

# Return a real array if the complex parts are close to 0
print(np.real_if_close(myArray,tol=1000))

[[ 1.+5.j  2.+0.j  3.+6.j]
 [ 4.+1.j  5.+0.j  6.+2.j]]

# Cast `myArray` to float
print(np.cast['f'](myArray))

[[ 1.  2.  3.]
 [ 4.  5.  6.]]

Try to add print() calls to see the results of the code that is given above. Then, you’ll see that complex numbers have a real and an imaginary part to them. The np.real() and np.imag() functions are designed to return these parts to the user, respectively.

Alternatively, you might also be able to use np.cast to cast an array object to a different data type, such as float in the example above.

The only thing that really stands out in difficulty in the above code chunk is the np.real_if_close() function. When you give it a complex input, such as myArray, you’ll get a real array back if the complex parts are close to zero. This last part, “close to 0”, can be adjusted by yourself with the tol argument that you can pass to the function. Play around with it the treshold to see what happens!

Array Creation

You have now seen how to inspect your array and to make adjustments in the data type of it, but you haven’t explicitly seen how to create arrays. You should already know that you can use np.array() to to this, but there are other routines for array creation that you should know about: np.eye() and np.identity().

The np.eye() funcion allows you to create a square matrix with dimensions that are equal to the positive integer that you give as an argument to the function. The entries are generally filled with zeros, only the matrix diagonal is filled with ones. The np.identity() function works and does the same and also returns an identity array.

However, note that np.eye() can take an additional argument k that you can specify to pick the index of the diagonal that you want to populate with ones.

Other array creation functions that will most definitely come in handy when you’re working with the matrices for linear algebra are the following:

- The np.arange() function creates an array with uniformly spaced values between two numbers. You can specify the spacing between the elements,

- The latter also holds for np.linspace(), but with this function you specify the number of elements that you want in your array.

- Lastly, the np.logspace() function also creates arrays with uniformly spaced values, but this time in a logarithmic scale. This means that the spacing is now logarithmical: two numbers are evenly spaced between the logarithms of these two to the base of 10.

These functions create arrays in the form of meshes, which can be used to create mesh grids, about which you read earlier in this post.

Note that numpy is already imported as np in the code chunk below!

# Create a 2X2 identity matrix with `np.eye()`
print(np.eye(2))

[[ 1.  0.]
 [ 0.  1.]]

# Create a 3X3 identity matrix with `np.identity()`
print(np.identity(3))

[[ 1.  0.  0.]
 [ 0.  1.  0.]
 [ 0.  0.  1.]]

# Uniformly spaced values: spacing
print(np.arange(3, 7, 2))

[3 5]

# Uniformly spaced values: number of samples
print(np.linspace(2, 3, 5))

[ 2.    2.25  2.5   2.75  3.  ]

# Uniformly spaced values: logarithmic spacing 
print(np.logspace(2, 3, 4))

[  100.           215.443469     464.15888336  1000.        ]

Now that you have refreshed your memory and you know how to handle the data types of your arrays, it’s time to also tackle the topic of indexing and slicing.

Indexing and Slicing

With indexing, you basically use square brackets [] to index the array values. In other words, you can use indexing if you want to gain access to selected elements -a subset- of an array. Slicing your data is very similar to subsetting it, but you can consider it to be a little bit more advanced. When you slice an array, you don’t consider just particular values, but you work with “regions” of data instead of pure “locations”.

Refresh both concepts with the following code examples:

print(myArray)

[1 2 3 4]

# Slice `myArray` at index 0 and 1
print(myArray[0:2])

[1 2]

print(my_2dArray)

[[1 2 3 4]
 [5 6 7 8]]

# Slice `my_2dArray` at row 0 and 1, column 1
print(my_2dArray[0:2,1])

[2 6]

print(my_3dArray)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

# Slice `my_3dArray` at row 1
print(my_3dArray[1,...])

[[ 1  2  3  4]
 [ 9 10 11 12]]

# Boolean indexing: only values < 2
print(myArray[myArray<2])

[1]

# Fancy indexing
print(my_2dArray[[1, 0, 1, 0],[0, 1, 2, 0]])

[5 2 7 1]

Now that your mind is fresh with slicing and indexing, you might also be interested in some index tricks that can make your work more efficient when you’re going into scientific computing together with SciPy. There are four functions that will definitely come up and these are np.mgrid(), np.ogrid(), np.r and np.c.

You might already know the two last functions if you already have some experience with NumPy. np.r and np.c are often used when you need to stack arrays row-wise or column-wise, respectively. With these functions, you can quickly construct arrays instead of using the np.concatenate() function.

# Create a dense meshgrid
print(np.mgrid[1:11:2, -12:-3:3])

[[[  1   1   1]
  [  3   3   3]
  [  5   5   5]
  [  7   7   7]
  [  9   9   9]]

 [[-12  -9  -6]
  [-12  -9  -6]
  [-12  -9  -6]
  [-12  -9  -6]
  [-12  -9  -6]]]

# Create an open meshgrid
print(np.ogrid[1:11:2, -12:-3:3])

[array([[1],
       [3],
       [5],
       [7],
       [9]]), array([[-12,  -9,  -6]])]

# Stack arrays vertically
print(np.r_[3,[0]*5,-1:1:10j])

[ 3.          0.          0.          0.          0.          0.         -1.
 -0.77777778 -0.55555556 -0.33333333 -0.11111111  0.11111111  0.33333333
  0.55555556  0.77777778  1.        ]

print(array)

[[ 1.  0.]
 [ 0.  1.]]

print(my_2dArray)

[[1 2 3 4]
 [5 6 7 8]]

# Stack arrays horizontally 
print(np.c_[array, my_2dArray])

[[ 1.  0.  1.  2.  3.  4.]
 [ 0.  1.  5.  6.  7.  8.]]

By looking at the two first of these four functions, you might ask yourself why you would need a meshgrid. You can use meshgrids to generate two arrays containing the x- and y-coordinates at each position in a rectilinear grid. The np.meshgrid() function takes two 1D arrays and produces two 2D matrices corresponding to all pairs of (x, y) in the two arrays.

- The np.mgrid() function is an implementation of MATLAB’s meshgrid and returns arrays that have the same shape. That means that the dimensions and number of the output arrays are equal to the number of indexing dimensions.

- The np.ogrid() function, on the other hand, gives an open meshgrid and isn’t as dense as the result that the np.mgrid() function gives. You can see the visual difference between the two in the code chunk above.

You can also read up on the differences between these two functions here.

Another function that you might be able to use for indexing/slicing purposes is the np.select() function. You can use it to return values from a list of arrays depending on conditions, which you can specify yourself in the first argument of the function. In the second argument, you pass the array that you want to consider for this selection process.

Check out the following example:

# Initialize a 2D array 
my_2dArray = np.array([[1,2,3,4], [5,6,7,8]], dtype=np.int64)

print(my_2dArray)

[[1 2 3 4]
 [5 6 7 8]]

# Select values from `my_2dArray`
print(np.select([my_2dArray < 4],[my_2dArray * 2]))

[[2 4 6 0]
 [0 0 0 0]]

Awesome! Now that you have selected the right values of your original array, you can still select the shape and manipulate your new array.

Shape Selection and Manipulation

NumPy offers a lot of ways to select and manipulate the shape of your arrays and you’ll probably already know a lot of them. The following section will only give a short overview of the functions that might come in handy, so the post won’t cover all of them.

Now, is there such a thing as functions that are handy when you’re working with SciPy?

Well, the ones that are most useful are the ones that can help you to flatten arrays, stack and split arrays. You have already seen the np.c and np.r functions that you’ll often prefer instead of np.concatenate(), but there are many more that you want to know!

Like np.hstack() to horizontally stack your arrays or np.vstack() to vertically stack your arrays. Similarly, you can use np.vsplit() and np.hsplit() to split your arrays vertically and horizontally. But you’ll probably know all of this already.

Prove it in the following code chunk:

print(my_2dArray)

[[1 2 3 4]
 [5 6 7 8]]

#Stack arrays horizontally (column-wise)
print(np.hstack((np.eye(2), my_2dArray)))

[[ 1.  0.  1.  2.  3.  4.]
 [ 0.  1.  5.  6.  7.  8.]]

print(myArray)

[1 2 3 4]

# Stack arrays vertically (row-wise)
print(np.vstack((myArray, my_2dArray)))

[[1 2 3 4]
 [1 2 3 4]
 [5 6 7 8]]

# Split the array horizontally at the 2nd index
print(np.hsplit(my_2dArray, 2))

[array([[1, 2],
       [5, 6]]), array([[3, 4],
       [7, 8]])]

# Split the array vertically at the 2nd index
print(np.vsplit(my_2dArray, 2))

[array([[1, 2, 3, 4]]), array([[5, 6, 7, 8]])]

Remember that the np.eye() function creates a 2X2 identity array, which is perfect to stack with the 2-D array that has been loaded in for you in the code chunk above.

If you want to know more about the conditions that you need to take into account if you want to stack arrays, go here. However, you can also just look at the arrays and what the functions to do gather an intuition of which ‘rules’ you need to respect if you want to join the two arrays by row or column.

The most important thing that you need to take into account when splitting arrays is probably the shape of your array, because you want to select the correct index at which you want the split to occur.

Besides functions to stack and split arrays, you will also want to keep in mind that you have functions that help to ensure that you’re working with arrays of a certain dimension are indispensable when you’re diving deeper into scientific computing.

Consider the following functions:

print(myArray)

[1 2 3 4]

print(my_2dArray)

[[1 2 3 4]
 [5 6 7 8]]

print(my_3dArray)

[[[ 1  2  3  4]
  [ 5  6  7  8]]

 [[ 1  2  3  4]
  [ 9 10 11 12]]]

#Permute `myArray` dimensions
print(np.transpose(myArray))

[1 2 3 4]

# Flatten `my_3dArray`
print(my_3dArray.flatten())

[ 1  2  3  4  5  6  7  8  1  2  3  4  9 10 11 12]

# Reshape but don't change the data
print(my_2dArray.reshape(4,2))

[[1 2]
 [3 4]
 [5 6]
 [7 8]]

# Resize to (6,4)
print(np.resize(my_3dArray, (6,4)))

[[ 1  2  3  4]
 [ 5  6  7  8]
 [ 1  2  3  4]
 [ 9 10 11 12]
 [ 1  2  3  4]
 [ 5  6  7  8]]

Note the difference between reshaping and resizing your array. With the first, you change the shape of the data but you don’t change the data itself. When you resize, there is the possibility that the data that is contained within the array will change, depending on the shape that you select, of course.

Besides the splitting and stacking routines and the functions that allow you to further manipulate your arrays, there is also something like “vectorization” that you need to consider. When you apply a function to an array, you usually apply it to each element of the array. Consider, for example, applying np.cos(), np.sin() or np.tan() to an array. You’ll see that it works on all array elements. Now, when you see this, you know that the function is vectorized.

But, when you define functions by yourself, as you will most likely do when you’re getting into scientific computing, you might also want to vectorize them. In those cases, you can call np.vectorize():

(findvectorizescipy)
vectorize

# Define a function `myfunc`
def myfunc(a,b):
  if a > b:
     return a - b
  else:
     return a + b
    
# Vectorize `myfunc`
vectorizedFunc = np.vectorize(myfunc) 

print(myArray)

[1 2 3 4]

# Apply 
print(vectorizedFunc(myArray, 2))

[3 4 1 2]

When it comes to other vectorized mathematical functions that you might want to know, you should consider np.angle() to provide the angle of the elements of complex array elements, but also basic trigonometric, exponential or logarithmic functions will come in handy.

Linear Algebra With SciPy

Now that you know what you need to use both packages to your advantage, it’s time to dig into the topic of this tutorial: linear algebra.

But before you go into how you can use Python, make sure that your workspace is completely ready!

Install SciPy

Of course you first need to make sure firstly that you have Python installed. Go to this page if you still need to do this :) If you’re working on Windows, make sure that you have added Python to the PATH environment variable. In addition, don’t forget to install a package manager, such as pip, which will ensure that you’re able to use Python’s open-source libraries.

Note that recent versions of Python 3 come with pip, so double check if you have it and if you do, upgrade it before you install any other packages:

pip install pip --upgrade
 pip --version

But just installing a package manager is not enough; You also need to download to download the wheel for the library: go here to get your SciPy wheel. After the download, open up the terminal at the download directory on your pc and install it. Additionally, you can check whether the installation was successful and confirm the package version that you’re running:

# Install the wheel 
install "scipy‑0.18.1‑cp36‑cp36m‑win_amd64.whl"

# Confirm successful install
import scipy

# Check package version
scipy.__version__

After these steps, you’re ready to goy!

Tip: install the package by downloading the Anaconda Python distribution. It’s an easy way to get started quickly, as Anaconda not only includes 100 of the most popular Python, R and Scala packages for data science, but also includes several open course development environments such as Jupyter and Spyder. If you’d like to start working with Jupyter Notebook, check out this Jupyter notebook tutorial.

If you haven’t downloaded it already, go here to get it.

Vectors and Matrices: The Basics

Now that you have made sure that your workspace is prepped, you can finally get started with linear algebra in Python. In essence, this discipline is occupied with the study of vector spaces and the linear mappings that exist between them. These linear mappings can be described with matrices, which also makes it easier to calculate.

Remember that a vector space is a fundamental concept in linear algebra. It’s a space where you have a collection of objects (vectors) and where you can add or scale two vectors without the resulting vector leaving the space. Remember also that vectors are rows (or columns) of a matrix.

But how does this work in Python?

You can easily create a vector with the np.array() function. Similarly, you can give a matrix structure to every one-or two-dimensional ndarray with either the np.matrix() or np.mat() commands.

Try it out in the following code chunk:

# Create a vector
myVector = np.array([1,2,3,4])

print(myVector)

[1 2 3 4]

(findmatrixscipy)
Matrix

# Create a matrix
myMatrix = np.matrix(np.random.random((5,5)))

print(myMatrix)

[[ 0.69646919  0.28613933  0.22685145  0.55131477  0.71946897]
 [ 0.42310646  0.9807642   0.68482974  0.4809319   0.39211752]
 [ 0.34317802  0.72904971  0.43857224  0.0596779   0.39804426]
 [ 0.73799541  0.18249173  0.17545176  0.53155137  0.53182759]
 [ 0.63440096  0.84943179  0.72445532  0.61102351  0.72244338]]

So arrays and matrices are the same, besides from the formatting?

Well, not exactly. There are some differences:

- A matrix is 2-D, while arrays are usually n-D,

- As the functions above already implied, the matrix is a subclass of ndarray,

- Both arrays and matrices have .T(), but only matrices have .H() and .I(),

- Matrix multiplication works differently from element-wise array multiplication, and

- To add to this, the ** operation has different results for matrices and arrays

When you’re working with matrices, you might sometimes have some in which most of the elements are zero. These matrices are called “sparse matrices”, while the ones that have mostly non-zero elements are called “dense matrices”.

In itself, this seems trivial, but when you’re working with SciPy for linear algebra, this can sometimes make a difference in the modules that you use to get certain things done. More concretely, you can use scipy.linalg for dense matrices, but when you’re working with sparse matrices, you might also want to consider checking up on the scipy.sparse module, which also contains its own scipy.sparse.linalg.

For sparse matrices, there are quite a number of options to create them. The code chunk below lists some:

# Create a 2X2 identity matrix
print(np.eye(3, k=1))       

[[ 0.  1.  0.]
 [ 0.  0.  1.]
 [ 0.  0.  0.]]

# Create a 2x2 identity matrix
print(np.mat(np.identity(2)))         

[[ 1.  0.]
 [ 0.  1.]]

# Compressed Sparse Row matrix
print(sparse.csr_matrix(C)) 

  (0, 1)	0.28613933495
  (0, 2)	0.226851453564
  (1, 0)	0.423106460124
  (1, 3)	0.480931901484
  (1, 4)	0.392117518194
  (2, 0)	0.343178016151
  (2, 2)	0.43857224468
  (2, 3)	0.0596778966096
  (2, 4)	0.39804425533
  (3, 1)	0.182491730453
  (3, 2)	0.175451756147
  (5, 0)	0.322958913853
  (5, 1)	0.361788655622
  (5, 2)	0.228263230879
  (5, 3)	0.293714046389
  (6, 0)	0.0921049399451
  (6, 1)	0.43370117268
  (6, 2)	0.43086276333
  (6, 3)	0.49368509765
  (6, 4)	0.425830290296
  (7, 0)	0.312261222972
  (7, 1)	0.426351306963
  (8, 1)	0.115618395079
  (8, 2)	0.31728548182
  (8, 3)	0.414826211954
  (9, 0)	0.250455365397
  (9, 1)	0.483034264263

# Compressed Sparse Column matrix
print(sparse.csc_matrix(C))  

  (1, 0)	0.423106460124
  (2, 0)	0.343178016151
  (5, 0)	0.322958913853
  (6, 0)	0.0921049399451
  (7, 0)	0.312261222972
  (9, 0)	0.250455365397
  (0, 1)	0.28613933495
  (3, 1)	0.182491730453
  (5, 1)	0.361788655622
  (6, 1)	0.43370117268
  (7, 1)	0.426351306963
  (8, 1)	0.115618395079
  (9, 1)	0.483034264263
  (0, 2)	0.226851453564
  (2, 2)	0.43857224468
  (3, 2)	0.175451756147
  (5, 2)	0.228263230879
  (6, 2)	0.43086276333
  (8, 2)	0.31728548182
  (1, 3)	0.480931901484
  (2, 3)	0.0596778966096
  (5, 3)	0.293714046389
  (6, 3)	0.49368509765
  (8, 3)	0.414826211954
  (1, 4)	0.392117518194
  (2, 4)	0.39804425533
  (6, 4)	0.425830290296

# Dictionary Of Keys matrix
print(sparse.dok_matrix(C))  

  (6, 4)	0.425830290296
  (3, 2)	0.175451756147
  (1, 3)	0.480931901484
  (8, 2)	0.31728548182
  (7, 1)	0.426351306963
  (6, 0)	0.0921049399451
  (1, 4)	0.392117518194
  (6, 2)	0.43086276333
  (2, 3)	0.0596778966096
  (5, 1)	0.361788655622
  (1, 0)	0.423106460124
  (5, 3)	0.293714046389
  (0, 1)	0.28613933495
  (9, 0)	0.250455365397
  (7, 0)	0.312261222972
  (8, 1)	0.115618395079
  (6, 1)	0.43370117268
  (3, 1)	0.182491730453
  (2, 4)	0.39804425533
  (6, 3)	0.49368509765
  (2, 0)	0.343178016151
  (5, 0)	0.322958913853
  (2, 2)	0.43857224468
  (9, 1)	0.483034264263
  (8, 3)	0.414826211954
  (5, 2)	0.228263230879
  (0, 2)	0.226851453564

Additionally, there are also some other functions that you might be able to use to create sparse matrices: Block Sparse Row matrices with bsr_matrix(), COOrdinate format sparse matrices with coo_matrix(), DIAgonal storage sparse matrices with dia_matrix(), and Row-based linked list sparse matrices with lil_matrix().

There are really a lot of options, but which one should you choose if you’re making a sparse matrix yourself?

It’s not that hard.

Basically, it boils down to first is how you’re going to initialize it. Next, consider what you want to be doing with your sparse matrix.

More concretely, you can go through the following checklist to decide what type of sparse matrix you want to use:

- If you plan to fill the matrix with numbers one by one, pick a coo_matrix() or dok_matrix() to create your matrix.

- If you want to initialize the matrix with an array as the diagonal, pick dia_matrix() to initialize your matrix.

- For sliced-based matrices, use lil_matrix().

- If you’re constructing the matrix from blocks of smaller matrices, consider using bsr_matrix().

- If you want to have fast access to your rows and columns, convert your matrices by using the csr_matrix() and csc_matrix() functions, respectively. The last two functions are not great to pick when you need to initialize your matrices, but when you’re multiplying, you’ll definitely notice the difference in speed.

Easy peasy!

Vector Operations

Now that you have learned or refreshed the difference between vectors, dense matrices and sparse matrices, it’s time to take a closer look at vectors and what kind of mathematical operations you can do with them. The tutorial focuses here explicitly on mathematical operations so that you’ll come to see the similarities and differences with matrices, and because a huge part of linear algebra is, ultimately, working with matrices.

You have already seen that you can easily create a vector with np.array(). But now that you have vectors at your disposal, you might also want to know of some basic operations that can be performed on them. vector1 and vector2 are already loaded for you in the following code chunk:

print(vector1)

[1 2 3]

print(vector2)

[2 3 4]

# Addition of `vector1` and `vector2`
vector3 = vector1 + vector2

# Print `vector3` of `vector2` and `vector1`
print(vector3)

[3 5 7]

# Subtraction
vector4 = vector2 - vector1

# print `vector4`
print(vector4)

[1 1 1]

# Dot product of `vector1` and `vector2`
dotProduct = np.dot(vector1, vector2)

# Print `dotProduct`
print(dotProduct)

20

# Cross product of `vector1` and `vector2`
crossProduct = np.cross(vector1, vector2)

# Print `crossProduct`
print(crossProduct)

[-1  2 -1]

Now that you have successfully seen some vector operations, it’s time to get started on to the real matrix work!

Matrices: Operations and routines

Similarly to where you left it off at the start of the previous section, you know how to create matrices, but you don’t know yet how you can use them to your advantage. This section will provide you with an overview of some matrix functions and basic matrix routines that you can use to work efficiently.

Firstly, let’s go over some functions. These will come quite easily if you have worked with NumPy before, but even if you don’t have any experience with it yet, you’ll see that these functions are easy to get going.

Let’s look at some examples of functions.

There’s np.add() and np.subtract() to add and subtract arrays or matrices, and also np.divide() and np.multiply for division and multiplication. This really doesn’t seem like a big msytery, does it? Also the np.dot() function that you have seen in the previous section where it was used to calculate the dot product, can also be used with matrices. But don’t forget to pass in two matrices instead of vectors.

These are basic, right?

Let’s go a bit less basic. When it comes to multiplications, there are also some other functions that you can consider, such as np.vdot() for the dot product of vectors, np.inner() ornp.outer() for the inner or outer products of arrays, np.tensordot() and np.kron() for the Kronecker product of two arrays:

print(vector1)

[1 2 3]

print(vector2)

[2 3 4]

# Vector dot product
vectorDotProduct = np.vdot(vector1, vector2)

print(vectorDotProduct)

20

# Inner product
innerProduct = np.inner(vector1, vector2)

print(innerProduct)

20

# Outer product
outerProduct = np.outer(vector1, vector2)

print(outerProduct)

[[ 2  3  4]
 [ 4  6  8]
 [ 6  9 12]]

# Tensor dot product
tensorDotProduct = np.tensordot(matrix1, matrix2)

print(tensorDotProduct)

7.676110778913466

# Kronecker product
kronProduct = np.kron(matrix1, matrix2)

print(kronProduct)

[[ 2.08940756  2.78587674  0.858418    1.14455734]
 [ 3.48234593  4.17881511  1.43069667  1.71683601]
 [ 0.68055436  0.90740581  1.65394431  2.20525908]
 [ 1.13425727  1.36110872  2.75657385  3.30788861]]

Tip: add print statements to the code chunk above to see the individual product results.

Besides these, it might also be useful to consider some functions of the linalg module: the matrix exponential functions linalg.expm(), linalg.expm2() and linalg.expm3(). The difference between these three lies in the ways that the exponential is calculated. Stick to the first one for a general matrix exponential, but definitely try the three of them out to see the difference in results!

Also trigonometric functions such as linalg.cosm(), linalg.sinm() and linalg.tanm(), hyperbolic trigonometric functions such as linalg.coshm(), linalg.sinhm() and linalg.tanhm(), the sign function linalg.signm(), the matrix logarithm linalg.logm(), and the matrix square root linalg.sqrtm().

Additionally, you can also evaluate a matrix function with the help of the linalg.funm() function. Check out the example below:

print(linalg.funm(myMatrix, lambda x: x*x))

[[ 1.54728673  1.35705863  1.07139479  1.26778988  1.51656513]
 [ 1.54835205  2.00308392  1.43643809  1.24104787  1.50063489]
 [ 0.99454858  1.48196572  1.06830684  0.84093171  1.02665305]
 [ 1.38109008  1.06681891  0.84788631  1.11261048  1.33926957]
 [ 1.95910926  2.26795642  1.67394003  1.56772771  1.92475777]]

You see that you pass in the matrix to which you want to apply a function as a first argument and a function (in this case a lambda function) that you want to apply to the matrix you passed. Note that the function that you pass to linalg.funm() has to be vectorized.

Let’s now take a look at some basic matrix routines. The first thing that you probably want to check out are the matrix attributes: T for transposition, H for conjugate transposition, I for inverse, and A to cast as an array.

print(myMatrix)

[[ 0.69646919  0.28613933  0.22685145  0.55131477  0.71946897]
 [ 0.42310646  0.9807642   0.68482974  0.4809319   0.39211752]
 [ 0.34317802  0.72904971  0.43857224  0.0596779   0.39804426]
 [ 0.73799541  0.18249173  0.17545176  0.53155137  0.53182759]
 [ 0.63440096  0.84943179  0.72445532  0.61102351  0.72244338]]

# Transposition
print(myMatrix.T)

[[ 0.69646919  0.42310646  0.34317802  0.73799541  0.63440096]
 [ 0.28613933  0.9807642   0.72904971  0.18249173  0.84943179]
 [ 0.22685145  0.68482974  0.43857224  0.17545176  0.72445532]
 [ 0.55131477  0.4809319   0.0596779   0.53155137  0.61102351]
 [ 0.71946897  0.39211752  0.39804426  0.53182759  0.72244338]]

# Conjugate transposition
print(myMatrix.H)

[[ 0.69646919  0.42310646  0.34317802  0.73799541  0.63440096]
 [ 0.28613933  0.9807642   0.72904971  0.18249173  0.84943179]
 [ 0.22685145  0.68482974  0.43857224  0.17545176  0.72445532]
 [ 0.55131477  0.4809319   0.0596779   0.53155137  0.61102351]
 [ 0.71946897  0.39211752  0.39804426  0.53182759  0.72244338]]

# Inverse
print(myMatrix.I)

[[-4.08027276 -1.02093921  1.76866955  5.00250993 -0.03948233]
 [ 3.93980994  4.01717263  0.73458457 -2.0909691  -4.969437  ]
 [-7.33374263 -5.03758733 -0.25380405  3.20898448  7.81531868]
 [ 2.76300946  3.09248766 -2.99480819 -1.62876504 -1.58106476]
 [ 3.96797085 -1.3907042   0.37060726 -3.77470911  0.76194749]]

# Array
print(myMatrix.A)

[[ 0.69646919  0.28613933  0.22685145  0.55131477  0.71946897]
 [ 0.42310646  0.9807642   0.68482974  0.4809319   0.39211752]
 [ 0.34317802  0.72904971  0.43857224  0.0596779   0.39804426]
 [ 0.73799541  0.18249173  0.17545176  0.53155137  0.53182759]
 [ 0.63440096  0.84943179  0.72445532  0.61102351  0.72244338]]

When you tranpose a matrix, you make a new matrix whose rows are the columns of the original. A conjugate transposition, on the other hand, interchanges the row and column index for each matrix element. The inverse of a matrix is a matrix that, if multiplied with the original matrix, results in an identity matrix.

But besides those attributes, there are also real functions that you can use to perform some basic matrix routines, such as np.transpose() and linalg.inv() for transposition and matrix inverse, respectively.

Besides these, you can also retrieve the trace or sum of the elements on the main matrix diagonal with np.trace(). Similarly, you can also retrieve the matrix rank or the number of Singular Value Decomposition singular values of an array that are greater than a certain treshold with linalg.matrix_rank from NumPy.

Don’t worry if the matrix rank doesn’t make sense for now; You’ll see more on that later on in this tutorial.

For now, let’s focus on two more routines that you can use:

- The norm of a matrix can be computed with linalg.norm: a matrix norm is a number defined in terms of the entries of the matrix. The norm is a useful quantity which can give important information about a matrix because it tells you how large the elements are.

- On top of that, you can also calculate the determinant, which is a useful value that can be computed from the elements of a square matrix, with linalg.det(). The determinant boils down a square matrix to a a single number, which determines whether the square matrix is invertible or not.

Lastly, solving large systems of linear equations are one of the most basic applications of matrices. If you have a system of Ax=b, where A is a square matrix and b a general matrix, you have two methods that you can use to find x, depending of course on which type of matrix you’re working with:

print(A)

[[ 0.69646919  0.28613933]
 [ 0.22685145  0.55131477]]

print(b)

[[ 1.+5.j  0.+2.j  0.+3.j]
 [ 0.+4.j  0.+5.j  0.+6.j]]

print(F)

[[ 0.  1.  0.]
 [ 0.  0.  1.]
 [ 0.  0.  0.]]

print(E)

[[1]
 [2]
 [3]]

# Dense matrix solver
print(linalg.solve(A, b))

[[ 1.72792025 +5.05235143j  0.00000000 -1.02822177j
   0.00000000 -0.19711398j]
 [-0.71099351 +5.1764743j   0.00000000 +9.49231527j
   0.00000000+10.96418223j]]

# Linear least-square solver
print(linalg.lstsq(F,E))

(array([[ 0.],
       [ 1.],
       [ 2.]]), array([], dtype=float64), 2, array([ 1.,  1.,  0.]))

To solve sparse matrices, you can use linalg.spsolve(). When you can not solve the equation, it might still be possible to obtain an approximate x with the help of the linalg.lstsq() command.

Tip: don’t miss DataCamp’s SciPy cheat sheet.

Now that you have gotten a clue on how you can create matrices and how you can use them for mathematical operations, it’s time to tackle some more advanced topics that you’ll need to really get into machine learning.

Eigenvalues and Eigenvectors

The first topic that you will tackle are the eigenvalues and eigenvectors.

Eigenvalues are a new way to see into the heart of a matrix. But before you go more into that, let’s explain first what eigenvectors are. Almost all vectors change direction, when they are multiplied by a matrix. However, certain exceptional, resulting vectors are in the same direction as the vectors that are the result of the multiplication. These are the eigenvectors.

In other words, multiply an eigenvector by a matrix, and the resulting vector of that multiplication is equal to a multiplication of the original eigenvector with λ, the eigenvalue:
Ax=λx.

This means that the eigenvalue gives you very valuable information: it tells you whether one of the eigenvectors is stretched, shrunk, reversed, or left unchanged—when it is multiplied by a matrix.

print(myMatrix)

[[ 0.69646919  0.28613933]
 [ 0.22685145  0.55131477]]

# Solve eigenvalue problem
la, v = linalg.eig(myMatrix) 

print(la)

[ 0.88880437+0.j  0.35897959+0.j]

print(v)

[[ 0.82993503 -0.64669426]
 [ 0.55786006  0.76274933]]

# Unpack eigenvalues
l1, l2 = la

print(l1)

(0.888804368923+0j)

print(l2)

(0.358979585758+0j)

# First eigenvector
print(v[:,0])

[ 0.82993503  0.55786006]

# Second eigenvector
print(v[:,1])

[-0.64669426  0.76274933]

# Or unpack eigenvalues with `eigvals()`
print(linalg.eigvals(myMatrix))

[ 0.88880437+0.j  0.35897959+0.j]

You use the eig() function from the linalg SciPy module to solve ordinary or generalized eigenvalue problems for square matrices.

Note that the eigvals() function is another way of unpacking the eigenvalues of a matrix.

When you’re working with sparse matrices, you can fall back on the module scipy.sparse to provide you with the correct functions to find the eigenvalues and eigenvectors:

la, v = sparse.linalg.eigs(myMatrix,1)

Note that the code above specifies the number of eigenvalues and eigenvectors that has to be retrieved, namely, 1.

The eigenvalues and eigenvectors are important concepts in many computer vision and machine learning techniques, such as Principal Component Analysis (PCA) for dimensionality reduction and EigenFaces for face recognition.

Singular Value Decomposition (SVD)

Next, you need to know about SVD if you want to really learn data science. The singular value decomposition of a matrix A is the decomposition or facorization of A into the product of three matrices: A=U∗Σ∗Vt.

The size of the individual matrices is as follows if you know that matrix A is of size M x N:

- Matrix U is of size M x M

- Matrix V is of size N x N

- Matrix Σ is of size M x N

The ∗ indicates that the matrices are multiplied and the t that you see in Vt means that the matrix is transposed, which means that the rows and columns are interchanged.

Simply stated, singular value decomposition provides a way to break a matrix into simpler, meaningful pieces. These pieces may contain some data we are interested in.

print(myMatrix)

[[ 1.+5.j  0.+2.j  0.+3.j]
 [ 0.+4.j  0.+5.j  0.+6.j]]

# Singular Value Decomposition
U,s,Vh = linalg.svd(myMatrix) 

print(U)

[[-0.56260618 +7.06586639e-19j  0.82672503 +0.00000000e+00j]
 [-0.82386933 -6.86557772e-02j -0.56066281 -4.67219005e-02j]]

print(s)

[ 10.47751754   2.49431877]

print(Vh)

[[-0.07990722 -5.83011023e-01j -0.03276338 -5.00553588e-01j
  -0.03931606 -6.32882215e-01j]
 [ 0.25651791 +7.58112384e-01j -0.09365663 -4.60993186e-01j
  -0.11238796 -3.54325894e-01j]
 [-0.11479126 -3.56425932e-04j -0.68803469 -2.31721068e-01j
   0.64988974 +1.93338508e-01j]]

# Initialize `M` and `N`
M,N = myMatrix.shape

print(M)

2

print(N)

3

# Construct sigma matrix in SVD
Sig = linalg.diagsvd(s,M,N)

print(Sig)

[[ 10.47751754   0.           0.        ]
 [  0.           2.49431877   0.        ]]

Note that for sparse matrices, you can use the sparse.linalg.svds() function to perform the decomposition.

If you’re new to data science, the matrix decomposition will be quite opaque for you: you might not immediately see any use cases to apply this. But SVD is useful in many tasks, such as data compression, noise reduction and data analysis. In the following, you will see how SVD can be used to compress images:

# Import the necessary packages
import numpy as np
from scipy import linalg
from skimage import data
import matplotlib.pyplot as plt

# Get an image from `skimage`
img= data.camera()

# Check number of singular values
linalg.svdvals(img)

# Singular Value Decomposition
U, s, Vh = linalg.svd(img)

# Use only 32 singular values
A = np.dot(U[:,0:32], 
          np.dot(np.diag(s[0:32]), Vh[0:32,:]))

fig = plt.figure(figsize=(8, 3))

# Add a subplot to the figure
ax = fig.add_subplot(121)

# Plot `img` on grayscale
ax.imshow(img, cmap='gray')

# Add a second subplot to the figure
ax2 = fig.add_subplot(122)

# Plot `A` in the second subplot
ax2.imshow(A)

# Add a title
fig.suptitle('Image Compression with SVD', fontsize=14, fontweight='bold')

# Show the plot
plt.show()

Which will give you the following result:

(A picture)

Consider also the following examples where SVD is used:

- SVD is closely linked to Principal Component Analysis (PCA), which is used for dimensionality reduction: both result in a set of “new axes” that are constructed from linear combinations of the the feature space axes of your data. These “new axes” break down the variance in the data points based on each direction’s contribution to the variance in the data. To see a concrete example of how PCA works on data, go to our Scikit-Learn Tutorial.

- Another link is one with data mining and natural language processing (NLP): Latent Semantic Indexing (LSI). It is a technique that is used in document retrieval and word similarity. Latent semantic indexing uses SVD to group documents to the concepts that could consist of different words found in those documents. Various words can be grouped into a concept. Also here, SVD reduces the noisy correlation between words and their documents, and it decreases the number of dimensions that the original data has.

You see, SVD is an important concept in your data science journey that you must cover. That’s why you should consider going deeper into SVD than what this tutorial covers: for example, go to this page to read more about this matrix decomposition.

What’s Up Next?

You’ve made it to the end of the tutorial! Where you go on out from here is totally up to you.

But hold up!

Don’t miss out on courses that go deeper into linear algebra: this tutorial has only been an introduction to the topic and hasn’t covered everything!

Of course, also consider taking DataCamp’s Machine Learning tutorial, which will definitely add value to your learning curriculum after going through this Scipy tutorial about linear algebra. But, if you want to go back to the basics, go through our NumPy tutorial or the Intermediate Python for Data Science course.

==
(findpandas)
Pandas

Tao: all the example code in this note (the note from the following link in datacamp.com) have been tested and works in Tao's Jupyter Notebook with Python 3.

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/pandas-tutorial-dataframe-python

Pandas Tutorial: DataFrames in Python

Explore data analysis with Python. Pandas DataFrames make manipulating your data easy, from selecting or replacing columns and indices to reshaping your data.

Pandas is a popular Python package for data science, and with good reason: it offers powerful, expressive and flexible data structures that make data manipulation and analysis easy, among many other things. The DataFrame is one of these structures.

This tutorial covers Pandas DataFrames, from basic manipulations to advanced operations, by tackling 11 of the most popular questions so that you understand -and avoid- the doubts of the Pythonistas who have gone before you.

Content
How To Create a Pandas DataFrame
How To Select an Index or Column From a DataFrame
How To Add an Index, Row or Column to a DataFrame
How To Delete Indices, Rows or Columns From a DataFrame
How To Rename the Columns or Indices of a DataFrame
How To Format the Data in Your DataFrame
How To Create an Empty DataFrame
Does Pandas Recognize Dates When Importing Data?
When, Why and How You Should Reshape Your DataFrame
How To Iterate Over a DataFrame
How To Write a DataFrame to a File

(For more practice, try the first chapter of this Pandas DataFrames course for free!)

What Are Pandas Data Frames?

Before you start, let’s have a brief recap of what DataFrames are.

Those who are familiar with R know the data frame as a way to store data in rectangular grids that can easily be overviewed. Each row of these grids corresponds to measurements or values of an instance, while each column is a vector containing data for a specific variable. This means that a data frame’s rows do not need to contain, but can contain, the same type of values: they can be numeric, character, logical, etc.

Now, DataFrames in Python are very similar: they come with the Pandas library, and they are defined as two-dimensional labeled data structures with columns of potentially different types.

In general, you could say that the Pandas DataFrame consists of three main components: the data, the index, and the columns.

Firstly, the DataFrame can contain data that is:

- a Pandas DataFrame

- a Pandas Series: a one-dimensional labeled array capable of holding any data type with axis labels or index. An example of a Series object is one column from a DataFrame.

- a NumPy ndarray, which can be a record or structured

- a two-dimensional ndarray

- dictionaries of one-dimensional ndarray’s, lists, dictionaries or Series.

Note the difference between np.ndarray and np.array(). The former is an actual data type, while the latter is a function to make arrays from other data structures.

Structured arrays allow users to manipulate the data by named fields: in the example below, a structured array of three tuples is created. The first element of each tuple will be called foo and will be of type int, while the second element will be named bar and will be a float.

Record arrays, on the other hand, expand the properties of structured arrays. They allow users to access fields of structured arrays by attribute rather than by index. You see below that the foo values are accessed in the r2 record array.

An example:

Tao: import pandas and numpy:
import numpy as np
import pandas as pd

# A structured array
my_array = np.ones(3, dtype=([('foo', int), ('bar', float)]))

print(my_array['foo'])

[1 1 1]

print(my_array['bar'])

[1. 1. 1.]

# A record array
my_array2 = my_array.view(np.recarray)

print(my_array2.foo)

[1 1 1]

print(my_array2.bar)

[1. 1. 1.]

Besides data, you can also specify the index and column names for your DataFrame. The index, on the one hand, indicates the difference in rows, while the column names indicate the difference in columns. You will see later that these two components of the DataFrame will come in handy when you’re manipulating your data.

If you’re still in doubt about Pandas DataFrames and how they differ from other data structures such as a NumPy array or a Series, you can watch the small presentation below:

(A video)

Note that in this post, most of the times, the libraries that you need have already been loaded in. The Pandas library is usually imported under the alias pd, while the NumPy library is loaded as np. Remember that when you code in your own data science environment, you shouldn’t forget this import step, which you write just like this:

import numpy as np
import pandas as pd

Now that there is no doubt in your mind about what DataFrames are, what they can do and how they differ from other structures, it’s time to tackle the most common questions that users have about working with them!

1. How To Create a Pandas DataFrame

Obviously, making your DataFrames is your first step in almost anything that you want to do when it comes to data munging in Python. Sometimes, you will want to start from scratch, but you can also convert other data structures, such as lists or NumPy arrays, to Pandas DataFrames. In this section, you’ll only cover the latter. However, if you want to read more on making empty DataFrames that you can fill up with data later, go to question 7.

Among the many things that can serve as input to make a ‘DataFrame’, a NumPy ndarray is one of them. To make a data frame from a NumPy array, you can just pass it to the DataFrame() function in the data argument.

(findcreatedataframefromnumpyarray)

data = np.array([['','Col1','Col2'],
                ['Row1',1,2],
                ['Row2',3,4]])

print(data)

[['' 'Col1' 'Col2']
 ['Row1' '1' '2']
 ['Row2' '3' '4']]

print(pd.DataFrame(data=data[1:,1:],
                  index=data[1:,0],
                  columns=data[0,1:]))

     Col1 Col2
Row1    1    2
Row2    3    4

Pay attention to how the code chunks above select elements from the NumPy array to construct the DataFrame: you first select the values that are contained in the lists that start with Row1 and Row2, then you select the index or row numbers Row1 and Row2 and then the column names Col1 and Col2.

Next, you also see that, in the DataCamp Light chunk above, you printed out a small selection of the data. This works the same as subsetting 2D NumPy arrays: you first indicate the row that you want to look in for your data, then the column. Don’t forget that the indices start at 0! For data in the example above, you go and look in the rows at index 1 to end, and you select all elements that come after index 1. As a result, you end up selecting 1, 2, 3 and 4.

This approach to making DataFrames will be the same for all the structures that DataFrame() can take on as input.

Try it out in the code chunk below:

Remember that the Pandas library has already been imported for you as pd.

# Tao: the following my_2darray is never used
# Take a 2D array as input to your DataFrame 
my_2darray = np.array([[1, 2, 3], [4, 5, 6]])
print(pd.DataFrame(my_2darray))

   0  1  2
0  1  2  3
1  4  5  6

(findcreatedataframefromdictionary)

# Take a dictionary as input to your DataFrame 
my_dict = {1: ['1', '3'], 2: ['1', '2'], 3: ['2', '4']}
print(pd.DataFrame(my_dict))

   1  2  3
0  1  1  2
1  3  2  4

# Take a DataFrame as input to your DataFrame 
my_df = pd.DataFrame(data=[4,5,6,7], index=range(0,4), columns=['A'])
print(pd.DataFrame(my_df))

   A
0  4
1  5
2  6
3  7

# Take a Series as input to your DataFrame
my_series = pd.Series({"United Kingdom":"London", "India":"New Delhi", "United States":"Washington", "Belgium":"Brussels"})
print(pd.DataFrame(my_series))

                         0
Belgium           Brussels
India            New Delhi
United Kingdom      London
United States   Washington

Note that the index of your Series (and DataFrame) contains the keys of the original dictionary, but that they are sorted: Belgium will be the index at 0, while the United States will be the index at 3.

After you have created your DataFrame, you might want to know a little bit more about it. You can use the shape property or the len() function in combination with the .index property:

df = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6]]))

df

	0	1	2
0	1	2	3
1	4	5	6

(findshapepandas)
# Use the `shape` property
df.shape

(2, 3)

(findlenpandas)
# Or use the `len()` function with the `index` property
len(df.index)

2

len(df)

2

These two options give you slightly different information on your DataFrame: the shape property will provide you with the dimensions of your DataFrame. That means that you will get to know the width and the height of your DataFrame. On the other hand, the len() function, in combination with the index property, will only give you information on the height of your DataFrame.

This all is totally not extraordinary, though, as you explicitly give in the index property.

(findcountpandas)

df[0].count()

2

You could also use df[0].count() to get to know more about the height of your DataFrame, but this will exclude the NaN values (if there are any). That is why calling .count() on your DataFrame is not always the better option.

list(df.columns.values)

[0, 1, 2]

If you want more information on your DataFrame columns, you can always execute list(my_dataframe.columns.values). Try this out for yourself in the DataCamp Light block above!

Fundamental DataFrame Operations

Now that you have put your data in a more convenient Pandas DataFrame structure, it’s time to get to the real work!

This first section will guide you through the first steps of working with DataFrames in Python. It will cover the basic operations that you can do on your newly created DataFrame: adding, selecting, deleting, renaming, … You name it!

2. How To Select an Index or Column From a Pandas DataFrame

Before you start with adding, deleting and renaming the components of your DataFrame, you first need to know how you can select these elements. So, how do you do this?

Even though you might still remember how to do it from the previous section: selecting an index, column or value from your DataFrame isn’t that hard, quite the contrary. It’s similar to what you see in other languages (or packages!) that are used for data analysis. If you aren’t convinced, consider the following:

In R, you use the [,] notation to access the data frame’s values.

Now, let’s say you have a DataFrame like this one:

df_dict = {'A': [1, 4, 7], 'B': [2, 5, 8], 'C': [3, 6, 9]}
df = pd.DataFrame(df_dict)
df

	A	B	C
0	1	2	3
1	4	5	6
2	7	8	9

And you want to access the value that is at index 0, in column ‘A’.

Various options exist to get your value 1 back:

# Tao: I have more detailed and compact notes for iloc elsewhere, search for "find ilocpandas"
# Using `iloc[]`
df.iloc[0][0]

1

# Tao: I have more detailed and compact notes for loc elsewhere, search for "find locpandas"
# Using `loc[]`
df.loc[0]['A']

1

# Using `at[]`
df.at[0,'A']

1

# Using `iat[]`
df.iat[0,0]

1

The most important ones to remember are, without a doubt, .loc[] and .iloc[]. The subtle differences between these two will be discussed in the next sections.

Enough for now about selecting values from your DataFrame. What about selecting rows and columns? In that case, you would use:

# Use `iloc[]` to select a row
df.iloc[0]

A    1
B    2
C    3
Name: 0, dtype: int64

# Use `loc[]` to select a column
df.loc[:,'A']

0    1
1    4
2    7
Name: A, dtype: int64

For now, it’s enough to know that you can either access the values by calling them by their label or by their position in the index or column. If you don’t see this, look again at the slight differences in the commands: one time, you see [0][0], the other time, you see [0,'A'] to retrieve your value 1.

3. How To Add an Index, Row or Column to a Pandas DataFrame

Now that you have learned how to select a value from a DataFrame, it’s time to get to the real work and add an index, row or column to it!

Adding an Index to a DataFrame

(findindexpandas)
When you create a DataFrame, you have the option to add input to the ‘index’ argument to make sure that you have the index that you desire. When you don’t specify this, your DataFrame will have, by default, a numerically valued index that starts with 0 and continues until the last row of your DataFrame.

(findsetindexpandas)
However, even when your index is specified for you automatically, you still have the power to re-use one of your columns and make it your index. You can easily do this by calling set_index() on your DataFrame. Try this out below!

# Print out your DataFrame `df` to check it out
print(df)

   A  B  C
0  1  2  3
1  4  5  6

# Set 'C' as the index of your DataFrame
df.set_index('C')

   A  B
C      
3  1  2
6  4  5

Adding Rows to a DataFrame

Before you can get to the solution, it’s first a good idea to grasp the concept of loc and how it differs from other indexing attributes such as .iloc[] and .ix[]:

- .loc[] works on labels of your index. This means that if you give in loc[2], you look for the values of your DataFrame that have an index labeled 2.

- .iloc[] works on the positions in your index. This means that if you give in iloc[2], you look for the values of your DataFrame that are at index ’2`.

- .ix[] is a more complex case: when the index is integer-based, you pass a label to .ix[]. ix[2] then means that you’re looking in your DataFrame for values that have an index labeled 2. This is just like .loc[]! However, if your index is not solely integer-based, ix will work with positions, just like .iloc[].

This all might seem very complicated. Let’s illustrate all of this with a small example:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2, 'A', 4], columns=[48, 49, 50])

df:

   48  49  50
2   1   2   3
A   4   5   6
4   7   8   9

# Pass `2` to `loc`
df.loc[2]

48    1
49    2
50    3
Name: 2, dtype: int64

# Pass `2` to `iloc`
df.iloc[2]

48    7
49    8
50    9
Name: 4, dtype: int64

# Pass `2` to `ix`
print(df.ix[2])

48    7
49    8
50    9
Name: 4, dtype: int64

Note that in this case, you used an example of a DataFrame that is not solely integer-based as to make it easier for you to understand the differences. You clearly see that passing 2 to .loc[] or .iloc[]/.ix[] does not give back the same result!

- You know that .loc[] will go and look at the values that are at label 2. The result that you get back will be

48    1
49    2
50    3

- You also know that .iloc[] will go and look at the positions in the index. When you pass 2, you will get back:

48    7
49    8
50    9

- Since the index doesn’t only contain integers, .ix[] will have the same behavior as iloc and look at the positions in the index. You will get back the same result as .iloc[].

Now that the difference between .iloc[], .loc[] and .ix[] is clear, you are ready to give adding rows to your DataFrame a go!

Tip: as a consequence of what you have just read, you understand now also that the general recommendation is that you use .loc to insert rows in your DataFrame. That is because if you would use df.ix[], you might try to reference a numerically valued index with the index value and accidentally overwrite an existing row of your DataFrame. You better avoid this!

Check out the difference once more in the DataFrame below:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2.5, 12.6, 4.8], columns=[48, 49, 50])

df

	  48 49	50
2.5	   1  2	 3
12.6   4  5	 6
4.8	   7  8	 9

# There's no index labeled `2`, so you will change the index at position `2`
df.ix[2] = [60, 50, 40]
print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8   60  50  40

(findaddrowpandas)   
# This will make an index labeled `2` and add the new values
df.loc[2] = [11, 12, 13]
print(df)

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8   60  50  40
2.0   11  12  13

You can see why all of this can be confusing, right?

(findaddcolumnpandas)
Adding a Column to Your DataFrame

In some cases, you want to make your index part of your DataFrame. You can easily do this by taking a column from your DataFrame or by referring to a column that you haven’t made yet and assigning it to the .index property, just like this:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Use `.index`
df['D'] = df.index

# Print `df`
df

   A  B  C  D
0  1  2  3  0
1  4  5  6  1
2  7  8  9  2

In other words, you tell your DataFrame that it should take column A as its index.

However, if you want to append columns to your DataFrame, you could also follow the same approach as when you would add an index to your DataFrame: you use .loc[] or .iloc[]. In this case, you add a Series to an existing DataFrame with the help of .loc[]:

# Study the DataFrame `df`
dic = {1: [1, 3], 2: [1, 2], 3: [2, 4]}
df = pd.DataFrame(dic)
df

   1  2  3
0  1  1  2
1  3  2  4

# Append a column to `df`
df.loc[:, 4] = pd.Series(['5', '6'], index=df.index)

# Print out `df` again to see the changes
df

   1  2  3  4
0  1  1  2  5
1  3  2  4  6

Remember a Series object is much like a column of a DataFrame. That explains why you can easily add a Series to an existing DataFrame. Note also that the observation that was made earlier about .loc[] still stays valid, even when you’re adding columns to your DataFrame!

(findresetindexpandas)
Resetting the Index of Your DataFrame

When your index doesn’t look entirely the way you want it to, you can opt to reset it. You can easily do this with .reset_index(). However, you should still watch out, as you can pass several arguments that can make or break the success of your reset:

# Check out the weird index of your dataframe
df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2.5, 12.6, 4.8], columns=[48, 49, 50])

df

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8    7   8   9

# Use `reset_index()` to reset the values
df_reset = df.reset_index(level=0, drop=True)

# Print `df_reset`
df_reset

   48  49  50
0   1   2   3
1   4   5   6
2   7   8   9

Now try replacing the drop argument by inplace in the code chunk above and see what happens!

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), index= [2.5, 12.6, 4.8], columns=[48, 49, 50])

df

      48  49  50
2.5    1   2   3
12.6   4   5   6
4.8    7   8   9

df.reset_index(level=0, inplace=True)

df

   level_0  index  48  49  50
0        0    2.5   1   2   3
1        1   12.6   4   5   6
2        2    4.8   7   8   9

Note how you use the drop argument to indicate that you want to get rid of the index that was there. If you would have used inplace, the original index with floats is added as an extra column to your DataFrame.

4. How to Delete Indices, Rows or Columns From a Pandas Data Frame

Now that you have seen how to select and add indices, rows, and columns to your DataFrame, it’s time to consider another use case: removing these three from your data structure.

Deleting an Index from Your DataFrame

If you want to remove the index from your DataFrame, you should reconsider because DataFrames and Series always have an index.

However, what you *can* do is, for example:

- resetting the index of your DataFrame (go back to the previous section to see how it is done) or

- remove the index name, if there is any, by executing del df.index.name,

(finddropduplicatespandas)
- remove duplicate index values by resetting the index, dropping the duplicates of the index column that has been added to your DataFrame and reinstating that duplicateless column again as the index:

df = pd.DataFrame(data=np.array([[1, 2, 3], [1, 5, 6], [7, 8, 9], [40, 50, 60], [23, 35, 37]]), 
                  index= [2.5, 12.6, 4.8, 4.8, 2.5], 
                  columns=[48, 49, 50])

df

      48  49  50
2.5    1   2   3
12.6   1   5   6
4.8    7   8   9
4.8   40  50  60
2.5   23  35  37

df.reset_index().drop_duplicates(subset='index', keep='last').set_index('index')

       48  49  50
index            
12.6    1   5   6
4.8    40  50  60
2.5    23  35  37

df.reset_index().drop_duplicates(subset=48, keep='last').set_index('index')

       48  49  50
index            
12.6    1   5   6
4.8     7   8   9
4.8    40  50  60
2.5    23  35  37

- and lastly, remove an index, and with it a row. This is elaborated further on in this tutorial.

Now that you know how to remove an index from your DataFrame, you can go on to removing columns and rows!

Deleting a Column from Your DataFrame

To get rid of (a selection of) columns from your DataFrame, you can use the drop() method:


# Check out the DataFrame `df`
df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

(finddroppandas)
(finddropcolumnpandas)
(finddroprowpandas)

(findaxispandas)
Tao: from below, we see that axis=0 means row, axis=1 means column.

# Drop the column with label 'A'                  
df.drop('A', axis=1, inplace=True)

df

   B  C
0  2  3
1  5  6
2  8  9

# Drop the column at position 1
print(df.drop(df.columns[[1]], axis=1))

   B
0  2
1  5
2  8

print(df)

   B  C
0  2  3
1  5  6
2  8  9

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

df.drop(2, axis=0, inplace=True)
df

   A  B  C
0  1  2  3
1  4  5  6

You might think now: well, this is not so straightforward; There are some extra arguments that are passed to the drop() method!

- The axis argument is either 0 when it indicates rows and 1 when it is used to drop columns.

- You can set inplace to True to delete the column without having to reassign the DataFrame.

Removing a Row from Your DataFrame

You can remove duplicate rows from your DataFrame by executing df.drop_duplicates(). You can also remove rows from your DataFrame, taking into account only the duplicate values that exist in one column.

Check out this example:

# Check out your DataFrame `df`
print(df)

      48  49  50  50
2.5    1   2   3   4
12.6   4   5   6   5
4.8    7   8   9   6
4.8   23  50  60   7
2.5   23  35  37  23

# Drop the duplicates in `df`
print(df.drop_duplicates([48], keep='last'))

      48  49  50  50
2.5    1   2   3   4
12.6   4   5   6   5
4.8    7   8   9   6
2.5   23  35  37  23

If there is no uniqueness criterion to the deletion that you want to perform, you can use the drop() method, where you use the index property to specify the index of which rows you want to remove from your DataFrame:

# Check out the DataFrame `df`
df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Drop the index at position 1
df.drop(df.index[1])

   A  B  C
0  1  2  3
2  7  8  9

After this command, you might want to reset the index again.

Tip: try resetting the index of the resulting DataFrame for yourself! Don’t forget to use the drop argument if you deem it necessary.

5. How to Rename the Index or Columns of a Pandas DataFrame

To give the columns or your index values of your dataframe a different value, it’s best to use the .rename() method.

# Check out your DataFrame `df`
df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

(findrenamecolumnpandas)

# Define the new names of your columns
newcols = {
    'A': 'new_column_1', 
    'B': 'new_column_2', 
    'C': 'new_column_3'
}

# Use `rename()` to rename your columns
df.rename(columns=newcols, inplace=True)

df

   new_column_1  new_column_2  new_column_3
0             1             2             3
1             4             5             6
2             7             8             9

# Rename your index
df.rename(index={1: 'a'})

   new_column_1  new_column_2  new_column_3
0             1             2             3
a             4             5             6
2             7             8             9

Tip: try changing the inplace argument in the first task (renaming your columns) to False and see what the script now renders as a result. You see that now the DataFrame hasn’t been reassigned when renaming the columns. As a result, the second task takes the original DataFrame as input and not the one that you just got back from the first rename() operation.

Beyond The Pandas DataFrame Basics

Now that you have gone through a first set of questions about Pandas’ DataFrames, it’s time to go beyond the basics and get your hands dirty for real because there is far more to DataFrames than what you have seen in the first section.

6. How To Format The Data in Your Pandas DataFrame

Most of the times, you will also want to be able to do some operations on the actual values that are contained within your DataFrame. In the following sections, you’ll cover several ways in which you can format your DataFrame’s values

Replacing All Occurrences of a String in a DataFrame
To replace certain strings in your DataFrame, you can easily use replace(): pass the values that you would like to change, followed by the values you want to replace them by.

Just like this:

# Study the DataFrame `df` first

dic = {'Student1': ['OK', 'Awful', 'Acceptable'], 'Student2': ['Perfect', 'Awful', 'OK'], 'Student3': ['Acceptable', 'Perfect', 'Poor']}

df = pd.DataFrame(dic)

df

     Student1 Student2    Student3
0          OK  Perfect  Acceptable
1       Awful    Awful     Perfect
2  Acceptable       OK        Poor

(findreplacepandas)

# Replace the strings by numerical values (0-4)
df.replace(['Awful', 'Poor', 'OK', 'Acceptable', 'Perfect'], [0, 1, 2, 3, 4])

   Student1  Student2  Student3
0         2         4         3
1         0         0         4
2         3         2         1

Note that there is also a regex argument that can help you out tremendously when you’re faced with strange string combinations:

# Check out your DataFrame `df`
print(df)

     0    1    2
0  1\n    2  3\n
1    4    5  6\n
2    7  8\n    9

# Replace strings by others with `regex`
print(df.replace({'\n': '<br>'}, regex=True))

       0      1      2
0  1<br>      2  3<br>
1      4      5  6<br>
2      7  8<br>      9

In short, replace() is mostly what you need to deal with when you want to replace values or strings in your DataFrame by others!

Removing Parts From Strings in the Cells of Your DataFrame
Removing unwanted parts of strings is cumbersome work. Luckily, there is an easy solution to this problem!

# Check out your DataFrame
dic = {'class': [1, 4, 7], 'test': [2, 5, 8], 'result': ['+3b', '-6B', '+9A']}

df = pd.DataFrame(dic)

df

  class test result
0     1    2    +3b
1     4    5    -6B
2     7    8    +9A

# Tao: the udf or lambda or map below is recorded elsewhere
# Delete unwanted parts from the strings in the `result` column
df['result'] = df['result'].map(lambda x: x.lstrip('+-').rstrip('aAbBcC'))

# Check out the result again
df

  class test result
0     1    2      3
1     4    5      6
2     7    8      9

You use map() on the column result to apply the lambda function over each element or element-wise of the column. The function in itself takes the string value and strips the + or - that’s located on the left, and also strips away any of the six aAbBcC on the right.

Splitting Text in a Column into Multiple Rows in a DataFrame

This is somewhat a more difficult formatting task. However, the next code chunk will walk you through the steps:

# Inspect your DataFrame `df`
df

  Age PlusOne             Ticket
0  34       0           23:44:55
1  22       0           66:77:88
2  19       1  43:68:05 56:34:12

# Split out the two values in the third row
# Make it a Series
# Stack the values

# Tao: see explanation of this line below
ticket_series = df['Ticket'].str.split(' ').apply(pd.Series, 1).stack()
ticketdf = pd.DataFrame(ticket_series)

ticketdf

            0
0 0  23:44:55
1 0  66:77:88
2 0  43:68:05
  1  56:34:12

# Get rid of the stack:
# Drop the level to line up with the DataFrame
ticket_series.index = ticket_series.index.droplevel(-1)
ticketdf = pd.DataFrame(ticket_series)

print(ticketdf)

          0
0  23:44:55
1  66:77:88
2  43:68:05
2  56:34:12

(findremovecolumnpandas)
(finddeletecolumnpandas)
# Delete the `Ticket` column from your DataFrame
del df['Ticket']

df

   Age  PlusOne
0   34        0
1   22        0
2   19        1

# Join the ticket DataFrame to `df`
df.join(ticketdf)

   Age  PlusOne         0
0   34        0  23:44:55
1   22        0  66:77:88
2   19        1  43:68:05
2   19        1  56:34:12

In short, what you do is:

- First, you inspect the DataFrame at hand. You see that the values in the last row and in the last column are a bit too long. It appears there are two tickets because a guest has taken a plus-one to the concert.

- You take the Ticket column from the DataFrame df and strings on a space. This will make sure that the two tickets will end up in two separate rows in the end. Next, you take these four values (the four ticket numbers) and put them into a Series object:

      0            1
0  23:44:55       NaN
1  66:77:88       NaN
2  43:68:05  56:34:12

That still doesn’t seem quite right. You have NaN values in there! You have to stack the Series to make sure you don’t have any NaN values in the resulting Series.

- Next, you see that your Series is stacked.

0  0    23:44:55
1  0    66:77:88
2  0    43:68:05
   1    56:34:12

That is not ideal either. That is why you drop the level to line up with the DataFrame:

0    23:44:55
1    66:77:88
2    43:68:05
2    56:34:12
dtype: object

That is what you’re looking for.

- Transform your Series to a DataFrame to make sure you can join it back to your initial DataFrame. However, to avoid having any duplicates in your DataFrame, you can delete the original Ticket column.

Applying A Function to Your Pandas DataFrame’s Columns or Rows

You might want to adjust the data in your DataFrame by applying a function to it. Let’s begin answering this question by making your own lambda function:

(findudfpandas)
(findlambdapandas)
(findapplypandas)

doubler = lambda x: x*2

Tip: if you want to know more about functions in Python, consider taking this Python functions tutorial.

# Study the `df` DataFrame
df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

# Apply the `doubler` function to the `A` DataFrame column
print(df['A'].apply(doubler))

0     2
1     8
2    14
Name: A, dtype: int64

Note that you can also select the row of your DataFrame and apply the doubler lambda function to it. Remember that you can easily select a row from your DataFrame by using .loc[] or .iloc[].

Then, you would execute something like this, depending on whether you want to select your index based on its position or based on its label:

df.loc[0].apply(doubler)

Note that the apply() function only applies the doubler function along the axis of your DataFrame. That means that you target either the index or the columns. Or, in other words, either a row or a column.

(findmappandas)
However, if you want to apply it to each element or element-wise, you can make use of the map() function. You can just replace the apply() function in the code chunk above with map(). Don’t forget to still pass the doubler function to it to make sure you multiply the values by 2.

(findapplymappandas)
Let’s say you want to apply this doubling function not only to the A column of your DataFrame but to the whole of it. In this case, you can use applymap() to apply the doubler function to every single element in the entire DataFrame:

doubled_df = df.applymap(doubler)
doubled_df

    A   B   C
0   2   4   6
1   8  10  12
2  14  16  18

Note that in these cases, we have been working with lambda functions or anonymous functions that get created at runtime. However, you can also write your own function. For example:

def doubler(x):
    if x % 2 == 0:
        return x
    else:
        return x * 2

# Use `applymap()` to apply `doubler()` to your DataFrame
doubled_df = df.applymap(doubler)

# Check the DataFrame
doubled_df

    A   B   C
0   2   2   6
1   4  10   6
2  14   8  18

If you want more information on the flow of control in Python, you can always read up on it here.

7. How To Create an Empty DataFrame

The function that you will use is the Pandas Dataframe() function: it requires you to pass the data that you want to put in, the indices and the columns.

Remember that the data that is contained within the data frame doesn’t have to be homogenous. It can be of different data types!

There are several ways in which you can use this function to make an empty DataFrame. Firstly, you can use numpy.nan to initialize your data frame with NaNs. Note that numpy.nan has type float.

df = pd.DataFrame(np.nan, index=[0,1,2,3], columns=['A'])
print(df)

    A
0 NaN
1 NaN
2 NaN
3 NaN

Right now, the data type of the data frame is inferred by default: because numpy.nan has type float, the data frame will also contain values of type float. You can, however, also force the DataFrame to be of a particular type by adding the attribute dtype and filling in the desired type. Just like in this example:

df = pd.DataFrame(index=range(0,4),columns=['A'], dtype='float')
print(df)

    A
0 NaN
1 NaN
2 NaN
3 NaN

Note that if you don’t specify the axis labels or index, they will be constructed from the input data based on common sense rules.

(finddatecsvpandas)
8. Does Pandas Recognize Dates When Importing Data?

Pandas can recognize it, but you need to help it a tiny bit: add the argument parse_dates when you’reading in data from, let’s say, a comma-separated value (CSV) file:

import pandas as pd
pd.read_csv('yourFile', parse_dates=True)

# or this option:
pd.read_csv('yourFile', parse_dates=['columnName'])

There are, however, always weird date-time formats.

No worries! In such cases, you can construct your own parser to deal with this. You could, for example, make a lambda function that takes your DateTime and controls it with a format string.

import pandas as pd
dateparser = lambda x: pd.datetime.strptime(x, '%Y-%m-%d %H:%M:%S')

# Which makes your read command:
pd.read_csv(infile, parse_dates=['columnName'], date_parser=dateparse)

# Or combine two columns into a single DateTime column
pd.read_csv(infile, parse_dates={'datetime': ['date', 'time']}, date_parser=dateparse)

9. When, Why And How You Should Reshape Your Pandas DataFrame

Reshaping your DataFrame is transforming it so that the resulting structure makes it more suitable for your data analysis. In other words, reshaping is not so much concerned with formatting the values that are contained within the DataFrame, but more about transforming the shape of it.

This answers the when and why. But how would you reshape your DataFrame?

There are three ways of reshaping that frequently raise questions with users: pivoting, stacking and unstacking and melting.

Pivotting Your DataFrame

You can use the pivot() function to create a new derived table out of your original one. When you use the function, you can pass three arguments:

values: this argument allows you to specify which values of your original DataFrame you want to see in your pivot table.

columns: whatever you pass to this argument will become a column in your resulting table.

index: whatever you pass to this argument will become an index in your resulting table.

# Import pandas
import pandas as pd

# Create your DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
        'price':[11.42, 23.50, 19.99, 15.95, 55.75, 111.55],
        'testscore': [4, 3, 5, 7, 5, 8]})

products

        category   price    store  testscore
0       Cleaning   11.42  Walmart          4
1       Cleaning   23.50      Dia          3
2  Entertainment   19.99  Walmart          5
3  Entertainment   15.95     Fnac          7
4           Tech   55.75      Dia          5
5           Tech  111.55  Walmart          8

(findpivotpandas)
(findreshapepandas)
(findlongtowidepandas)
# Use `pivot()` to pivot the DataFrame
# Tao: what does the following line do? First it discards the column testscore. Then it transforms from long to wide. category becomes rows and store becomes columns. price is the values.
pivot_products = products.pivot(index='category', columns='store', values='price')

# Check out the result
pivot_products

          store  Dia     Fnac   Walmart
     category                            
     Cleaning    23.50   NaN    11.42
Entertainment    NaN     15.95  19.99
         Tech    55.75   NaN    111.55

When you don’t specifically fill in what values you expect to be present in your resulting table, you will pivot by multiple columns:

# Import the Pandas library
import pandas as pd

# Construct the DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
                        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
                        'price':[11.42, 23.50, 19.99, 15.95, 55.75, 111.55],
                        'testscore': [4, 3, 5, 7, 5, 8]})

products

            category   price    store  testscore
    0       Cleaning   11.42  Walmart          4
    1       Cleaning   23.50      Dia          3
    2  Entertainment   19.99  Walmart          5
    3  Entertainment   15.95     Fnac          7
    4           Tech   55.75      Dia          5
    5           Tech  111.55  Walmart          8

# Use `pivot()` to pivot your DataFrame
# Tao: in the following result, the long table is reshpaed to wide table for price, which is the left half of the result table. The long table is also reshpaed to wide table for testscore, which is the right half of the result table.
pivot_products = products.pivot(index='category', columns='store')

# Check out the results
pivot_products

                 price                    testscore             
store            Dia   Fnac Walmart       Dia Fnac Walmart
category                                                  
Cleaning       23.50    NaN   11.42       3.0  NaN     4.0
Entertainment    NaN  15.95   19.99       NaN  7.0     5.0
Tech           55.75    NaN  111.55       5.0  NaN     8.0

Note that your data can not have rows with duplicate values for the columns that you specify. If this is not the case, you will get an error message. If you can’t ensure the uniqueness of your data, you will want to use the pivot_table method instead:

# Import the Pandas library
import pandas as pd

# Your DataFrame
products = pd.DataFrame({'category': ['Cleaning', 'Cleaning', 'Entertainment', 'Entertainment', 'Tech', 'Tech'],
                        'store': ['Walmart', 'Dia', 'Walmart', 'Fnac', 'Dia','Walmart'],
                        'price':[11.42, 23.50, 19.99, 15.95, 19.99, 111.55],
                        'testscore': [4, 3, 5, 7, 5, 8]})

products

        category   price    store  testscore
0       Cleaning   11.42  Walmart          4
1       Cleaning   23.50      Dia          3
2  Entertainment   19.99  Walmart          5
3  Entertainment   15.95     Fnac          7
4           Tech   19.99      Dia          5
5           Tech  111.55  Walmart          8

# Pivot your `products` DataFrame with `pivot_table()`
pivot_products = products.pivot_table(index='category', columns='store', values='price', aggfunc='mean')

# Check out the results
pivot_products

store            Dia   Fnac  Walmart
category                            
Cleaning       23.50    NaN    11.42
Entertainment    NaN  15.95    19.99
Tech           19.99    NaN   111.55

Note the additional argument aggfunc that gets passed to the pivot_table method. This argument indicates that you use an aggregation function used to combine multiple values. In this example, you can clearly see that the mean function is used.

Using stack() and unstack() to Reshape Your Pandas DataFrame

You have already seen an example of stacking in the answer to question 5! In essence, you might still remember that, when you stack a DataFrame, you make it taller. You move the innermost column index to become the innermost row index. You return a DataFrame with an index with a new inner-most level of row labels.

Go back to the full walk-through of the answer to question 5 if you’re unsure of the workings of stack().
The inverse of stacking is called unstacking. Much like stack(), you use unstack() to move the innermost row index to become the innermost column index.

For a good explanation of pivoting, stacking and unstacking, go to this page.

Reshape Your DataFrame With melt()

Melting is considered useful in cases where you have data that has one or more columns that are identifier variables, while all other columns are considered measured variables.

These measured variables are all “unpivoted” to the row axis. That is, while the measured variables that were spread out over the width of the DataFrame, the melt will make sure that they will be placed in the height of it. Or, yet in other words, your DataFrame will now become longer instead of wider.

As a result, you have two non-identifier columns, namely, ‘variable’ and ‘value’.

Let’s illustrate this with an example:

(findmeltpandas)
(findwidetolongpandas)
# Tao: in the following example, melt does the opposite of pivot: wide to long.
# The `people` DataFrame
people = pd.DataFrame({'FirstName' : ['John', 'Jane'],
                       'LastName' : ['Doe', 'Austen'],
                       'BloodType' : ['A-', 'B+'],
                       'Weight' : [90, 64]})

people

  FirstName LastName BloodType  Weight
0      John      Doe        A-      90
1      Jane   Austen        B+      64

# Use `melt()` on the `people` DataFrame
print(pd.melt(people, id_vars=['FirstName', 'LastName'], var_name='measurements'))  

  FirstName LastName measurements value
0      John      Doe    BloodType    A-
1      Jane   Austen    BloodType    B+
2      John      Doe       Weight    90
3      Jane   Austen       Weight    64

If you’re looking for more ways to reshape your data, check out the documentation.

(finditeraterowspandas)
(finditerrowspandas)
10. How To Iterate Over a Pandas DataFrame

You can iterate over the rows of your DataFrame with the help of a for loop in combination with an iterrows() call on your DataFrame:

df = pd.DataFrame(data=np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['A', 'B', 'C'])

df

   A  B  C
0  1  2  3
1  4  5  6
2  7  8  9

for index, row in df.iterrows() :
    print(row['A'], row['B'])

1 2
4 5
7 8

iterrows() allows you to efficiently loop over your DataFrame rows as (index, Series) pairs. In other words, it gives you (index, row) tuples as a result.

11. How To Write a Pandas DataFrame to a File

When you have done your data munging and manipulation with Pandas, you might want to export the DataFrame to another format. This section will cover two ways of outputting your DataFrame: to a CSV or to an Excel file.

Output a DataFrame to CSV

To write a DataFrame as a CSV file, you can use to_csv():

(findsavecsvpandas)
(findtocsvpandas)
import pandas as pd
df.to_csv('myDataFrame.csv')

That piece of code seems quite simple, but this is just where the difficulties begin for most people because you will have specific requirements for the output of your data. Maybe you don’t want a comma as a delimiter, or you want to specify a specific encoding, …

Don’t worry! You can pass some additional arguments to to_csv() to make sure that your data is outputted the way you want it to be!

- To delimit by a tab, use the sep argument:

import pandas as pd
df.to_csv('myDataFrame.csv', sep='\t')

- To use a specific character encoding, you can use the encoding argument:

import pandas as pd
df.to_csv('myDataFrame.csv', sep='\t', encoding='utf-8')

- Furthermore, you can specify how you want your NaN or missing values to be represented, whether or not you want to output the header, whether or not you want to write out the row names, whether you want compression, … Read up on the options here.

Writing a DataFrame to Excel

Similarly to what you did to output your DataFrame to CSV, you can use to_excel() to write your table to Excel. However, it is a bit more complicated:

import pandas as pd
writer = pd.ExcelWriter('myDataFrame.xlsx')
df.to_excel(writer, 'DataFrame')
writer.save()

Note, however, that, just like with to_csv(), you have a lot of extra arguments such as startcol, startrow, and so on, to make sure output your data correctly. Go to this page to read up on them.

If, however, you want more information on IO tools in Pandas, you check out this page.

Python For Data Science Is More Than DataFrames

That’s it! You've successfully completed the Pandas DataFrame tutorial!

The answers to the 11 frequently asked Pandas questions represent essential functions that you will need to import, clean and manipulate your data for your data science work. Are you not sure that you have gone deep enough into this matter? Our Importing Data In Python course will help you out! If you’ve got the hang out of this, you might want to see Pandas at work in a real-life project. The Importance of Preprocessing in Data Science and the Machine Learning Pipeline tutorial series is a must-read, and the open course Introduction to Python & Machine Learning is a must-complete.

--
From 
https://www.hackerearth.com/practice/machine-learning/data-manipulation-visualisation-r-python/tutorial-data-manipulation-numpy-pandas-python/tutorial/

Let's move on to pandas now. Make sure you following each line below because it'll help you in doing data manipulation using pandas.

Let's start with Pandas

#load library - pd is just an alias. I used pd because it's short and literally abbreviates pandas.
#You can use any name as an alias. 
import pandas as pd
#create a data frame - dictionary is used here where keys get converted to column names and values to row values.
data = pd.DataFrame({'Country': ['Russia','Colombia','Chile','Equador','Nigeria'],
                    'Rank':[121,40,100,130,11]})
data
Country	Rank
0	Russia	121
1	Colombia	40
2	Chile	100
3	Equador	130
4	Nigeria	11
#We can do a quick analysis of any data set using:
data.describe()
Rank
count	5.000000
mean	80.400000
std	52.300096
min	11.000000
25%	40.000000
50%	100.000000
75%	121.000000
max	130.000000
Remember, describe() method computes summary statistics of integer / double variables. To get the complete information about the data set, we can use info() function.

#Among other things, it shows the data set has 5 rows and 2 columns with their respective names.
data.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 5 entries, 0 to 4
Data columns (total 2 columns):
Country    5 non-null object
Rank       5 non-null int64
dtypes: int64(1), object(1)
memory usage: 152.0+ bytes


#Let's create another data frame.
data = pd.DataFrame({'group':['a', 'a', 'a', 'b','b', 'b', 'c', 'c','c'],'ounces':[4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
group	ounces
0	a	4.0
1	a	3.0
2	a	12.0
3	b	6.0
4	b	7.5
5	b	8.0
6	c	3.0
7	c	5.0
8	c	6.0
#Let's sort the data frame by ounces - inplace = True will make changes to the data
data.sort_values(by=['ounces'],ascending=True,inplace=False)
group	ounces
1	a	3.0
6	c	3.0
0	a	4.0
7	c	5.0
3	b	6.0
8	c	6.0
4	b	7.5
5	b	8.0
2	a	12.0
We can sort the data by not just one column but multiple columns as well.

data.sort_values(by=['group','ounces'],ascending=[True,False],inplace=False)
group	ounces
2	a	12.0
0	a	4.0
1	a	3.0
5	b	8.0
4	b	7.5
3	b	6.0
8	c	6.0
7	c	5.0
6	c	3.0
Often, we get data sets with duplicate rows, which is nothing but noise. Therefore, before training the model, we need to make sure we get rid of such inconsistencies in the data set. Let's see how we can remove duplicate rows.

#create another data with duplicated rows
data = pd.DataFrame({'k1':['one']*3 + ['two']*4, 'k2':[3,2,1,3,3,4,4]})
data
k1	k2
0	one	3
1	one	2
2	one	1
3	two	3
4	two	3
5	two	4
6	two	4
#sort values 
data.sort_values(by='k2')
k1	k2
2	one	1
1	one	2
0	one	3
3	two	3
4	two	3
5	two	4
6	two	4
#remove duplicates - ta da! 
data.drop_duplicates()
k1	k2
0	one	3
1	one	2
2	one	1
3	two	3
5	two	4
Here, we removed duplicates based on matching row values across all columns. Alternatively, we can also remove duplicates based on a particular column. Let's remove duplicate values from the k1 column.

data.drop_duplicates(subset='k1')
k1	k2
0	one	3
3	two	3
Now, we will learn to categorize rows based on a predefined criteria. It happens a lot while data processing where you need to categorize a variable. For example, say we have got a column with country names and we want to create a new variable 'continent' based on these country names. In such situations, we will require the steps below:

data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami','corned beef', 'Bacon', 'pastrami', 'honey ham','nova lox'],
                 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
food	ounces
0	bacon	4.0
1	pulled pork	3.0
2	bacon	12.0
3	Pastrami	6.0
4	corned beef	7.5
5	Bacon	8.0
6	pastrami	3.0
7	honey ham	5.0
8	nova lox	6.0
Now, we want to create a new variable which indicates the type of animal which acts as the source of the food. To do that, first we'll create a dictionary to map the food to the animals. Then, we'll use map function to map the dictionary's values to the keys. Let's see how is it done.

meat_to_animal = {
'bacon': 'pig',
'pulled pork': 'pig',
'pastrami': 'cow',
'corned beef': 'cow',
'honey ham': 'pig',
'nova lox': 'salmon'
}

def meat_2_animal(series):
    if series['food'] == 'bacon':
        return 'pig'
    elif series['food'] == 'pulled pork':
        return 'pig'
    elif series['food'] == 'pastrami':
        return 'cow'
    elif series['food'] == 'corned beef':
        return 'cow'
    elif series['food'] == 'honey ham':
        return 'pig'
    else:
        return 'salmon'


#create a new variable
data['animal'] = data['food'].map(str.lower).map(meat_to_animal)
data
food	ounces	animal
0	bacon	4.0	pig
1	pulled pork	3.0	pig
2	bacon	12.0	pig
3	Pastrami	6.0	cow
4	corned beef	7.5	cow
5	Bacon	8.0	pig
6	pastrami	3.0	cow
7	honey ham	5.0	pig
8	nova lox	6.0	salmon
#another way of doing it is: convert the food values to the lower case and apply the function
lower = lambda x: x.lower()
data['food'] = data['food'].apply(lower)
data['animal2'] = data.apply(meat_2_animal, axis='columns')
data
food	ounces	animal	animal2
0	bacon	4.0	pig	pig
1	pulled pork	3.0	pig	pig
2	bacon	12.0	pig	pig
3	pastrami	6.0	cow	cow
4	corned beef	7.5	cow	cow
5	bacon	8.0	pig	pig
6	pastrami	3.0	cow	cow
7	honey ham	5.0	pig	pig
8	nova lox	6.0	salmon	salmon
Another way to create a new variable is by using the assign function. With this tutorial, as you keep discovering the new functions, you'll realize how powerful pandas is.

data.assign(new_variable = data['ounces']*10)
food	ounces	animal	animal2	new_variable
0	bacon	4.0	pig	pig	40.0
1	pulled pork	3.0	pig	pig	30.0
2	bacon	12.0	pig	pig	120.0
3	pastrami	6.0	cow	cow	60.0
4	corned beef	7.5	cow	cow	75.0
5	bacon	8.0	pig	pig	80.0
6	pastrami	3.0	cow	cow	30.0
7	honey ham	5.0	pig	pig	50.0
8	nova lox	6.0	salmon	salmon	60.0
Let's remove the column animal2 from our data frame.

data.drop('animal2',axis='columns',inplace=True)
data
food	ounces	animal
0	bacon	4.0	pig
1	pulled pork	3.0	pig
2	bacon	12.0	pig
3	Pastrami	6.0	cow
4	corned beef	7.5	cow
5	Bacon	8.0	pig
6	pastrami	3.0	cow
7	honey ham	5.0	pig
8	nova lox	6.0	salmon
We frequently find missing values in our data set. A quick method for imputing missing values is by filling the missing value with any random number. Not just missing values, you may find lots of outliers in your data set, which might require replacing. Let's see how can we replace values.

#Series function from pandas are used to create arrays
data = pd.Series([1., -999., 2., -999., -1000., 3.])
data
0       1.0
1    -999.0
2       2.0
3    -999.0
4   -1000.0
5       3.0
dtype: float64


#replace -999 with NaN values
data.replace(-999, np.nan,inplace=True)
data
0       1.0
1       NaN
2       2.0
3       NaN
4   -1000.0
5       3.0
dtype: float64


#We can also replace multiple values at once.
data = pd.Series([1., -999., 2., -999., -1000., 3.])
data.replace([-999,-1000],np.nan,inplace=True)
data
0    1.0
1    NaN
2    2.0
3    NaN
4    NaN
5    3.0
dtype: float64
Now, let's learn how to rename column names and axis (row names).

data = pd.DataFrame(np.arange(12).reshape((3, 4)),index=['Ohio', 'Colorado', 'New York'],columns=['one', 'two', 'three', 'four'])
data
one	two	three	four
Ohio	0	1	2	3
Colorado	4	5	6	7
New York	8	9	10	11
#Using rename function
data.rename(index = {'Ohio':'SanF'}, columns={'one':'one_p','two':'two_p'},inplace=True)
data
one_p	two_p	three	four
SanF	0	1	2	3
Colorado	4	5	6	7
New York	8	9	10	11
#You can also use string functions
data.rename(index = str.upper, columns=str.title,inplace=True)
data
One_p	Two_p	Three	Four
SANF	0	1	2	3
COLORADO	4	5	6	7
NEW YORK	8	9	10	11
Next, we'll learn to categorize (bin) continuous variables.

ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
We'll divide the ages into bins such as 18-25, 26-35,36-60 and 60 and above.

#Understand the output - '(' means the value is included in the bin, '[' means the value is excluded
bins = [18, 25, 35, 60, 100]
cats = pd.cut(ages, bins)
cats
[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]]
Length: 12
Categories (4, object): [(18, 25] < (25, 35] < (35, 60] < (60, 100]]


#To include the right bin value, we can do:
pd.cut(ages,bins,right=False)
[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [25, 35), [60, 100), [35, 60), [35, 60), [25, 35)]
Length: 12
Categories (4, object): [[18, 25) < [25, 35) < [35, 60) < [60, 100)]


#pandas library intrinsically assigns an encoding to categorical variables.
cats.labels
array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1], dtype=int8)


#Let's check how many observations fall under each bin
pd.value_counts(cats)
(18, 25]     5
(35, 60]     3
(25, 35]     3
(60, 100]    1
dtype: int64
Also, we can pass a unique name to each label.

bin_names = ['Youth', 'YoungAdult', 'MiddleAge', 'Senior']
new_cats = pd.cut(ages, bins,labels=bin_names)

pd.value_counts(new_cats)
Youth	5
MiddleAge	3
YoungAdult	3
Senior	1
dtype: int64	
#we can also calculate their cumulative sum
pd.value_counts(new_cats).cumsum()
Youth	5
MiddleAge	3
YoungAdult	3
Senior	1
dtype: int64	
Let's proceed and learn about grouping data and creating pivots in pandas. It's an immensely important data analysis method which you'd probably have to use on every data set you work with.

df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],
                   'key2' : ['one', 'two', 'one', 'two', 'one'],
                   'data1' : np.random.randn(5),
                   'data2' : np.random.randn(5)})
df
data1	data2	key1	key2
0	0.973599	0.001761	a
1	0.207283	-0.990160	a
2	1.099642	1.872394	b
3	0.939897	-0.241074	b
4	0.606389	0.053345	a
#calculate the mean of data1 column by key1
grouped = df['data1'].groupby(df['key1'])
grouped.mean()
key1
a    0.595757
b    1.019769
Name: data1, dtype: float64
Now, let's see how to slice the data frame.
dates = pd.date_range('20130101',periods=6)
df = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
df
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
#get first n rows from the data frame
df[:3]
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
#slice based on date range
df['20130101':'20130104']
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
#slicing based on column names
df.loc[:,['A','B']]
A	B
2013-01-01	1.030816	-1.276989
2013-01-02	-1.070215	-0.209129
2013-01-03	1.524227	1.863575
2013-01-04	0.918203	-0.158800
2013-01-05	0.089731	0.114854
2013-01-06	0.222260	0.435183
#slicing based on both row index labels and column names
df.loc['20130102':'20130103',['A','B']]
A	B
2013-01-02	-1.070215	-0.209129
2013-01-03	1.524227	1.863575
#slicing based on index of columns
df.iloc[3] #returns 4th row (index is 3rd)
A    0.918203
B   -0.158800
C   -0.964063
D   -1.990779
Name: 2013-01-04 00:00:00, dtype: float64


#returns a specific range of rows
df.iloc[2:4, 0:2]
A	B
2013-01-03	1.524227	1.863575
2013-01-04	0.918203	-0.158800
#returns specific rows and columns using lists containing columns or row indexes
df.iloc[[1,5],[0,2]] 
A	C
2013-01-02	-1.070215	0.604572
2013-01-06	0.222260	-0.045748
Similarly, we can do Boolean indexing based on column values as well. This helps in filtering a data set based on a pre-defined condition.

df[df.A > 1]
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-03	1.524227	1.863575	1.291378	1.300696
#we can copy the data set
df2 = df.copy()
df2['E']=['one', 'one','two','three','four','three']
df2
A	B	C	D	E
2013-01-01	1.030816	-1.276989	0.837720	-1.490111	one
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058	one
2013-01-03	1.524227	1.863575	1.291378	1.300696	two
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779	three
2013-01-05	0.089731	0.114854	-0.585815	0.298772	four
2013-01-06	0.222260	0.435183	-0.045748	0.049898	three
#select rows based on column values
df2[df2['E'].isin(['two','four'])]
A	B	C	D	E
2013-01-03	1.524227	1.863575	1.291378	1.300696	two
2013-01-05	0.089731	0.114854	-0.585815	0.298772	four
#select all rows except those with two and four
df2[~df2['E'].isin(['two','four'])]
A	B	C	D	E
2013-01-01	1.030816	-1.276989	0.837720	-1.490111	one
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058	one
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779	three
2013-01-06	0.222260	0.435183	-0.045748	0.049898	three
We can also use a query method to select columns based on a criterion. Let's see how!

#list all columns where A is greater than C
df.query('A > C')
A	B	C	D
2013-01-01	1.030816	-1.276989	0.837720	-1.490111
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-04	0.918203	-0.158800	-0.964063	-1.990779
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
#using OR condition
df.query('A < B | C > A')
A	B	C	D
2013-01-02	-1.070215	-0.209129	0.604572	-1.743058
2013-01-03	1.524227	1.863575	1.291378	1.300696
2013-01-05	0.089731	0.114854	-0.585815	0.298772
2013-01-06	0.222260	0.435183	-0.045748	0.049898
Pivot tables are extremely useful in analyzing data using a customized tabular format. I think, among other things, Excel is popular because of the pivot table option. It offers a super-quick way to analyze data.

#create a data frame
data = pd.DataFrame({'group': ['a', 'a', 'a', 'b','b', 'b', 'c', 'c','c'],
                 'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})
data
group	ounces
0	a	4.0
1	a	3.0
2	a	12.0
3	b	6.0
4	b	7.5
5	b	8.0
6	c	3.0
7	c	5.0
8	c	6.0
#calculate means of each group
data.pivot_table(values='ounces',index='group',aggfunc=np.mean)
group
a    6.333333
b    7.166667
c    4.666667
Name: ounces, dtype: float64


#calculate count by each group
data.pivot_table(values='ounces',index='group',aggfunc='count')
group
a    3
b    3
c    3
Name: ounces, dtype: int64
Up till now, we've become familiar with the basics of pandas library using toy examples. Now, we'll take up a real-life data set and use our newly gained knowledge to explore it.

Exploring ML Data Set

We'll work with the popular adult data set.The data set has been taken from UCI Machine Learning Repository. You can download the data from here. In this data set, the dependent variable is "target." It is a binary classification problem. We need to predict if the salary of a given person is less than or more than 50K.

#load the data
train  = pd.read_csv("~/Adult/train.csv")
test = pd.read_csv("~/Adult/test.csv")
#check data set
train.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 32561 entries, 0 to 32560
Data columns (total 15 columns):
age               32561 non-null int64
workclass         30725 non-null object
fnlwgt            32561 non-null int64
education         32561 non-null object
education.num     32561 non-null int64
marital.status    32561 non-null object
occupation        30718 non-null object
relationship      32561 non-null object
race              32561 non-null object
sex               32561 non-null object
capital.gain      32561 non-null int64
capital.loss      32561 non-null int64
hours.per.week    32561 non-null int64
native.country    31978 non-null object
target            32561 non-null object
dtypes: int64(6), object(9)
memory usage: 3.7+ MB
We see that, the train data has 32561 rows and 15 columns. Out of these 15 columns, 6 have integers classes and the rest have object (or character) classes. Similarly, we can check for test data. An alternative way of quickly checking rows and columns is

print ("The train data has",train.shape)
print ("The test data has",test.shape)
('The train data has', (32561, 15))
('The test data has', (16281, 15))


#Let have a glimpse of the data set
train.head()
age	workclass	fnlwgt	education	education.num	marital.status	occupation	relationship	race	sex	capital.gain	capital.loss	hours.per.week	native.country	target
0	39	State-gov	77516	Bachelors	13	Never-married	Adm-clerical	Not-in-family	White	Male	2174	0	40	United-States	<=50K
1	50	Self-emp-not-inc	83311	Bachelors	13	Married-civ-spouse	Exec-managerial	Husband	White	Male	0	0	13	United-States	<=50K
2	38	Private	215646	HS-grad	9	Divorced	Handlers-cleaners	Not-in-family	White	Male	0	0	40	United-States	<=50K
3	53	Private	234721	11th	7	Married-civ-spouse	Handlers-cleaners	Husband	Black	Male	0	0	40	United-States	<=50K
4	28	Private	338409	Bachelors	13	Married-civ-spouse	Prof-specialty	Wife	Black	Female	0	0	40	Cuba	<=50K
Now, let's check the missing values (if present) in this data.

nans = train.shape[0] - train.dropna().shape[0]
print ("%d rows have missing values in the train data" %nans)

nand = test.shape[0] - test.dropna().shape[0]
print ("%d rows have missing values in the test data" %nand)

2399 rows have missing values in the train data
1221 rows have missing values in the test data
We should be more curious to know which columns have missing values.

#only 3 columns have missing values
train.isnull().sum()
age	0
workclass	1836
fnlwgt	0
education	0
education.num	0
marital.status	0
occupation	1843
relationship	0
race	0
sex	0
capital.gain	0
capital.loss	0
hours.per.week	0
native.country	583
target	0
dtype: int64	
Let's count the number of unique values from character variables.

cat = train.select_dtypes(include=['O'])
cat.apply(pd.Series.nunique)
workclass	8
education	16
marital.status	7
occupation	14
relationship	6
race	5
sex	2
native.country	41
target	2
dtype: int64	
Since missing values are found in all 3 character variables, let's impute these missing values with their respective modes.

#Education
train.workclass.value_counts(sort=True)
train.workclass.fillna('Private',inplace=True)


#Occupation
train.occupation.value_counts(sort=True)
train.occupation.fillna('Prof-specialty',inplace=True)


#Native Country
train['native.country'].value_counts(sort=True)
train['native.country'].fillna('United-States',inplace=True)
Let's check again if there are any missing values left.

train.isnull().sum()
age	0
workclass	0
fnlwgt	0
education	0
education.num	0
marital.status	0
occupation	0
relationship	0
race	0
sex	0
capital.gain	0
capital.loss	0
hours.per.week	0
native.country	0
target	0
dtype: int64	
Now, we'll check the target variable to investigate if this data is imbalanced or not.

#check proportion of target variable
train.target.value_counts()/train.shape[0]
<=50K    0.75919
>50K     0.24081
Name: target, dtype: float64
We see that 75% of the data set belongs to <=50K class. This means that even if we take a rough guess of target prediction as <=50K, we'll get 75% accuracy. Isn't that amazing? Let's create a cross tab of the target variable with education. With this, we'll try to understand the influence of education on the target variable.

pd.crosstab(train.education, train.target,margins=True)/train.shape[0]
target	<=50K	>50K	All
education			
10th	0.026750	0.001904	0.028654
11th	0.034243	0.001843	0.036086
12th	0.012285	0.001013	0.013298
1st-4th	0.004975	0.000184	0.005160
5th-6th	0.009736	0.000491	0.010227
7th-8th	0.018611	0.001228	0.019840
9th	0.014957	0.000829	0.015786
Assoc-acdm	0.024631	0.008139	0.032769
Assoc-voc	0.031357	0.011087	0.042443
Bachelors	0.096250	0.068210	0.164461
Doctorate	0.003286	0.009398	0.012684
HS-grad	0.271060	0.051442	0.322502
Masters	0.023464	0.029452	0.052916
Preschool	0.001566	0.000000	0.001566
Prof-school	0.004699	0.012991	0.017690
Some-college	0.181321	0.042597	0.223918
All	0.759190	0.240810	1.000000
We see that out of 75% people with <=50K salary, 27% people are high school graduates, which is correct as people with lower levels of education are expected to earn less. On the other hand, out of 25% people with >=50K salary, 6% are bachelors and 5% are high-school grads. Now, this pattern seems to be a matter of concern. That's why we'll have to consider more variables before coming to a conclusion.

If you've come this far, you might be curious to get a taste of building your first machine learning model. In the coming week we'll share an exclusive tutorial on machine learning in python. However, let's get a taste of it here.

We'll use the famous and formidable scikit learn library. Scikit learn accepts data in numeric format. Now, we'll have to convert the character variable into numeric. We'll use the labelencoder function.

In label encoding, each unique value of a variable gets assigned a number, i.e., let's say a variable color has four values ['red','green','blue','pink'].

Label encoding this variable will return output as: red = 2 green = 0 blue = 1 pink = 3

#load sklearn and encode all object type variables
from sklearn import preprocessing

for x in train.columns:
    if train[x].dtype == 'object':
        lbl = preprocessing.LabelEncoder()
        lbl.fit(list(train[x].values))
        train[x] = lbl.transform(list(train[x].values))
Let's check the changes applied to the data set.

train.head()
age	workclass	fnlwgt	education	education.num	marital.status	occupation	relationship	race	sex	capital.gain	capital.loss	hours.per.week	native.country	target
0	39	6	77516	9	13	4	0	1	4	1	2174	0	40	38	0
1	50	5	83311	9	13	2	3	0	4	1	0	0	13	38	0
2	38	3	215646	11	9	0	5	1	4	1	0	0	40	38	0
3	53	3	234721	1	7	2	5	0	2	1	0	0	40	38	0
4	28	3	338409	9	13	2	9	5	2	0	0	0	40	4	0
As we can see, all the variables have been converted to numeric, including the target variable.

#<50K = 0 and >50K = 1
train.target.value_counts()
0    24720
1     7841
Name: target, dtype: int64
Building a Random Forest Model

Let's create a random forest model and check the model's accuracy.

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_score
from sklearn.metrics import accuracy_score

y = train['target']
del train['target']

X = train
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1,stratify=y)

#train the RF classifier
clf = RandomForestClassifier(n_estimators = 500, max_depth = 6)
clf.fit(X_train,y_train)

    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
                max_depth=6, max_features='auto', max_leaf_nodes=None,
                min_impurity_split=1e-07, min_samples_leaf=1,
                min_samples_split=2, min_weight_fraction_leaf=0.0,
                n_estimators=500, n_jobs=1, oob_score=False, random_state=None,
                verbose=0, warm_start=False)

clf.predict(X_test)
Now, let's make prediction on the test set and check the model's accuracy.

#make prediction and check model's accuracy
prediction = clf.predict(X_test)
acc =  accuracy_score(np.array(y_test),prediction)
print ('The accuracy of Random Forest is {}'.format(acc))
The accuracy of Random Forest is 0.85198075545.
Hurrah! Our learning algorithm gave 85% accuracy. Well, we can do tons of things on this data and improve the accuracy. We'll learn about it in future articles. What's next?

In this tutorial, we divided the train data into two halves and made prediction on the test data. As your exercise, you should use this model and make prediction on the test data we loaded initially. You can perform same set of steps we did on the train data to complete this exercise. In case you face any difficulty, feel free to share it in Comments below.

Summary

This tutorial is meant to help python developers or anyone who's starting with python to get a taste of data manipulation and a little bit of machine learning using python. I'm sure, by now you would be convinced that python is actually very powerful in handling and processing data sets. But, what we learned here is just the tip of the iceberg. Don't get complacent with this knowledge.

To dive deeper in pandas, check its documentation and start exploring. If you get stuck anywhere, you can drop your questions or suggestions in Comments below. Hope you found this tutorial useful.

--
From ml4t:

Pandas: This library was created by Wes McKinney at a hedge fund call AQR. It's used at many hedge funds and by many people in the finance industry. One of the key components of Pandas is something called the dataframe.

--
Use pandas in my Thinkpad computer:

export PATH=~/anaconda2/bin:$PATH
source activate homework1
python file_name.py

--
(findreadcsvpandas)
Read csv file to a dataframe:

df = pd.read_csv("data/AAPL.csv")

Practice shows it will read the data as appropriate data types automatically.

--
Only read in selected columns (col1, col2):

dfSPY = pd.read_csv("data.csv", usecols = ['col1', 'col2'])

--
Read the string 'nan' as not-a-number:

dfSPY = pd.read_csv("data.csv", na_values = ['nan'])

Let's understand that csv 'nan' as string, so we need to tell the read_csv that 'nan' should be interpreted as not a number. Tao: otherwise it will read 'nan' as a string, instead of not-a-number.

--
Make a column to be the index:

dfSPY = pd.read_csv("data.csv", index_col = "date", parse_dates = True)

We make the date column in the csv file as index. We do this by using the index_col parameter. We also want the dates present in the DataFrame to be converted into date time index objects. This can be done by setting the value for the parse_dates parameter to True.

--
(finddropnanpandas)
Drop the rows with NaN:

df2 = df1.dropna()
df2 = df1.dropna(subset = ["col1"]) # Drop only the rows which have col1 equals NaN.

--
(findcreatedataframepandas)
# Create DataFrame:

# Tao: practice shows that d below is just a normal dictionary. The key is string type, and value is a list.

d = {'col1': [1, 2], 'col2': [3, 4]}

df = pd.DataFrame(data=d)

df

   col1  col2
0     1     3
1     2     4

You will also observe there is a column that is not named and has values 0, 1, 2, 3. And this is not from the .csv. These are called index for the data frame, which help you to access rows.

--
Add a new column from computation:

df['col_c'] = 2 * df['col_a'] + df['col_a'] * df['col_b']

--
(findheadpandas)
(findtailpandas)
head & tail

df.head() # Top 5 rows 
df.tail() # Last 5 rows 

df.head(5)
df.tail(3)

--
(findrankpandas)
Rank

df:

           coverage	 name	reports	 year
Cochice	    25	     Jason	4	     2012
Pima	    94	     Molly	24	     2012
Santa Cruz	57	     Tina	31	     2013
Maricopa	62	     Jake	2	     2014
Yuma	    70	     Amy	3	     2014

# Create a new column that is the rank of the value of coverage in ascending order
# Tao: the new df contains all the old columns and the new column

df['coverageRanked'] = df['coverage'].rank(ascending = True)

df:

           coverage	 name	reports	 year  coverageRanked
Cochice	    25	     Jason	4	     2012  1
Pima	    94	     Molly	24	     2012  5
Santa Cruz	57	     Tina	31	     2013  2
Maricopa	62	     Jake	2	     2014  3
Yuma	    70	     Amy	3	     2014  4

--
(findcorrelationpandas)
Correlation

df['A'].corr(df['B'])

--
(findilocpandas)
(findlocpandas)
(findixpandas)
iloc, loc, ix:

Abstract from https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/

There’s three main options to achieve the selection and indexing activities in Pandas, which can be confusing. The three selection cases and methods covered in this post are:

1. Selecting data by row numbers (.iloc)
2. Selecting data by label or by a conditional statment (.loc)
3. Selecting in a hybrid approach (.ix) (now Deprecated in Pandas 0.20.1). The ix[] indexer is a hybrid of .loc and .iloc.

.iloc selections: position based selection:

data.iloc[<row selection], <column selectoin>]
 
row selectoin and column selection: 
iteger list of rows:[0,1,2]  
integer list of columns: [0,1,2]
slice of rows: [4:7]
slice of columns:[4:7]
single values: 1
single column selections: 1

Examples of iloc:

# Single selections using iloc and DataFrame
# Rows:
data.iloc[0] # first row of data frame. Note a Series data type output.
data.iloc[1] # second row of data frame
data.iloc[-1] # last row of data frame
# Columns:
data.iloc[:,0] # first column of data frame 
data.iloc[:,1] # second column of data frame 
data.iloc[:,-1] # last column of data frame

# Multiple row and column selections using iloc and DataFrame
data.iloc[0:5] # first five rows of dataframe
data.iloc[:, 0:2] # first two columns of data frame with all rows
data.iloc[[0,3,6,24], [0,5,6]] # 1st, 4th, 7th, 25th row + 1st 6th 7th columns.
data.iloc[0:5, 5:8] # first 5 rows and 5th, 6th, 7th columns of data frame (county -> phone1).

Note that .iloc returns a Pandas Series when one row is selected, and a Pandas DataFrame when multiple rows are selected, or if any column in full is selected. To counter this, pass a single-valued list if you require DataFrame output.

# Get value from one row:
# From online: If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:

sub_df

          A         B
2 -0.133653 -0.030854

sub_df.iloc[0]

A   -0.133653
B   -0.030854

sub_df.iloc[0]['A']
-0.13365288513107493

--
loc selectoins: position based selection:

data.loc[<row selection], <column selection>]

row selectoin and column selection: 
index/label value: 'john'
named column: 'first_name'
list of labels: ['john', 'sarah']
list of column names: ['first_name', 'age']
logical/boolean index: data['age'] == 10
slice of columns: 'first_name': 'address'

Examples of loc:

# Select rows with index values 'Andrade' and 'Veness', with all columns between 'city' and 'email'
data.loc[['Andrade', 'Veness'], 'city':'email']

# Select same rows, with just 'first_name', 'address' and 'city' columns
data.loc['Andrade':'Veness', ['first_name', 'address', 'city']]
 
# Change the index to be based on the 'id' column
data.set_index('id', inplace=True)

# select the row with 'id' = 487
data.loc[487]

# Select rows with first name Antonio, # and all columns between 'city' and 'email'
data.loc[data['first_name'] == 'Antonio', 'city':'email']
 
# Select rows where the email column ends with 'hotmail.com', include all columns
data.loc[data['email'].str.endswith("hotmail.com")]   
 
# Select rows with last_name equal to some values, all columns
data.loc[data['first_name'].isin(['France', 'Tyisha', 'Eric'])]   
       
# Select rows with first name Antonio AND hotmail email addresses
data.loc[data['email'].str.endswith("gmail.com") & (data['first_name'] == 'Antonio')] 
 
# select rows with id column between 100 and 200, and just return 'postal' and 'web' columns
data.loc[(data['id'] > 100) & (data['id'] <= 200), ['postal', 'web']] 
 
# A lambda function that yields True/False values can also be used.
# Select rows where the company name has 4 words in it.
data.loc[data['company_name'].apply(lambda x: len(x.split(' ')) == 4)] 
 
# Selections can be achieved outside of the main .loc for clarity:
# Form a separate variable with your selections:
idx = data['company_name'].apply(lambda x: len(x.split(' ')) == 4)
# Select only the True values in 'idx' and only the 3 columns specified:
data.loc[idx, ['email', 'first_name', 'company']]

--
(findslicepandas)
(findselectpandas)
Slice dataframe
Select rows and columns

Select rows:

df[10:21] # Data from index 10 to 20, because 21 is not inclusive in the range. This operation is called slicing and it is a very important operation in Python pandas

df.ix[10:21] # Equivalent as df[10:21], just looks more Pythonic and robust.

Select colums:
df['col1'] <- the returned type is not dataframe!
df[['A', 'B', 'C']]: returns a dataframe with columns A, B, C

print df['col1']

--
Select rows and columns:
df[1:5, ['A', 'B']] # Selects rows 1 to 4, columns A and B.
df.ix[1:5, ['A', 'B']] # Equivalent as above

--
df[df.A > 0]

                   A         B         C         D
2013-01-01  0.469112 -0.282863 -1.509059 -1.135632
2013-01-02  1.212112 -0.173215  0.119209 -1.044236
2013-01-04  0.721555 -0.706771 -1.039575  0.271860

--
# select * from a_df
# where label = 100

a_df[a_df.label == 100]

--
(findmaxpandas)
(findstatisticspandas)
Max
Statistics functions

df['col1'].max() 

df.mean() # Return the mean of each column
df.median()
df.std() # Standard deviation

df.kurtosis() 

Kurtosis:

If we’ve got a positive kurtosis, that means we’ve got fat tails, there are more occurrences outside in the tails than would normally happen
with a Gaussian distribution. If negative.... less...

Correlation:
df.corr(method = 'pearson') # df only has two columns, this returns the correlation of the two columns.

--
sqrt, square, power

import numpy as np
df_sqrt = np.sqrt(df['col_a'])
df_square = np.power(df['col_a'], 2)

--
(findlinearregressionpandas)
(findpolyfitpandas)
Linear Regression
Polyfit

import pandas as pd
import numpy as np

beta, alpha = np.polyfit(df['col1'], df['col2'], 1) 

1 means polynomial degree is 1 (linear).
So the line is y = beta * x + alpha

--
(findsortpandas)
Sort

df.sort_index(axis=1, ascending=False)

                   D         C         B         A
2013-01-01 -1.135632 -1.509059 -0.282863  0.469112
2013-01-02 -1.044236  0.119209 -0.173215  1.212112
2013-01-03  1.071804 -0.494929 -2.104569 -0.861849
2013-01-04  0.271860 -1.039575 -0.706771  0.721555
2013-01-05 -1.087401  0.276232  0.567020 -0.424972
2013-01-06  0.524988 -1.478427  0.113648 -0.673690

--
df.sort_values(by='B')

                   A         B         C         D
2013-01-03 -0.861849 -2.104569 -0.494929  1.071804
2013-01-04  0.721555 -0.706771 -1.039575  0.271860
2013-01-01  0.469112 -0.282863 -1.509059 -1.135632
2013-01-02  1.212112 -0.173215  0.119209 -1.044236
2013-01-06 -0.673690  0.113648 -1.478427  0.524988
2013-01-05 -0.424972  0.567020  0.276232 -1.087401


--
(findisnullpandas)
(findmissingvaluespandas)
isnull or missing values

df['A'].isnull().sum(): returns number of missing values in column A.

Fill the missing data:

df.fillna()

df.fillna(method = 'ffill') # Forwad fill, ie, fill in the last, previous known value. <- Think of stock curve (price vs time).

df.fillna(method = 'bfill') # Backward fill.

df.fillna(method="ffill", inplace=True)

--
(findgroupbypandas)
group by

# Now a new df:

df

     A      B         C         D
0  foo    one -1.202872 -0.055224
1  bar    one -1.814470  2.395985
2  foo    two  1.018601  1.552825
3  bar  three -0.595447  0.166599
4  foo    two  1.395433  0.047609
5  bar    two -0.392670 -0.136473
6  foo    one  0.007207 -0.561757
7  foo  three  1.928123 -1.623033

--
df.groupby('A').sum()

            C        D
A                     
bar -2.802588  2.42611
foo  3.146492 -0.63958

--
df.groupby(['A','B']).sum()

                  C         D
A   B                        
bar one   -1.814470  2.395985
    three -0.595447  0.166599
    two   -0.392670 -0.136473
foo one   -1.195665 -0.616981
    three  1.928123 -1.623033
    two    2.414034  1.600434

--
# group by, count

df

  col1 col2  col3  col4  col5  col6
0    A    B  0.20 -0.61 -0.49  1.49
1    A    B -1.53 -1.01 -0.39  1.82
2    A    B -0.44  0.27  0.72  0.11
3    A    B  0.28 -1.32  0.38  0.18
4    C    D  0.12  0.59  0.81  0.66
5    C    D -0.13 -1.65 -1.64  0.50
6    C    D -1.42 -0.11 -0.18 -0.44
7    E    F -0.00  1.42 -0.26  1.17
8    E    F  0.91 -0.47  1.35 -0.34
9    G    H  1.48 -0.63 -1.14  0.17

df.groupby(['col1', 'col2']).size()

col1  col2
A     B       4
C     D       3
E     F       2
G     H       1

--
# Sort within each group:

Sort the PVE score within each job_id:

score_df['rank'] = score_df.groupby(['job_id'])['score'].rank(ascending = True)

--
(findplotpandas)
Plot in Python, using matplotlib

Tao's standard way:

tao_df is a DataFrame.

tao_df:

col_a  col_b  col_c
 1     0.1    0.3
 2     0.2    0.2
 3     0.3    0.1

The following code will make two curves in the same plot.
The first curve uses the values of col_a as x-axis, the values of col_b as y-axis.
The second curve uses the values of col_a as x-axis, the values of col_c as y-axis.

import pandas as pd
import matplotlib.pyplot as plt

# Plot curves:
plt.plot(tao_df['col_a'], tao_df['col_b'], label = 'Values of b', color = 'blue') # label sets the legend text
plt.plot(tao_df['col_a'], tao_df['col_c'], label = 'Values of c', color = 'red') # label sets the legend text
plt.xlabel('Values of a')
plt.ylabel('Magnitudes')
plt.title('Tao plot')
plt.legend()
plt.show()

# Plot scatters:
dot_size = 3
plt.scatter(tao_df['col_a'], tao_df['col_b'], label = 'Values of b', color = 'blue', s = dot_size) # label sets the legend text
plt.plot(tao_df['col_a'], tao_df['col_c'], label = 'Values of c', color = 'red', s = dot_size) # label sets the legend text
plt.xlabel('Values of a')
plt.ylabel('Magnitudes')
plt.title('Tao plot')
plt.legend()
plt.show()

--
Make multiple plots side-by-side:

Should use Python 3:

# One row, two columns:
from matplotlib import pyplot as plt
fig, axes = plt.subplots(1, 2)

# Sets size of each subplot. If axis text is hidden, the increase the height.
fig.set_figheight(15)
fig.set_figwidth(15)

axes[0].plot(data['row_num'], data['score_pve'], label = 'score_pve', color = 'blue')
axes[1].plot(data['row_num'], data['score_pve'], label = 'score_pve', color = 'blue')

# Two rows, two columns:
fig, axes = plt.subplots(2, 2)
axes[0, 0].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[0, 0].set_xlabel('user_id')
axes[0, 0].set_ylabel('score')
axes[0, 0].set_title('Hello')
axes[0, 0].text(1000, 0.5, "Helo", dict(size=10, color='black')) # Add text in each subplot
axes[0, 1].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[1, 0].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')
axes[1, 1].plot(data['row_num'], data['score_pve_scaled'], label = 'score_pve', color = 'blue')

--
Using dataframe.plot():

import pandas as pd
import matplotlib.pyplot as plot

# Make the plot and show it in a window:
df['col1'].plot()
plot.show() # Must be called to show plots

# Make the plot and save it to file:
df['col1'].plot()
plot.savefig('fig_1.png')

# Set the y axis range:
df1 = pd.DataFrame(data = dic1)
df1.plot()
plot.ylim(-256, 100)

# Scatter plot:
df.plot(kind = 'scatter', x = 'compay_1', y = 'compay_2')
plot.show()

# Plot histogram, set number of bins to 20:
df.hist(bins = 20)
plot.show()

# Add verticle line at the position x coordinate = 5, with white color:
plot.axvline(5, color = 'w', linestyle = 'dashed', linewidth = 2)
plot.show()

Add title and labels:

ax = df.plot(title = 'prices', fontsize = 2)
ax.set_xlabel("date")
ax.set_ylabel("price")
plot.show()

--
(finddatespandas)
Dates in pandas

import pandas as pd

def test_run():
	start_date = '2010-01-22'
	end_date = '2010-01-26'
	dates = pd.date_range(start_date, end_date)
	print dates[0] # Output: 2010-01-22 00:00:00
	df1 = pd.DataFrame(index = dates)

We used pandas date range method which takes two parameters, that is start and end date. The output you see is not the list of strings, but the list of date time index objects. 

The output above: 2010-01-22 00:00:00:
This is the first element of the list which a date/time index object. The trailing zero zeros for each object is the default time stamp.

Next we define an empty dataframe df1 with these dates as index. We use the parameter index to supply the dates. Note that without this parameter the dataframe will have an index of integers 0,1,2 as seen before. So here's your DataFrame, DF1. It's an empty DataFrame with no columns. However, as we pass the index parameter, we have an index as dates. And you can see that it's a date time index object.

--
(findjoinpandas)
Join

df_join = df1.join(df2) <- Left join, avadoles!
df_join = df1.join(df2, how = 'inner') <- Inner join

Avadoles! Different from Hive, Spark, SQL:
DataFrame.join does a left join by default. So if we write a.join b, it will read in all the rows from a, but only those rows from b whose index values are present in a's index.

--
(findmergepandas)
(findconcatpandas)

Join, merge, concat:

From online:

Use merge, which is inner join by default:

pd.merge(df1, df2, left_index=True, right_index=True)
Or join, which is left join by default:

df1.join(df2)
Or concat, which is outer join by default:

pd.concat([df1, df2], axis=1)
Samples:

df1 = pd.DataFrame({'a':range(6),
                    'b':[5,3,6,9,2,4]}, index=list('abcdef'))

print (df1)
   a  b
a  0  5
b  1  3
c  2  6
d  3  9
e  4  2
f  5  4

df2 = pd.DataFrame({'c':range(4),
                    'd':[10,20,30, 40]}, index=list('abhi'))

print (df2)
   c   d
a  0  10
b  1  20
h  2  30
i  3  40

#default inner join
df3 = pd.merge(df1, df2, left_index=True, right_index=True)

print (df3)
   a  b  c   d
a  0  5  0  10
b  1  3  1  20

#default left join
df4 = df1.join(df2)

print (df4)
   a  b    c     d
a  0  5  0.0  10.0
b  1  3  1.0  20.0
c  2  6  NaN   NaN
d  3  9  NaN   NaN
e  4  2  NaN   NaN
f  5  4  NaN   NaN

#default outer join
df5 = pd.concat([df1, df2], axis=1)

print (df5)
     a    b    c     d
a  0.0  5.0  0.0  10.0
b  1.0  3.0  1.0  20.0
c  2.0  6.0  NaN   NaN
d  3.0  9.0  NaN   NaN
e  4.0  2.0  NaN   NaN
f  5.0  4.0  NaN   NaN
h  NaN  NaN  2.0  30.0
i  NaN  NaN  3.0  40.0

--
(findnormalizepandas)
Normalize

df1 = df1 / df1[0] # Divide the entire dataframe by its first row. It is equivalent as below, but it is more elegant and much FASTER.

df1 = df1 / df1.ix[0,:] # Should be the same as above.

Does the same thing as above:

for date in df1.index:
    for s in symbols:
        df1[date, s] = df1[date, s] / df1[0, s]

--
(findsumcolumnspandas)
(findmeancolumnspandas)
Sum (or mean) of all columns into a new column

df2 = pd.DataFrame(data = dic2)
df2['sum'] = df2.iloc[:,-num_columns:].sum(axis=1) # Sum all columns
df2['mean'] = df2['sum'] / num_columns
df2['std'] = df2.iloc[:,-num_columns:].std(axis=1)

print df2.head(5)

   run_0  run_1  run_2  sum      mean       std
0      0      0      0    0  0.000000  0.000000
1      1      1     -1    1  0.333333  1.018350
2      2      0      1    3  1.000000  1.154701
3      1      2      2    5  1.666667  1.835857
4     -1      3      1    3  1.000000  1.154701

==
(findscikitlearn)
(findsklearn)

Scikit-Learn

Notes from the following site (double checked and confirmed it does not miss anything from the site)

https://www.datacamp.com/community/tutorials/machine-learning-python

Python Machine Learning: Scikit-Learn Tutorial

An easy-to-follow scikit-learn tutorial that will help you get started with Python machine learning.

Machine Learning with Python

Machine learning is a branch in computer science that studies the design of algorithms that can learn.

Typical tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. These tasks are learned through available data that were observed through experiences or instructions, for example.

The hope that comes with this discipline is that including the experience into its tasks will eventually improve the learning. But this improvement needs to happen in such a way that the learning itself becomes automatic so that humans like ourselves don’t need to interfere anymore is the ultimate goal.

Today’s scikit-learn tutorial will introduce you to the basics of Python machine learning:

- You'll learn how to use Python and its libraries to explore your data with the help of matplotlib and Principal Component Analysis (PCA),

- And you'll preprocess your data with normalization, and you'll split your data into training and test sets.

- Next, you'll work with the well-known KMeans algorithm to construct an unsupervised model, fit this model to your data, predict values, and validate the model that you have built.

- As an extra, you'll also see how you can also use Support Vector Machines (SVM) to construct another model to classify your data.

If you’re more interested in an R tutorial, take a look at our Machine Learning with R for Beginners tutorial.

Alternatively, check out DataCamp's Supervised Learning with scikit-learn and Unsupervised Learning in Python courses!

Loading Your Data Set

The first step to about anything in data science is loading your data. This is also the starting point of this scikit-learn tutorial.

This discipline typically works with observed data. This data might be collected by yourself, or you can browse through other sources to find data sets. But if you’re not a researcher or otherwise involved in experiments, you’ll probably do the latter.

If you’re new to this and you want to start problems on your own, finding these data sets might prove to be a challenge. However, you can typically find good data sets at the UCI Machine Learning Repository or on the Kaggle website. Also, check out this KD Nuggets list with resources.

For now, you should warm up, not worry about finding any data by yourself and just load in the digits data set that comes with a Python library, called scikit-learn.

Fun fact: did you know the name originates from the fact that this library is a scientific toolbox built around SciPy? By the way, there is more than just one scikit out there. This scikit contains modules specifically for machine learning and data mining, which explains the second component of the library name. :)

To load in the data, you import the module datasets from sklearn. Then, you can use the load_digits() method from datasets to load in the data:

# Import `datasets` from `sklearn`
from sklearn import datasets

# Load in the `digits` data
# Tao: note that load_digits() only loads a particular dataset
digits = datasets.load_digits()

# Print the `digits` data 
print(digits)

{'images': array([[[  0.,   0.,   5., ...,   1.,   0.,   0.],
        [  0.,   0.,  13., ...,  15.,   5.,   0.],
        [  0.,   3.,  15., ...,  11.,   8.,   0.],
        ..., 
        [  0.,   4.,  11., ...,  12.,   7.,   0.],
        [  0.,   2.,  14., ...,  12.,   0.,   0.],
        [  0.,   0.,   6., ...,   0.,   0.,   0.]],

       [[  0.,   0.,   0., ...,   5.,   0.,   0.],
        [  0.,   0.,   0., ...,   9.,   0.,   0.],
        [  0.,   0.,   3., ...,   6.,   0.,   0.],
        ..., 
        [  0.,   0.,   1., ...,   6.,   0.,   0.],
        [  0.,   0.,   1., ...,   6.,   0.,   0.],
        [  0.,   0.,   0., ...,  10.,   0.,   0.]],

       [[  0.,   0.,   0., ...,  12.,   0.,   0.],
        [  0.,   0.,   3., ...,  14.,   0.,   0.],
        [  0.,   0.,   8., ...,  16.,   0.,   0.],
        ..., 
        [  0.,   9.,  16., ...,   0.,   0.,   0.],
        [  0.,   3.,  13., ...,  11.,   5.,   0.],
        [  0.,   0.,   0., ...,  16.,   9.,   0.]],

       ..., 
       [[  0.,   0.,   1., ...,   1.,   0.,   0.],
        [  0.,   0.,  13., ...,   2.,   1.,   0.],
        [  0.,   0.,  16., ...,  16.,   5.,   0.],
        ..., 
        [  0.,   0.,  16., ...,  15.,   0.,   0.],
        [  0.,   0.,  15., ...,  16.,   0.,   0.],
        [  0.,   0.,   2., ...,   6.,   0.,   0.]],

       [[  0.,   0.,   2., ...,   0.,   0.,   0.],
        [  0.,   0.,  14., ...,  15.,   1.,   0.],
        [  0.,   4.,  16., ...,  16.,   7.,   0.],
        ..., 
        [  0.,   0.,   0., ...,  16.,   2.,   0.],
        [  0.,   0.,   4., ...,  16.,   2.,   0.],
        [  0.,   0.,   5., ...,  12.,   0.,   0.]],

       [[  0.,   0.,  10., ...,   1.,   0.,   0.],
        [  0.,   2.,  16., ...,   1.,   0.,   0.],
        [  0.,   0.,  15., ...,  15.,   0.,   0.],
        ..., 
        [  0.,   4.,  16., ...,  16.,   6.,   0.],
        [  0.,   8.,  16., ...,  16.,   8.,   0.],
        [  0.,   1.,   8., ...,  12.,   1.,   0.]]]), 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'data': array([[  0.,   0.,   5., ...,   0.,   0.,   0.],
       [  0.,   0.,   0., ...,  10.,   0.,   0.],
       [  0.,   0.,   0., ...,  16.,   9.,   0.],
       ..., 
       [  0.,   0.,   1., ...,   6.,   0.,   0.],
       [  0.,   0.,   2., ...,  12.,   0.,   0.],
       [  0.,   0.,  10., ...,  12.,   1.,   0.]]), 'target': array([0, 1, 2, ..., 8, 9, 8]), 'DESCR': "Optical Recognition of Handwritten Digits Data Set\n===================================================\n\nNotes\n-----\nData Set Characteristics:\n    :Number of Instances: 5620\n    :Number of Attributes: 64\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n    :Missing Attribute Values: None\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n    :Date: July; 1998\n\nThis is a copy of the test set of the UCI ML hand-written digits datasets\nhttp://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n\nThe data set contains images of hand-written digits: 10 classes where\neach class refers to a digit.\n\nPreprocessing programs made available by NIST were used to extract\nnormalized bitmaps of handwritten digits from a preprinted form. From a\ntotal of 43 people, 30 contributed to the training set and different 13\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n4x4 and the number of on pixels are counted in each block. This generates\nan input matrix of 8x8 where each element is an integer in the range\n0..16. This reduces dimensionality and gives invariance to small\ndistortions.\n\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n1994.\n\nReferences\n----------\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n    Graduate Studies in Science and Engineering, Bogazici University.\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n    Linear dimensionalityreduction using relevance weighted LDA. School of\n    Electrical and Electronic Engineering Nanyang Technological University.\n    2005.\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\n    Algorithm. NIPS. 2000.\n"}

Note that the datasets module contains other methods to load and fetch popular reference datasets, and you can also count on this module in case you need artificial data generators. Also, this data set is also available through the UCI Repository that was mentioned above: you can find the data here.

If you had decided to pull the data from the latter page, your data import would’ve looked like this:

# Import the `pandas` library as `pd`
import pandas as pd

# Load in the data with `read_csv()`
digits = pd.read_csv("http://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)

# Print out `digits`
print(digits)

      0   1   2   3   4   5   6   7   8   9  ...  55  56  57  58  59  60  61  \
0      0   1   6  15  12   1   0   0   0   7 ...   0   0   0   6  14   7   1   
1      0   0  10  16   6   0   0   0   0   7 ...   0   0   0  10  16  15   3   
2      0   0   8  15  16  13   0   0   0   1 ...   0   0   0   9  14   0   0   
3      0   0   0   3  11  16   0   0   0   0 ...   0   0   0   0   1  15   2   
4      0   0   5  14   4   0   0   0   0   0 ...   0   0   0   4  12  14   7   
5      0   0  11  16  10   1   0   0   0   4 ...   3   0   0  10  16  16  16   
6      0   0   1  11  13  11   7   0   0   0 ...   0   0   0   1  13   5   0   
7      0   0   8  10   8   7   2   0   0   1 ...   0   0   0   4  13   8   0   
8      0   0  15   2  14  13   2   0   0   0 ...   0   0   0  10  12   5   0   
9      0   0   3  13  13   2   0   0   0   6 ...   0   0   0   3  15  11   6   
10     0   0   6  14  14  16  16   8   0   0 ...   0   0   0  10  12   0   0   
11     0   0   0   3  16  11   1   0   0   0 ...   0   0   0   0   2  14  14   
12     0   0   0   4  13  16  16   3   0   0 ...   0   0   0   0   5  15   4   
13     0   0   7  12   6   2   0   0   0   0 ...   0   0   0   5  16   9   0   
14     0   0   7  11  11   6   0   0   0   9 ...   0   0   0  14  16  12  10   
15     0   1  10  15   8   0   0   0   0   6 ...   0   0   0   9  15   8   0   
16     0   0   0   1  11   7   0   0   0   0 ...   0   0   0   0   3  15   0   
17     0   0   5  12  16  16   3   0   0   0 ...   0   0   0   8  12   0   0   
18     0   0   1   8  13  13   2   0   0   4 ...   0   0   0   1  13  12   4   
19     0   0   0   2  13  12   4   0   0   0 ...   0   0   0   0   0  15   3   
20     0   0   4  11  15  16  15   0   0   0 ...   0   0   0   6  14   2   0   
21     0   0   4  10  13  11   1   0   0   2 ...   0   0   0   6  13  11   1   
22     0   0   3  11  13  14   6   0   0   0 ...   0   0   0   3  13  10   0   
23     0   0   1   4  11  13   7   0   0   2 ...   0   0   0   0   1  14   3   
24     0   0   9  13   1   0   0   0   0   0 ...   5   0   0   4  15  16  16   
25     0   0   9  16  11   0   0   0   0   4 ...   1   0   0  10  16   9   9   
26     0   0   2  13   9   0   0   0   0   0 ...   0   0   0   3  16  14   4   
27     0   0   0  10  12   0   0   0   0   0 ...   0   0   0   1  11  14  12   
28     0   0   0   0  10  13   0   0   0   0 ...   0   0   0   0   0   8  15   
29     0   0   7   9  13  11   2   0   0   6 ...   0   0   0  13  12   8   1   
...   ..  ..  ..  ..  ..  ..  ..  ..  ..  .. ...  ..  ..  ..  ..  ..  ..  ..   
3793   0   0  15  15  16  14   1   0   0   3 ...   0   0   0  15  16  12   0   
3794   0   0   2  13  16  15   4   0   0   0 ...   0   0   0   3  14   1   0   
3795   0   0  12  16   7   0   0   0   0   2 ...   0   0   0  11  16  16  16   
3796   0   0   0   3  13   0   0   0   0   0 ...   0   0   0   0   6  12   0   
3797   0   0   0  12   8   0   0   0   0   0 ...   0   0   0   0  14   6   0   
3798   0   0   0   9  12   0   0   0   0   0 ...   0   0   0   0  10  11   0   
3799   0   0   5  16  13   1   0   0   0   0 ...   0   0   1   6  10  12  12   
3800   0   0   6  16   8   0   0   0   0   2 ...   0   0   0   7  13  16  13   
3801   0   0   5  16  11   0   0   0   0   1 ...   0   0   0   4  14  12   2   
3802   0   1  12  16  14   4   0   0   0   8 ...   0   0   0  13  15  15  10   
3803   0   0   3  14  13   3   0   0   0   0 ...   0   0   0   3  13  13   6   
3804   0   0  12  16  16  10   1   0   0   0 ...   0   0   1  10  16  16  16   
3805   0   0   0   8  11   0   0   0   0   0 ...   0   0   0   0   7  12   0   
3806   0   1   9  16  14   6   0   0   0   5 ...   0   0   0  11  16   7   0   
3807   0   0   4  14  15   3   0   0   0   0 ...   0   0   0   3  14  14   6   
3808   0   2  15  16  15   1   0   0   0   3 ...   0   0   0  14  13  16  11   
3809   0   0   5  15  16   6   0   0   0   0 ...   0   0   0   5  15  16  15   
3810   0   0   4  16  11   1   0   0   0   0 ...   0   0   0   2  12  16  11   
3811   0   0   0   0   7  11   1   0   0   0 ...   0   0   0   2   0   7  11   
3812   0   0   0   6  13   0   0   0   0   0 ...   0   0   0   0   6   9   0   
3813   0   0   0   6   8   0   0   0   0   0 ...   0   0   1   4   9   8   0   
3814   0   0   9  16   6   0   0   0   0   2 ...   0   0   0  10  13   1   0   
3815   0   0   9  16  12   1   0   0   0   3 ...   0   0   0   8  16  16  16   
3816   0   1  10  16  16   4   0   0   0   8 ...   0   0   2  13  16  12   5   
3817   0   0   6  16  11   0   0   0   0   1 ...   0   0   1   7  14  16  12   
3818   0   0   5  13  11   2   0   0   0   2 ...   0   0   0   8  13  15  10   
3819   0   0   0   1  12   1   0   0   0   0 ...   0   0   0   0   4   9   0   
3820   0   0   3  15   0   0   0   0   0   0 ...   0   0   0   4  14  16   9   
3821   0   0   6  16   2   0   0   0   0   0 ...   0   0   0   5  16  16  16   
3822   0   0   2  15  16  13   1   0   0   0 ...   0   0   0   4  14   1   0   

      62  63  64  
0      0   0   0  
1      0   0   0  
2      0   0   7  
3      0   0   4  
4      0   0   6  
5     16   6   2  
6      0   0   5  
7      0   0   5  
8      0   0   0  
9      0   0   8  
10     0   0   7  
11     1   0   1  
12     0   0   9  
13     0   0   5  
14     1   0   3  
15     0   0   0  
16     0   0   4  
17     0   0   7  
18     0   0   8  
19     0   0   4  
20     0   0   7  
21     0   0   8  
22     0   0   5  
23     0   0   9  
24    16  16   1  
25    13   6   2  
26     0   0   0  
27     1   0   6  
28     2   0   1  
29     0   0   8  
...   ..  ..  ..  
3793   0   0   5  
3794   0   0   7  
3795  16   8   2  
3796   0   0   4  
3797   0   0   4  
3798   0   0   4  
3799   2   0   9  
3800   9   0   9  
3801   0   0   0  
3802   2   0   3  
3803   0   0   0  
3804   7   0   3  
3805   0   0   4  
3806   0   0   8  
3807   0   0   0  
3808   1   0   3  
3809   1   0   0  
3810   1   0   1  
3811   0   0   1  
3812   0   0   4  
3813   0   0   4  
3814   0   0   8  
3815   8   0   9  
3816   0   0   3  
3817   1   0   9  
3818   1   0   9  
3819   0   0   4  
3820   0   0   6  
3821   5   0   6  
3822   0   0   7  

[3823 rows x 65 columns]

Note that if you download the data like this, the data is already split up in a training and a test set, indicated by the extensions .tra and .tes. You’ll need to load in both files to elaborate your project. With the command above, you only load in the training set.

Tip: if you want to know more about importing data with the Python data manipulation library Pandas, consider taking DataCamp’s Importing Data in Python course.

Explore Your Data

When first starting out with a data set, it’s always a good idea to go through the data description and see what you can already learn. When it comes to scikit-learn, you don’t immediately have this information readily available, but in the case where you import data from another source, there's usually a data description present, which will already be a sufficient amount of information to gather some insights into your data.

However, these insights are not merely deep enough for the analysis that you are going to perform. You really need to have a good working knowledge about the data set.

Performing an exploratory data analysis (EDA) on a data set like the one that this tutorial now has might seem difficult.

Where do you start exploring these handwritten digits?

Gathering Basic Information on Your Data

Let’s say that you haven’t checked any data description folder (or maybe you want to double-check the information that has been given to you).

Then you should start by gathering the necessary information.

When you printed out the digits data after having loaded it with the help of the scikit-learn datasets module, you will have noticed that there is already a lot of information available. You already know things such as the target values and the description of your data. You can access the digits data through the attribute data. Similarly, you can also access the target values or labels through the target attribute and the description through the DESCR attribute.

To see which keys you have available to already get to know your data, you can just run digits.keys().

Try this all out in the following DataCamp Light blocks:

# Get the keys of the `digits` data
print(digits.keys())

dict_keys(['images', 'target_names', 'data', 'target', 'DESCR'])

# Print out the data
print(digits.data)

[[  0.   0.   5. ...,   0.   0.   0.]
 [  0.   0.   0. ...,  10.   0.   0.]
 [  0.   0.   0. ...,  16.   9.   0.]
 ..., 
 [  0.   0.   1. ...,   6.   0.   0.]
 [  0.   0.   2. ...,  12.   0.   0.]
 [  0.   0.  10. ...,  12.   1.   0.]]

# Print out the target values
print(digits.target)

[0 1 2 ..., 8 9 8]

# Print out the description of the `digits` data
print(digits.DESCR)

Optical Recognition of Handwritten Digits Data Set
===================================================

Notes
-----
Data Set Characteristics:
    :Number of Instances: 5620
    :Number of Attributes: 64
    :Attribute Information: 8x8 image of integer pixels in the range 0..16.
    :Missing Attribute Values: None
    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)
    :Date: July; 1998

This is a copy of the test set of the UCI ML hand-written digits datasets
http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits

The data set contains images of hand-written digits: 10 classes where
each class refers to a digit.

Preprocessing programs made available by NIST were used to extract
normalized bitmaps of handwritten digits from a preprinted form. From a
total of 43 people, 30 contributed to the training set and different 13
to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of
4x4 and the number of on pixels are counted in each block. This generates
an input matrix of 8x8 where each element is an integer in the range
0..16. This reduces dimensionality and gives invariance to small
distortions.

For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.
T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
1994.

References
----------
  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
    Graduate Studies in Science and Engineering, Bogazici University.
  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.
  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.
    Linear dimensionalityreduction using relevance weighted LDA. School of
    Electrical and Electronic Engineering Nanyang Technological University.
    2005.
  - Claudio Gentile. A New Approximate Maximal Margin Classification
    Algorithm. NIPS. 2000.

The next thing that you can (double)check is the type of your data.

If you used read_csv() to import the data, you would have had a data frame that contains just the data. There wouldn’t be any description component, but you would be able to resort to, for example, head() or tail() to inspect your data. In these cases, it’s always wise to read up on the data description folder!

However, this tutorial assumes that you make use of the library's data and the type of the digits variable is not that straightforward if you’re not familiar with the library. Look at the print out in the first code chunk. You’ll see that digits actually contains numpy arrays!

This is already quite vital information. But how do you access these arrays?

It’s straightforward, actually: you use attributes to access the relevant arrays.

Remember that you have already seen which attributes are available when you printed digits.keys(). For instance, you have the data attribute to isolate the data, target to see the target values and the DESCR for the description, …

But what then?

The first thing that you should know of an array is its shape. That is the number of dimensions and items that are contained within an array. The array’s shape is a tuple of integers that specify the sizes of each dimension. In other words, if you have a 3d array like this y = np.zeros((2, 3, 4)), the shape of your array will be (2,3,4).

Now let’s try to see what the shape is of these three arrays that you have distinguished (the data, target and DESCR arrays).

Use first the data attribute to isolate the numpy array from the digits data and then use the shape attribute to find out more. You can do the same for the target and DESCR. There’s also the images attribute, which is basically the data in images. You’re also going to test this out.

Check up on this statement by using the shape attribute on the array:

# Isolate the `digits` data
digits_data = digits.data

# Inspect the shape
print(digits_data.shape)

(1797, 64)

# Isolate the target values with `target`
digits_target = digits.target

# Inspect the shape
print(digits_target.shape)

(1797,)

# Print the number of unique labels
number_digits = len(np.unique(digits.target))

print(number_digits)

10

# Isolate the `images`
digits_images = digits.images

# Inspect the shape
print(digits_images.shape)

(1797, 8, 8)

print(digits_images)

[[[  0.   0.   5. ...,   1.   0.   0.]
  [  0.   0.  13. ...,  15.   5.   0.]
  [  0.   3.  15. ...,  11.   8.   0.]
  ..., 
  [  0.   4.  11. ...,  12.   7.   0.]
  [  0.   2.  14. ...,  12.   0.   0.]
  [  0.   0.   6. ...,   0.   0.   0.]]

 [[  0.   0.   0. ...,   5.   0.   0.]
  [  0.   0.   0. ...,   9.   0.   0.]
  [  0.   0.   3. ...,   6.   0.   0.]
  ..., 
  [  0.   0.   1. ...,   6.   0.   0.]
  [  0.   0.   1. ...,   6.   0.   0.]
  [  0.   0.   0. ...,  10.   0.   0.]]

 [[  0.   0.   0. ...,  12.   0.   0.]
  [  0.   0.   3. ...,  14.   0.   0.]
  [  0.   0.   8. ...,  16.   0.   0.]
  ..., 
  [  0.   9.  16. ...,   0.   0.   0.]
  [  0.   3.  13. ...,  11.   5.   0.]
  [  0.   0.   0. ...,  16.   9.   0.]]

 ..., 
 [[  0.   0.   1. ...,   1.   0.   0.]
  [  0.   0.  13. ...,   2.   1.   0.]
  [  0.   0.  16. ...,  16.   5.   0.]
  ..., 
  [  0.   0.  16. ...,  15.   0.   0.]
  [  0.   0.  15. ...,  16.   0.   0.]
  [  0.   0.   2. ...,   6.   0.   0.]]

 [[  0.   0.   2. ...,   0.   0.   0.]
  [  0.   0.  14. ...,  15.   1.   0.]
  [  0.   4.  16. ...,  16.   7.   0.]
  ..., 
  [  0.   0.   0. ...,  16.   2.   0.]
  [  0.   0.   4. ...,  16.   2.   0.]
  [  0.   0.   5. ...,  12.   0.   0.]]

 [[  0.   0.  10. ...,   1.   0.   0.]
  [  0.   2.  16. ...,   1.   0.   0.]
  [  0.   0.  15. ...,  15.   0.   0.]
  ..., 
  [  0.   4.  16. ...,  16.   6.   0.]
  [  0.   8.  16. ...,  16.   8.   0.]
  [  0.   1.   8. ...,  12.   1.   0.]]]

To recap: by inspecting digits.data, you see that there are 1797 samples and that there are 64 features. Because you have 1797 samples, you also have 1797 target values.

But all those target values contain 10 unique values, namely, from 0 to 9. In other words, all 1797 target values are made up of numbers that lie between 0 and 9. This means that the digits that your model will need to recognize are numbers from 0 to 9.

Lastly, you see that the images data contains three dimensions: there are 1797 instances that are 8 by 8 pixels big. You can visually check that the images and the data are related by reshaping the images array to two dimensions: digits.images.reshape((1797, 64)).

But if you want to be entirely sure, better to check with

print(np.all(digits.images.reshape((1797,64)) == digits.data))

With the numpy method all(), you test whether all array elements along a given axis evaluate to True. In this case, you evaluate if it’s true that the reshaped images array equals digits.data. You’ll see that the result will be True in this case.

Visualize Your Data Images With matplotlib

Then, you can take your exploration up a notch by visualizing the images that you’ll be working with. You can use one of Python’s data visualization libraries, such as matplotlib, for this purpose:

# Import matplotlib
import matplotlib.pyplot as plt

# Figure size (width, height) in inches
fig = plt.figure(figsize=(6, 6))

# Adjust the subplots 
fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)

# For each of the 64 images
for i in range(64):
    # Initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-th position
    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])
    # Display an image at the i-th position
    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')
    # label the image with the target value
    ax.text(0, 7, str(digits.target[i]))

# Show the plot
plt.show()

The code chunk seems quite lengthy at first sight, and this might be overwhelming. But, what happens in the code chunk above is actually pretty easy once you break it down into parts:

-You import matplotlib.pyplot.

- Next, you set up a figure with a figure size of 6 inches wide and 6 inches long. This is your blank canvas where all the subplots with the images will appear.

- Then you go to the level of the subplots to adjust some parameters: you set the left side of the suplots of the figure to 0, the right side of the suplots of the figure to 1, the bottom to 0 and the top to 1. The height of the blank space between the suplots is set at 0.005 and the width is set at 0.05. These are merely layout adjustments.

- After that, you start filling up the figure that you have made with the help of a for loop.

- You initialize the suplots one by one, adding one at each position in the grid that is 8 by 8 images big.

- You display each time one of the images at each position in the grid. As a color map, you take binary colors, which in this case will result in black, gray values and white colors. The interpolation method that you use is 'nearest', which means that your data is interpolated in such a way that it isn’t smooth. You can see the effect of the different interpolation methods here.

- The cherry on the pie is the addition of text to your subplots. The target labels are printed at coordinates (0,7) of each subplot, which in practice means that they will appear in the bottom-left of each of the subplots.

- Don’t forget to show the plot with plt.show()!

In the end, you’ll get to see the following:

(A picture of many digits and their blurry images)

On a more simple note, you can also visualize the target labels with an image, just like this:

# Import matplotlib
import matplotlib.pyplot as plt 

# Join the images and target labels in a list
images_and_labels = list(zip(digits.images, digits.target))

# for every element in the list
for index, (image, label) in enumerate(images_and_labels[:8]):
    # initialize a subplot of 2X4 at the i+1-th position
    plt.subplot(2, 4, index + 1)
    # Don't plot any axes
    plt.axis('off')
    # Display images in all subplots 
    plt.imshow(image, cmap=plt.cm.gray_r,interpolation='nearest')
    # Add a title to each subplot
    plt.title('Training: ' + str(label))

# Show the plot
plt.show()

Which will render the following visualization:

(A picture of digits 0 - 7 and their blurry images)

Note that in this case, after you have imported matplotlib.pyplot, you zip the two numpy arrays together and save it into a variable called images_and_labels. You’ll see now that this list contains suples of each time an instance of digits.images and a corresponding digits.target value.

Then, you say that for the first eight elements of images_and_labels -note that the index starts at 0!-, you initialize subplots in a grid of 2 by 4 at each position. You turn of the plotting of the axes and you display images in all the subplots with a color map plt.cm.gray_r (which returns all grey colors) and the interpolation method used is nearest. You give a title to each subplot, and you show it.

Not too hard, huh?

And now you have an excellent idea of the data that you’ll be working with!

Visualizing Your Data: Principal Component Analysis (PCA)

But is there no other way to visualize the data?

As the digits data set contains 64 features, this might prove to be a challenging task. You can imagine that it’s tough to understand the structure and keep the overview of the digits data. In such cases, it is said that you’re working with a high dimensional data set.

High dimensionality of data is a direct result of trying to describe the objects via a collection of features. Other examples of high dimensional data are, for example, financial data, climate data, neuroimaging, …

But, as you might have gathered already, this is not always easy. In some cases, high dimensionality can be problematic, as your algorithms will need to take into account too many features. In such cases, you speak of the curse of dimensionality. Because having a lot of dimensions can also mean that your data points are far away from virtually every other point, which makes the distances between the data points uninformative.

Don’t worry, though, because the curse of dimensionality is not merely a matter of counting the number of features. There are also cases in which the effective dimensionality might be much smaller than the number of the features, such as in data sets where some features are irrelevant.

In addition, you can also understand that data with only two or three dimensions are easier to grasp and can also be visualized easily.

That all explains why you’re going to visualize the data with the help of one of the Dimensionality Reduction techniques, namely Principal Component Analysis (PCA). The idea in PCA is to find a linear combination of the two variables that contains most of the information. This new variable or “principal component” can replace the two original variables.

In short, it’s a linear transformation method that yields the directions (principal components) that maximize the variance of the data. Remember that the variance indicates how far a set of data points lie apart. If you want to know more, go to this page.

You can easily apply PCA do your data with the help of scikit-learn:

(findpcasklearn)

# Create a Randomized PCA model that takes two components
randomized_pca = RandomizedPCA(n_components=2)

# Fit and transform the data to the model
reduced_data_rpca = randomized_pca.fit_transform(digits.data)

# Create a regular PCA model 
pca = PCA(n_components=2)

# Fit and transform the data to the model
reduced_data_pca = pca.fit_transform(digits.data)

# Inspect the shape
reduced_data_pca.shape

# Print out the data
print(reduced_data_rpca)

[[ -1.22879838  21.26221767]
 [  7.92652923 -20.77510207]
 [  7.02035409  -9.99139534]
 ..., 
 [ 10.7899558   -6.96832698]
 [ -4.87957222  12.4545091 ]
 [ -0.33091545   6.3564161 ]]

print(reduced_data_pca)

[[ -1.25946509  21.27488229]
 [  7.95761152 -20.76869648]
 [  6.99192152  -9.95598685]
 ..., 
 [ 10.80128499  -6.96025203]
 [ -4.87210304  12.42394965]
 [ -0.34438868   6.36554785]]

Tip: you have used the RandomizedPCA() here because it performs better when there’s a high number of dimensions. Try replacing the randomized PCA model or estimator object with a regular PCA model and see what the difference is.

Note how you explicitly tell the model only to keep two components. This is to make sure that you have two-dimensional data to plot. Also, note that you don’t pass the target class with the labels to the PCA transformation because you want to investigate if the PCA reveals the distribution of the different labels and if you can clearly separate the instances from each other.

You can now build a scatterplot to visualize the data:

colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']
for i in range(len(colors)):
    x = reduced_data_rpca[:, 0][digits.target == i]
    y = reduced_data_rpca[:, 1][digits.target == i]
    plt.scatter(x, y, c=colors[i])
plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.xlabel('First Principal Component')
plt.ylabel('Second Principal Component')
plt.title("PCA Scatter Plot")
plt.show()

Which looks like this:

(A picutre of scatter plot)

Again you use matplotlib to visualize the data. It’s useful for a quick visualization of what you’re working with, but you might have to consider something a little bit fancier if you’re working on making this part of your data science portfolio.

Also note that the last call to show the plot (plt.show()) is not necessary if you’re working in Jupyter Notebook, as you’ll want to put the images inline. When in doubt, you can always check out our Definitive Guide to Jupyter Notebook.

What happens in the code chunk above is the following:

- You put your colors together in a list. Note that you list ten colors, which is equal to the number of labels that you have. This way, you make sure that your data points can be colored in according to the labels. Then, you set up a range that goes from 0 to 10. Mind you that this range is not inclusive! Remember that this is the same for indices of a list, for example.

- You set up your x and y coordinates. You take the first or the second column of reduced_data_rpca, and you select only those data points for which the label equals the index that you’re considering. That means that in the first run, you’ll consider the data points with label 0, then label 1, … and so on.

- You construct the scatter plot. Fill in the x and y coordinates and assign a color to the batch that you’re processing. The first run, you’ll give the color black to all data points, the next run blue, … and so on.

- You add a legend to your scatter plot. Use the target_names key to get the right labels for your data points.

- Add labels to your x and y axes that are meaningful.

- Reveal the resulting plot.

Where To Go Now?

Now that you have even more information about your data and you have a visualization ready, it does seem a bit like the data points sort of group together, but you also see there is quite some overlap.

This might be interesting to investigate further.

Do you think that, in a case where you knew that there are 10 possible digits labels to assign to the data points, but you have no access to the labels, the observations would group or “cluster” together by some criterion in such a way that you could infer the labels?

Now, this is a research question!

In general, when you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant to your data set. In other words, you think about what your data set might teach you or what you think you can learn from your data.

From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.

Tip: the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. The same also holds for finding the appropriate machine algorithm.

However, when you’re first getting started with scikit-learn, you’ll see that the amount of algorithms that the library contains is pretty vast and that you might still want additional help when you’re assessing your data set. That’s why this scikit-learn machine learning map will come in handy.

Note that this map does require you to have some knowledge about the algorithms that are included in the scikit-learn library. This, by the way, also holds some truth for taking this next step in your project: if you have no idea what is possible, it will be tough to decide on what your use case will be for the data.

As your use case was one for clustering, you can follow the path on the map towards “KMeans”. You’ll see the use case that you have just thought about requires you to have more than 50 samples (“check!”), to have labeled data (“check!”), to know the number of categories that you want to predict (“check!”) and to have less than 10K samples (“check!”).

But what exactly is the K-Means algorithm?

It is one of the simplest and widely used unsupervised learning algorithms to solve clustering problems. The procedure follows a simple and easy way to classify a given data set through a certain number of clusters that you have configured before you run the algorithm. This number of clusters is called k, and you select this number at random.

Then, the k-means algorithm will find the nearest cluster center for each data point and assign the data point closest to that cluster.

Once all data points have been assigned to clusters, the cluster centers will be recomputed. In other words, new cluster centers will emerge from the average of the values of the cluster data points. This process is repeated until most data points stick to the same cluster. The cluster membership should stabilize.

You can already see that, because the k-means algorithm works the way it does, the initial set of cluster centers that you give up can have a significant effect on the clusters that are eventually found. You can, of course, deal with this effect, as you will see further on.

However, before you can go into making a model for your data, you should definitely take a look into preparing your data for this purpose.

Preprocessing Your Data

As you have read in the previous section, before modeling your data, you’ll do well by preparing it first. This preparation step is called “preprocessing”.

Data Normalization

The first thing that we’re going to do is preprocessing the data. You can standardize the digits data by, for example, making use of the scale() method:

(findscalesklearn)
(findstandardizesklearn)

# Import
from sklearn.preprocessing import scale

print(digits.data)

[[  0.   0.   5. ...,   0.   0.   0.]
 [  0.   0.   0. ...,  10.   0.   0.]
 [  0.   0.   0. ...,  16.   9.   0.]
 ..., 
 [  0.   0.   1. ...,   6.   0.   0.]
 [  0.   0.   2. ...,  12.   0.   0.]
 [  0.   0.  10. ...,  12.   1.   0.]]

# Apply `scale()` to the `digits` data
data = scale(digits.data)

print(data)

[[ 0.         -0.33501649 -0.04308102 ..., -1.14664746 -0.5056698
  -0.19600752]
 [ 0.         -0.33501649 -1.09493684 ...,  0.54856067 -0.5056698
  -0.19600752]
 [ 0.         -0.33501649 -1.09493684 ...,  1.56568555  1.6951369
  -0.19600752]
 ..., 
 [ 0.         -0.33501649 -0.88456568 ..., -0.12952258 -0.5056698
  -0.19600752]
 [ 0.         -0.33501649 -0.67419451 ...,  0.8876023  -0.5056698
  -0.19600752]
 [ 0.         -0.33501649  1.00877481 ...,  0.8876023  -0.26113572
  -0.19600752]]

By scaling the data, you shift the distribution of each attribute to have a mean of zero and a standard deviation of one (unit variance).

Splitting Your Data Into Training And Test Sets

To assess your model’s performance later, you will also need to divide the data set into two parts: a training set and a test set. The first is used to train the system, while the second is used to evaluate the learned or trained system.

In practice, the division of your data set into a test and a training sets are disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

You will try to do this also here. You see in the code chunk below that this ‘traditional’ splitting choice is respected: in the arguments of the train_test_split() method, you clearly see that the test_size is set to 0.25.

You’ll also note that the argument random_state has the value 42 assigned to it. With this argument, you can guarantee that your split will always be the same. That is particularly handy if you want reproducible results.

(findsplitsklearn)
(findtraintestsplitsklearn)

# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the `digits` data into training and test sets
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)

After you have split up your data set into train and test sets, you can quickly inspect the numbers before you go and model the data:

# Number of training features
n_samples, n_features = X_train.shape

# Print out `n_samples`
print(n_samples)

1347

# Print out `n_features`
print(n_features)

64

# Number of Training labels
n_digits = len(np.unique(y_train))

print(n_digits)

10

# Inspect `y_train`
print(len(y_train))

1347

You’ll see that the training set X_train now contains 1347 samples, which is precisely 2/3d of the samples that the original data set contained, and 64 features, which hasn’t changed. The y_train training set also contains 2/3d of the labels of the original data set. This means that the test sets X_test and y_test contains 450 samples.

Clustering The digits Data

After all these preparation steps, you have made sure that all your known (training) data is stored. No actual model or learning was performed up until this moment.

Now, it’s finally time to find those clusters of your training set. Use KMeans() from the cluster module to set up your model. You’ll see that there are three arguments that are passed to this method: init, n_clusters and the random_state.

You might still remember this last argument from before when you split the data into training and test sets. This argument basically guaranteed that you got reproducible results.

(findkmeanssklearn)
(findclustersklearn)

# Import the `cluster` module
from sklearn import cluster

# Create the KMeans model
clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)

# Fit the training data to the model
clf.fit(X_train)

The init indicates the method for initialization and even though it defaults to ‘k-means++’, you see it explicitly coming back in the code. That means that you can leave it out if you want. Try it out in the DataCamp Light chunk above!

Next, you also see that the n_clusters argument is set to 10. This number not only indicates the number of clusters or groups you want your data to form, but also the number of centroids to generate. Remember that a cluster centroid is the middle of a cluster.

Do you also still remember how the previous section described this as one of the possible disadvantages of the K-Means algorithm?

That is that the initial set of cluster centers that you give up can have a significant effect on the clusters that are eventually found?

Usually, you try to deal with this effect by trying several initial sets in multiple runs and by selecting the set of clusters with the minimum sum of the squared errors (SSE). In other words, you want to minimize the distance of each point in the cluster to the mean or centroid of that cluster.

By adding the n-init argument to KMeans(), you can determine how many different centroid configurations the algorithm will try.

Note again that you don’t want to insert the test labels when you fit the model to your data: these will be used to see if your model is good at predicting the actual classes of your instances!

You can also visualize the images that make up the cluster centers as follows:

# Import matplotlib
import matplotlib.pyplot as plt

# Figure size in inches
fig = plt.figure(figsize=(8, 3))

# Add title
fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')

# For all labels (0-9)
for i in range(10):
    # Initialize subplots in a grid of 2X5, at i+1th position
    ax = fig.add_subplot(2, 5, 1 + i)
    # Display images
    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)
    # Don't show the axes
    plt.axis('off')

# Show the plot
plt.show()

(A picture of cluster center images)

If you want to see another example that visualizes the data clusters and their centers, go here.

The next step is to predict the labels of the test set:

# Predict the labels for `X_test`
y_pred=clf.predict(X_test)

# Print out the first 100 instances of `y_pred`
print(y_pred[:100])

[4 8 8 9 3 3 5 8 5 3 0 7 1 2 1 3 8 6 8 8 1 5 8 6 5 4 8 5 4 8 1 8 3 1 1 4 8
 1 6 4 4 8 0 8 4 7 8 2 4 5 5 0 8 5 4 2 8 2 2 7 2 1 5 3 1 5 6 2 6 8 8 8 8 6
 6 2 1 5 8 8 8 2 3 8 8 2 4 1 1 8 0 3 7 8 8 3 8 2 1 1]

# Print out the first 100 instances of `y_test`
print(y_test[:100])

[6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9
 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4
 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4]

# Study the shape of the cluster centers
print(clf.cluster_centers_.shape)

(10, 64)

In the code chunk above, you predict the values for the test set, which contains 450 samples. You store the result in y_pred. You also print out the first 100 instances of y_pred and y_test, and you immediately see some results.

In addition, you can study the shape of the cluster centers: you immediately see that there are 10 clusters with each 64 features.

But this doesn’t tell you much because we set the number of clusters to 10 and you already knew that there were 64 features.

Maybe a visualization would be more helpful.

Let’s visualize the predicted labels:

# Import `Isomap()`
from sklearn.manifold import Isomap

# Create an isomap and fit the `digits` data to it
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust layout
fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=clusters)
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)
ax[1].set_title('Actual Training Labels')

# Show the plots
plt.show()

You use Isomap() as a way to reduce the dimensions of your high-dimensional data set digits. The difference with the PCA method is that the Isomap is a non-linear reduction method.

(A picture of predicted versus training labels)

Tip: run the code from above again, but use the PCA reduction method instead of the Isomap to study the effect of reduction methods yourself.

You will find the solution here:

# Import `PCA()`
from sklearn.decomposition import PCA

# Model and fit the `digits` data to the PCA model
X_pca = PCA(n_components=2).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
clusters = clf.fit_predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust layout
fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)
ax[0].set_title('Predicted Training Labels')
ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)
ax[1].set_title('Actual Training Labels')

# Show the plots
plt.show()

(A picture of predicted versus training labels)

At first sight, the visualization doesn’t seem to indicate that the model works well.

But this needs some further investigation.

Evaluation of Your Clustering Model

And this need for further investigation brings you to the next essential step, which is the evaluation of your model’s performance. In other words, you want to analyze the degree of correctness of the model’s predictions.

Let’s print out a confusion matrix:

(findconfusionmatrixsklearn)

# Import `metrics` from `sklearn`
from sklearn import metrics

# Print out the confusion matrix with `confusion_matrix()`
print(metrics.confusion_matrix(y_test, y_pred))

[[ 0  0 43  0  0  0  0  0  0  0]
 [20  0  0  7  0  0  0 10  0  0]
 [ 5  0  0 31  0  0  0  1  1  0]
 [ 1  0  0  1  0  1  4  0 39  0]
 [ 1 50  0  0  0  0  1  2  0  1]
 [ 1  0  0  0  1 41  0  0 16  0]
 [ 0  0  1  0 44  0  0  0  0  0]
 [ 0  0  0  0  0  1 34  1  0  5]
 [21  0  0  0  0  3  1  2 11  0]
 [ 0  0  0  0  0  2  3  3 40  0]]

At first sight, the results seem to confirm our first thoughts that you gathered from the visualizations. Only the digit 5 was classified correctly in 41 cases. Also, the digit 8 was classified correctly in 11 instances. But this is not really a success.

You might need to know a bit more about the results than just the confusion matrix.

Let’s try to figure out something more about the quality of the clusters by applying different cluster quality metrics. That way, you can judge the goodness of fit of the cluster labels to the correct labels.

from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score

print('% 9s' % 'inertia    homo   compl  v-meas     ARI AMI  silhouette')

inertia    homo   compl  v-meas     ARI AMI  silhouette

print('%i   %.3f   %.3f   %.3f   %.3f   %.3f    %.3f'
          %(clf.inertia_,
      homogeneity_score(y_test, y_pred),
      completeness_score(y_test, y_pred),
      v_measure_score(y_test, y_pred),
      adjusted_rand_score(y_test, y_pred),
      adjusted_mutual_info_score(y_test, y_pred),
      silhouette_score(X_test, y_pred, metric='euclidean')))

54276   0.688   0.733   0.710   0.567   0.674    0.146

You’ll see that there are quite some metrics to consider:

- The homogeneity score tells you to what extent all of the clusters contain only data points which are members of a single class.

- The completeness score measures the extent to which all of the data points that are members of a given class are also elements of the same cluster.

- The V-measure score is the harmonic mean between homogeneity and completeness.

- The adjusted Rand score measures the similarity between two clusterings and considers all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.

- The Adjusted Mutual Info (AMI) score is used to compare clusters. It measures the similarity between the data points that are in the clusterings, accounting for chance groupings and takes a maximum value of 1 when clusterings are equivalent.

- The silhouette score measures how similar an object is to its own cluster compared to other clusters. The silhouette scores range from -1 to 1, where a higher value indicates that the object is better matched to its own cluster and worse matched to neighboring clusters. If many points have a high value, the clustering configuration is good.

You clearly see that these scores aren’t fantastic: for example, you see that the value for the silhouette score is close to 0, which indicates that the sample is on or very close to the decision boundary between two neighboring clusters. This could indicate that the samples could have been assigned to the wrong cluster.

Also, the ARI measure seems to indicate that not all data points in a given cluster are similar and the completeness score tells you that there are definitely data points that weren’t put in the right cluster.

Clearly, you should consider another estimator to predict the labels for the digits data.

Trying Out Another Model: Support Vector Machines

When you recapped all of the information that you gathered out of the data exploration, you saw that you could build a model to predict which group a digit belongs to without you knowing the labels. And indeed, you just used the training data and not the target values to build your KMeans model.

Let’s assume that you depart from the case where you use both the digits training data and the corresponding target values to build your model.

If you follow the algorithm map, you’ll see that the first model that you meet is the linear SVC. Let’s apply this now to the digits data:

# Import `train_test_split`
from sklearn.cross_validation import train_test_split

# Split the data into training and test sets 
X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(digits.data, digits.target, digits.images, test_size=0.25, random_state=42)

(finsvmsklearn)

# Import the `svm` model
from sklearn import svm

# Create the SVC model 
svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')

# Fit the data to the SVC model
svc_model.fit(X_train, y_train)

You see here that you make use of X_train and y_train to fit the data to the SVC model. This is clearly different from clustering. Note also that in this example, you set the value of gamma manually. It is possible to automatically find good values for the parameters by using tools such as grid search and cross-validation.

Even though this is not the focus of this tutorial, you will see how you could have gone about this if you would have made use of grid search to adjust your parameters. You would have done something like the following:

# Split the `digits` data into two equal sets
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.5, random_state=0)

(findcrossvalidationsklearn)

# Import GridSearchCV
from sklearn.grid_search import GridSearchCV

# Set the parameter candidates
parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]

# Create a classifier with the parameter candidates
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)

# Train the classifier on training data
clf.fit(X_train, y_train)

# Print out the results 
print('Best score for training data:', clf.best_score_)

Best score for training data: 0.9844097995545658

print('Best `C`:',clf.best_estimator_.C)

Best `C`: 10

print('Best kernel:',clf.best_estimator_.kernel)

Best kernel: rbf

print('Best `gamma`:',clf.best_estimator_.gamma)

Best `gamma`: 0.001

Next, you use the classifier with the classifier and parameter candidates that you have just created to apply it to the second part of your data set. Next, you also train a new classifier using the best parameters found by the grid search. You score the result to see if the best parameters that were found in the grid search are actually working.

# Apply the classifier to the test data, and view the accuracy score
print(clf.score(X_test, y_test))

0.99110122358175756

# Train and score a new classifier with the grid search parameters
svm.SVC(C=10, kernel='rbf', gamma=0.001).fit(X_train, y_train).score(X_test, y_test)

The parameters indeed work well!

Now, what does this new knowledge tell you about the SVC classifier that you had modeled before you had done the grid search?

Let’s back up to the model that you had made before.

You see that in the SVM classifier, the penalty parameter C of the error term is specified at 100.. Lastly, you see that the kernel has been explicitly specified as a linear one. The kernelargument specifies the kernel type that you’re going to use in the algorithm and by default, this is rbf. In other cases, you can specify others such as linear, poly, …

But what is a kernel exactly?

A kernel is a similarity function, which is used to compute the similarity between the training data points. When you provide a kernel to an algorithm, together with the training data and the labels, you will get a classifier, as is the case here. You will have trained a model that assigns new unseen objects into a particular category. For the SVM, you will typically try to divide your data points linearly.

However, the grid search tells you that an rbf kernel would’ve worked better. The penalty parameter and the gamma were specified correctly.

Tip: try out the classifier with an rbf kernel.

For now, let’s say you continue with a linear kernel and predict the values for the test set:

# Predict the label of `X_test`
print(svc_model.predict(X_test))

[6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9
 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 3 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4
 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7
 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 9 0 6 6 4 2 8 0 9 4 6 9 9 6 9
 0 5 5 6 6 0 6 4 3 9 3 7 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3
 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7
 1 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 5 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8
 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5
 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8
 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 9 5 6 0 0 3 0 5 0 0 4 1
 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4
 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 2 3 6 3 9 6 9 5 0 1 5 5 8
 3 3 6 2 6 5]

# Print `y_test` to check the results
print(y_test)

[6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9
 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4
 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7
 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9
 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3
 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7
 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8
 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5
 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8
 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5 6 0 0 3 0 5 0 0 4 1
 2 8 4 5 9 6 3 1 8 8 4 2 3 8 9 8 8 5 0 6 3 3 7 1 6 4 1 2 1 1 6 4 7 4 8 3 4
 0 5 1 9 4 5 7 6 3 7 0 5 9 7 5 9 7 4 2 1 9 0 7 5 3 3 6 3 9 6 9 5 0 1 5 5 8
 3 3 6 2 6 5]

You can also visualize the images and their predicted labels:

# Import matplotlib
import matplotlib.pyplot as plt

# Assign the predicted values to `predicted`
predicted = svc_model.predict(X_test)

# Zip together the `images_test` and `predicted` values in `images_and_predictions`
images_and_predictions = list(zip(images_test, predicted))

# For the first 4 elements in `images_and_predictions`
for index, (image, prediction) in enumerate(images_and_predictions[:4]):
    # Initialize subplots in a grid of 1 by 4 at positions i+1
    plt.subplot(1, 4, index + 1)
    # Don't show axes
    plt.axis('off')
    # Display images in all subplots in the grid
    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')
    # Add a title to the plot
    plt.title('Predicted: ' + str(prediction))

# Show the plot
plt.show()

This plot is very similar to the plot that you made when you were exploring the data:

(A picture of digits 6, 9, 3, 7 and their blurry images)

Only this time, you zip together the images and the predicted values, and you only take the first 4 elements of images_and_predictions.

But now the biggest question: how does this model perform?

# Import `metrics`
from sklearn import metrics

# Print the classification report of `y_test` and `predicted`
print(metrics.classification_report(y_test, predicted))

             precision    recall  f1-score   support

          0       1.00      1.00      1.00        43
          1       0.97      1.00      0.99        37
          2       0.97      1.00      0.99        38
          3       0.98      0.93      0.96        46
          4       1.00      0.98      0.99        55
          5       0.97      1.00      0.98        59
          6       1.00      1.00      1.00        45
          7       0.98      0.98      0.98        41
          8       1.00      0.97      0.99        38
          9       0.96      0.96      0.96        48

avg / total       0.98      0.98      0.98       450

# Print the confusion matrix
print(metrics.confusion_matrix(y_test, predicted))
    
[[43  0  0  0  0  0  0  0  0  0]
 [ 0 37  0  0  0  0  0  0  0  0]
 [ 0  0 38  0  0  0  0  0  0  0]
 [ 0  0  1 43  0  1  0  0  0  1]
 [ 0  1  0  0 54  0  0  0  0  0]
 [ 0  0  0  0  0 59  0  0  0  0]
 [ 0  0  0  0  0  0 45  0  0  0]
 [ 0  0  0  0  0  0  0 40  0  1]
 [ 0  0  0  0  0  1  0  0 37  0]
 [ 0  0  0  1  0  0  0  1  0 46]]

You clearly see that this model performs a whole lot better than the clustering model that you used earlier.

You can also see it when you visualize the predicted and the actual labels with the help of Isomap():

# Import `Isomap()`
from sklearn.manifold import Isomap

# Create an isomap and fit the `digits` data to it
X_iso = Isomap(n_neighbors=10).fit_transform(X_train)

# Compute cluster centers and predict cluster index for each sample
predicted = svc_model.predict(X_train)

# Create a plot with subplots in a grid of 1X2
fig, ax = plt.subplots(1, 2, figsize=(8, 4))

# Adjust the layout
fig.subplots_adjust(top=0.85)

# Add scatterplots to the subplots 
ax[0].scatter(X_iso[:, 0], X_iso[:, 1], c=predicted)
ax[0].set_title('Predicted labels')
ax[1].scatter(X_iso[:, 0], X_iso[:, 1], c=y_train)
ax[1].set_title('Actual Labels')


# Add title
fig.suptitle('Predicted versus actual labels', fontsize=14, fontweight='bold')

# Show the plot
plt.show()

This will give you the following scatterplots:

(A picture of predicted versus actual labels)

You’ll see that this visualization confirms your classification report, which is excellent news. :)

What's Next?

Digit Recognition in Natural Images

Congratulations, you have reached the end of this scikit-learn tutorial, which was meant to introduce you to Python machine learning! Now it's your turn.

Firstly, make sure you get a hold of DataCamp's scikit-learn cheat sheet.

Next, start your own digit recognition project with different data. One dataset that you can already use is the MNIST data, which you can download here.

The steps that you can take are very similar to the ones that you have gone through with this tutorial, but if you still feel that you can use some help, you should check out this page, which works with the MNIST data and applies the KMeans algorithm.

Working with the digits dataset was the first step in classifying characters with scikit-learn. If you’re done with this, you might consider trying out an even more challenging problem, namely, classifying alphanumeric characters in natural images.

A well-known dataset that you can use for this problem is the Chars74K dataset, which contains more than 74,000 images of digits from 0 to 9 and both lowercase and higher case letters of the English alphabet. You can download the dataset here.

Data Visualization and pandas

Whether you're going to start with the projects that have been mentioned above or not, this is definitely not the end of your journey of data science with Python. If you choose not to widen your view just yet, consider deepening your data visualization and data manipulation knowledge.

Don't miss out on our Interactive Data Visualization with Bokeh course to make sure you can impress your peers with a stunning data science portfolio or our pandas Foundation course, to learn more about working with data frames in Python.
